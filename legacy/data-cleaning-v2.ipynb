{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d44f20",
   "metadata": {},
   "source": [
    "# drugie podejscie do czyszczenia danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c960937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (750000, 12)\n",
      "Test shape: (250000, 11)\n",
      "\n",
      "Train columns: ['id', 'Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', 'Listening_Time_minutes']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import mstats\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ścieżki do plików\n",
    "TRAIN_PATH = Path(\"data-task/train.csv\")\n",
    "TEST_PATH = Path(\"data-task/test.csv\")\n",
    "\n",
    "# Wczytaj dane\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nTrain columns: {train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783008da",
   "metadata": {},
   "source": [
    "Analiza braków danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d432d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BRAKI W TRAIN ===\n",
      "id                                  0\n",
      "Podcast_Name                        0\n",
      "Episode_Title                       0\n",
      "Episode_Length_minutes          87093\n",
      "Genre                               0\n",
      "Host_Popularity_percentage          0\n",
      "Publication_Day                     0\n",
      "Publication_Time                    0\n",
      "Guest_Popularity_percentage    146030\n",
      "Number_of_Ads                       1\n",
      "Episode_Sentiment                   0\n",
      "Listening_Time_minutes              0\n",
      "dtype: int64\n",
      "\n",
      "Procent braków:\n",
      "id                              0.00\n",
      "Podcast_Name                    0.00\n",
      "Episode_Title                   0.00\n",
      "Episode_Length_minutes         11.61\n",
      "Genre                           0.00\n",
      "Host_Popularity_percentage      0.00\n",
      "Publication_Day                 0.00\n",
      "Publication_Time                0.00\n",
      "Guest_Popularity_percentage    19.47\n",
      "Number_of_Ads                   0.00\n",
      "Episode_Sentiment               0.00\n",
      "Listening_Time_minutes          0.00\n",
      "dtype: float64\n",
      "\n",
      "=== BRAKI W TEST ===\n",
      "id                                 0\n",
      "Podcast_Name                       0\n",
      "Episode_Title                      0\n",
      "Episode_Length_minutes         28736\n",
      "Genre                              0\n",
      "Host_Popularity_percentage         0\n",
      "Publication_Day                    0\n",
      "Publication_Time                   0\n",
      "Guest_Popularity_percentage    48832\n",
      "Number_of_Ads                      0\n",
      "Episode_Sentiment                  0\n",
      "dtype: int64\n",
      "\n",
      "Procent braków:\n",
      "id                              0.00\n",
      "Podcast_Name                    0.00\n",
      "Episode_Title                   0.00\n",
      "Episode_Length_minutes         11.49\n",
      "Genre                           0.00\n",
      "Host_Popularity_percentage      0.00\n",
      "Publication_Day                 0.00\n",
      "Publication_Time                0.00\n",
      "Guest_Popularity_percentage    19.53\n",
      "Number_of_Ads                   0.00\n",
      "Episode_Sentiment               0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BRAKI W TRAIN ===\")\n",
    "print(train.isnull().sum())\n",
    "print(f\"\\nProcent braków:\")\n",
    "print((train.isnull().sum() / len(train) * 100).round(2))\n",
    "\n",
    "print(\"\\n=== BRAKI W TEST ===\")\n",
    "print(test.isnull().sum())\n",
    "print(f\"\\nProcent braków:\")\n",
    "print((test.isnull().sum() / len(test) * 100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4908fb",
   "metadata": {},
   "source": [
    "Dodanie flag dla brakujących danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83bb082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagi dodane!\n",
      "Train - Episode_Length_missing: 87093\n",
      "Test - Episode_Length_missing: 28736\n",
      "Train - Guest_Pop_missing: 146030\n",
      "Test - Guest_Pop_missing: 48832\n"
     ]
    }
   ],
   "source": [
    "# Ponieważ test ma braki w tych samych kolumnach co train, flagi będą informatywne!\n",
    "\n",
    "# Flagi dla Episode_Length_minutes\n",
    "train[\"Episode_Length_missing\"] = train[\"Episode_Length_minutes\"].isnull().astype(int)\n",
    "test[\"Episode_Length_missing\"] = test[\"Episode_Length_minutes\"].isnull().astype(int)\n",
    "\n",
    "# Flagi dla Guest_Popularity_percentage\n",
    "train[\"Guest_Pop_missing\"] = train[\"Guest_Popularity_percentage\"].isnull().astype(int)\n",
    "test[\"Guest_Pop_missing\"] = test[\"Guest_Popularity_percentage\"].isnull().astype(int)\n",
    "\n",
    "print(\"Flagi dodane!\")\n",
    "print(f\"Train - Episode_Length_missing: {train['Episode_Length_missing'].sum()}\")\n",
    "print(f\"Test - Episode_Length_missing: {test['Episode_Length_missing'].sum()}\")\n",
    "print(f\"Train - Guest_Pop_missing: {train['Guest_Pop_missing'].sum()}\")\n",
    "print(f\"Test - Guest_Pop_missing: {test['Guest_Pop_missing'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be0de9",
   "metadata": {},
   "source": [
    "Usunięcie braków w Number_of_Ads (tylko train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b54306f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train przed usunięciem: 750000\n",
      "Train po usunięciu: 749999\n",
      "Usunięto: -516875 wierszy\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train przed usunięciem: {len(train)}\")\n",
    "train = train.dropna(subset=[\"Number_of_Ads\"])\n",
    "print(f\"Train po usunięciu: {len(train)}\")\n",
    "print(f\"Usunięto: {87093 + 146030 + 1 - len(train)} wierszy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93d5af",
   "metadata": {},
   "source": [
    "Imputation brakujących wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e7101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode_Length median: 63.84\n",
      "Guest_Popularity mean: 52.23645285195598\n",
      "\n",
      "Imputation zakończony!\n",
      "Train braki: 0\n",
      "Test braki: 0\n"
     ]
    }
   ],
   "source": [
    "# Obliczamy statystyki TYLKO na train, potem aplikujemy do obu zbiorów.\n",
    "\n",
    "\n",
    "# Oblicz statystyki na train\n",
    "ep_len_median = train[\"Episode_Length_minutes\"].median()\n",
    "guest_pop_mean = train[\"Guest_Popularity_percentage\"].mean()\n",
    "\n",
    "print(f\"Episode_Length median: {ep_len_median}\")\n",
    "print(f\"Guest_Popularity mean: {guest_pop_mean}\")\n",
    "\n",
    "# Wypełnij braki\n",
    "train[\"Episode_Length_minutes\"].fillna(ep_len_median, inplace=True)\n",
    "test[\"Episode_Length_minutes\"].fillna(ep_len_median, inplace=True)\n",
    "\n",
    "train[\"Guest_Popularity_percentage\"].fillna(guest_pop_mean, inplace=True)\n",
    "test[\"Guest_Popularity_percentage\"].fillna(guest_pop_mean, inplace=True)\n",
    "\n",
    "print(\"\\nImputation zakończony!\")\n",
    "print(f\"Train braki: {train.isnull().sum().sum()}\")\n",
    "print(f\"Test braki: {test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d137ed3",
   "metadata": {},
   "source": [
    "Winsorization outlierów (zamiast usuwania!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9d8f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode_Length bounds: [6.78, 118.89]\n",
      "Number_of_Ads bounds: [0.00, 3.00]\n",
      "\n",
      "Winsorization zakończony!\n"
     ]
    }
   ],
   "source": [
    "# Winsorization dla Episode_Length_minutes\n",
    "\n",
    "\n",
    "ep_len_winsorized = mstats.winsorize(train[\"Episode_Length_minutes\"], limits=[0.01, 0.01])\n",
    "ep_lower, ep_upper = ep_len_winsorized.min(), ep_len_winsorized.max()\n",
    "\n",
    "print(f\"Episode_Length bounds: [{ep_lower:.2f}, {ep_upper:.2f}]\")\n",
    "\n",
    "train[\"Episode_Length_minutes\"] = train[\"Episode_Length_minutes\"].clip(ep_lower, ep_upper)\n",
    "test[\"Episode_Length_minutes\"] = test[\"Episode_Length_minutes\"].clip(ep_lower, ep_upper)\n",
    "\n",
    "# Winsorization dla Number_of_Ads\n",
    "ads_winsorized = mstats.winsorize(train[\"Number_of_Ads\"], limits=[0.01, 0.01])\n",
    "ads_lower, ads_upper = ads_winsorized.min(), ads_winsorized.max()\n",
    "\n",
    "print(f\"Number_of_Ads bounds: [{ads_lower:.2f}, {ads_upper:.2f}]\")\n",
    "\n",
    "train[\"Number_of_Ads\"] = train[\"Number_of_Ads\"].clip(ads_lower, ads_upper)\n",
    "test[\"Number_of_Ads\"] = test[\"Number_of_Ads\"].clip(ads_lower, ads_upper)\n",
    "\n",
    "print(\"\\nWinsorization zakończony!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e930e9",
   "metadata": {},
   "source": [
    "Usunięcie duplikatów (tylko train!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce47f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train przed usunięciem duplikatów: 749999\n",
      "Train po usunięciu duplikatów: 749999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train przed usunięciem duplikatów: {len(train)}\")\n",
    "train = train.drop_duplicates()\n",
    "print(f\"Train po usunięciu duplikatów: {len(train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe63d71",
   "metadata": {},
   "source": [
    "Parsowanie czasu publikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1030be20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas sparsowany!\n",
      "Przykładowe godziny: 0   NaN\n",
      "1   NaN\n",
      "2   NaN\n",
      "3   NaN\n",
      "4   NaN\n",
      "Name: hour, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for df in [train, test]:\n",
    "    # Połącz dzień i czas w datetime\n",
    "    df[\"pub_datetime\"] = pd.to_datetime(\n",
    "        df[\"Publication_Day\"] + \" \" + df[\"Publication_Time\"],\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Wyciągnij komponenty\n",
    "    df[\"day_of_week\"] = df[\"pub_datetime\"].dt.dayofweek  # 0=Mon, 6=Sun\n",
    "    df[\"hour\"] = df[\"pub_datetime\"].dt.hour\n",
    "    df[\"month\"] = df[\"pub_datetime\"].dt.month\n",
    "    df[\"day_of_month\"] = df[\"pub_datetime\"].dt.day\n",
    "\n",
    "print(\"Czas sparsowany!\")\n",
    "print(f\"Przykładowe godziny: {train['hour'].head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568db00d",
   "metadata": {},
   "source": [
    "Feature Engineering - Podstawowe features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5cb8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podstawowe features dodane! Train shape: (749999, 36)\n"
     ]
    }
   ],
   "source": [
    "for df in [train, test]:\n",
    "    # === Czas publikacji ===\n",
    "    df[\"is_weekend\"] = (df[\"day_of_week\"] >= 5).astype(int)\n",
    "    df[\"is_primetime\"] = ((df[\"hour\"] >= 17) & (df[\"hour\"] <= 21)).astype(int)\n",
    "    df[\"is_morning\"] = ((df[\"hour\"] >= 6) & (df[\"hour\"] <= 12)).astype(int)\n",
    "    df[\"is_night\"] = ((df[\"hour\"] >= 22) | (df[\"hour\"] <= 5)).astype(int)\n",
    "\n",
    "    # zobrazowac te numery na wykresie i potem spraedzic np jaki jest prime time\n",
    "\n",
    "    # === Interakcje numeryczne ===\n",
    "    df[\"ads_per_minute\"] = df[\"Number_of_Ads\"] / (df[\"Episode_Length_minutes\"] + 1)\n",
    "    df[\"total_popularity\"] = df[\"Host_Popularity_percentage\"] + df[\"Guest_Popularity_percentage\"]\n",
    "    df[\"popularity_ratio\"] = df[\"Host_Popularity_percentage\"] / (df[\"Guest_Popularity_percentage\"] + 1)\n",
    "    df[\"popularity_diff\"] = df[\"Host_Popularity_percentage\"] - df[\"Guest_Popularity_percentage\"]\n",
    "    \n",
    "    # === Interakcje z flagami missing ===\n",
    "    df[\"missing_guest_x_host_pop\"] = df[\"Guest_Pop_missing\"] * df[\"Host_Popularity_percentage\"]\n",
    "    df[\"missing_length_x_ads\"] = df[\"Episode_Length_missing\"] * df[\"Number_of_Ads\"]\n",
    "    df[\"missing_guest_x_episode_length\"] = df[\"Guest_Pop_missing\"] * df[\"Episode_Length_minutes\"]\n",
    "    \n",
    "    # === Interakcje czasowe ===\n",
    "    df[\"weekend_x_primetime\"] = df[\"is_weekend\"] * df[\"is_primetime\"]\n",
    "    df[\"weekend_x_ads\"] = df[\"is_weekend\"] * df[\"Number_of_Ads\"]\n",
    "    \n",
    "    # === Sentiment jako numeric ===\n",
    "    sentiment_map = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    df[\"sentiment_numeric\"] = df[\"Episode_Sentiment\"].map(sentiment_map).fillna(0)\n",
    "    \n",
    "    df[\"sentiment_x_guest_pop\"] = df[\"sentiment_numeric\"] * df[\"Guest_Popularity_percentage\"]\n",
    "    df[\"sentiment_x_host_pop\"] = df[\"sentiment_numeric\"] * df[\"Host_Popularity_percentage\"]\n",
    "    df[\"negative_sentiment_x_ads\"] = (df[\"sentiment_numeric\"] == -1).astype(int) * df[\"Number_of_Ads\"]\n",
    "\n",
    "print(f\"Podstawowe features dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3077130",
   "metadata": {},
   "source": [
    "Text Features z tytułów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e089011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features dodane!\n"
     ]
    }
   ],
   "source": [
    "for df in [train, test]:\n",
    "    # Długość stringów\n",
    "    df[\"episode_title_len\"] = df[\"Episode_Title\"].astype(str).str.len()\n",
    "    df[\"podcast_name_len\"] = df[\"Podcast_Name\"].astype(str).str.len()\n",
    "    \n",
    "    # Liczba słów\n",
    "    df[\"episode_title_words\"] = df[\"Episode_Title\"].astype(str).str.split().str.len()\n",
    "    df[\"podcast_name_words\"] = df[\"Podcast_Name\"].astype(str).str.split().str.len()\n",
    "    \n",
    "    # Czy zawiera cyfry/znaki specjalne\n",
    "    df[\"title_has_numbers\"] = df[\"Episode_Title\"].astype(str).str.contains(r'\\d', na=False).astype(int)\n",
    "    df[\"title_has_special\"] = df[\"Episode_Title\"].astype(str).str.contains(r'[!?#]', na=False).astype(int)\n",
    "    \n",
    "    # Czy to odcinek specjalny?\n",
    "    df[\"is_special_episode\"] = df[\"Episode_Title\"].astype(str).str.contains(\n",
    "        r'special|bonus|exclusive|interview|live', case=False, na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Uppercase ratio (clickbait?)\n",
    "    df[\"title_uppercase_ratio\"] = df[\"Episode_Title\"].astype(str).apply(\n",
    "        lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1)\n",
    "    )\n",
    "\n",
    "print(\"Text features dodane!\")\n",
    "\n",
    "# do wywalenia, zamiast tego embeddingi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575da25",
   "metadata": {},
   "source": [
    "Frequency Encoding dla high-cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcf7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency encoding zakończony!\n",
      "Top 5 podcasts by frequency:\n",
      "Podcast_Name\n",
      "Tech Talks       22847\n",
      "Sports Weekly    20053\n",
      "Funny Folks      19635\n",
      "Tech Trends      19549\n",
      "Fitness First    19488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Podcast frequency\n",
    "podcast_freq = train[\"Podcast_Name\"].value_counts()\n",
    "train[\"podcast_frequency\"] = train[\"Podcast_Name\"].map(podcast_freq).fillna(0)\n",
    "test[\"podcast_frequency\"] = test[\"Podcast_Name\"].map(podcast_freq).fillna(0)\n",
    "\n",
    "# Genre frequency\n",
    "genre_freq = train[\"Genre\"].value_counts()\n",
    "train[\"genre_frequency\"] = train[\"Genre\"].map(genre_freq).fillna(0)\n",
    "test[\"genre_frequency\"] = test[\"Genre\"].map(genre_freq).fillna(0)\n",
    "\n",
    "# Normalizacja (0-1)\n",
    "train[\"podcast_frequency_norm\"] = train[\"podcast_frequency\"] / len(train)\n",
    "test[\"podcast_frequency_norm\"] = test[\"podcast_frequency\"] / len(train)\n",
    "\n",
    "train[\"genre_frequency_norm\"] = train[\"genre_frequency\"] / len(train)\n",
    "test[\"genre_frequency_norm\"] = test[\"genre_frequency\"] / len(train)\n",
    "\n",
    "print(\"Frequency encoding zakończony!\")\n",
    "print(f\"Top 5 podcasts by frequency:\\n{podcast_freq.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b753b6",
   "metadata": {},
   "source": [
    "Agregacje per Podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c2e2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podcast stats dodane! Train shape: (749999, 59)\n"
     ]
    }
   ],
   "source": [
    "# Statystyki per Podcast (obliczane TYLKO na train!)\n",
    "podcast_stats = train.groupby(\"Podcast_Name\").agg({\n",
    "    \"Listening_Time_minutes\": [\"mean\", \"median\", \"std\", \"min\", \"max\"],\n",
    "    \"Episode_Length_minutes\": [\"mean\", \"median\"],\n",
    "    \"Number_of_Ads\": [\"mean\", \"median\"],\n",
    "    \"Host_Popularity_percentage\": \"first\",\n",
    "    \"Guest_Popularity_percentage\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# Spłaszcz kolumny\n",
    "podcast_stats.columns = [\n",
    "    \"Podcast_Name\",\n",
    "    \"podcast_avg_listening\", \"podcast_med_listening\", \"podcast_std_listening\",\n",
    "    \"podcast_min_listening\", \"podcast_max_listening\",\n",
    "    \"podcast_avg_length\", \"podcast_med_length\",\n",
    "    \"podcast_avg_ads\", \"podcast_med_ads\",\n",
    "    \"podcast_host_pop\", \"podcast_avg_guest_pop\"\n",
    "]\n",
    "\n",
    "# Wypełnij std NaN (podcasty z 1 odcinkiem)\n",
    "podcast_stats[\"podcast_std_listening\"].fillna(0, inplace=True)\n",
    "\n",
    "# Merguj do train i test\n",
    "train = train.merge(podcast_stats, on=\"Podcast_Name\", how=\"left\")\n",
    "test = test.merge(podcast_stats, on=\"Podcast_Name\", how=\"left\")\n",
    "\n",
    "print(f\"Podcast stats dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fceaa04",
   "metadata": {},
   "source": [
    "Agregacje per Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80cca1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre stats dodane! Train shape: (749999, 64)\n"
     ]
    }
   ],
   "source": [
    "# Statystyki per Genre\n",
    "genre_stats = train.groupby(\"Genre\").agg({\n",
    "    \"Listening_Time_minutes\": [\"mean\", \"median\", \"std\"],\n",
    "    \"Guest_Popularity_percentage\": \"mean\",\n",
    "    \"Episode_Length_minutes\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "genre_stats.columns = [\n",
    "    \"Genre\",\n",
    "    \"genre_avg_listening\", \"genre_med_listening\", \"genre_std_listening\",\n",
    "    \"genre_avg_guest_pop\", \"genre_avg_length\"\n",
    "]\n",
    "\n",
    "genre_stats[\"genre_std_listening\"].fillna(0, inplace=True)\n",
    "\n",
    "# Merguj\n",
    "train = train.merge(genre_stats, on=\"Genre\", how=\"left\")\n",
    "test = test.merge(genre_stats, on=\"Genre\", how=\"left\")\n",
    "\n",
    "print(f\"Genre stats dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7190e",
   "metadata": {},
   "source": [
    "Relative Features (odcinek vs średnia podcastu/gatunku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655c763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative features dodane!\n"
     ]
    }
   ],
   "source": [
    "for df in [train, test]:\n",
    "    # Porównanie z podcast\n",
    "    df[\"length_vs_podcast_avg\"] = df[\"Episode_Length_minutes\"] / (df[\"podcast_avg_length\"] + 1)\n",
    "    df[\"ads_vs_podcast_avg\"] = df[\"Number_of_Ads\"] / (df[\"podcast_avg_ads\"] + 1)\n",
    "    df[\"guest_pop_vs_podcast_avg\"] = df[\"Guest_Popularity_percentage\"] / (df[\"podcast_avg_guest_pop\"] + 1)\n",
    "    \n",
    "    # Porównanie z genre\n",
    "    df[\"length_vs_genre_avg\"] = df[\"Episode_Length_minutes\"] / (df[\"genre_avg_length\"] + 1)\n",
    "    \n",
    "    # Czy ten odcinek jest powyżej/poniżej średniej?\n",
    "    df[\"above_podcast_avg_length\"] = (df[\"Episode_Length_minutes\"] > df[\"podcast_avg_length\"]).astype(int)\n",
    "    df[\"above_genre_avg_length\"] = (df[\"Episode_Length_minutes\"] > df[\"genre_avg_length\"]).astype(int)\n",
    "\n",
    "print(\"Relative features dodane!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea1bc7",
   "metadata": {},
   "source": [
    "Target Encoding dla kategorycznych (z CV leak protection!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09093604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding w toku (może potrwać ~1min)...\n",
      "✓ Podcast_Name zakończony\n",
      "✓ Genre zakończony\n",
      "\n",
      "Target encoding zakończony! Train shape: (749999, 72)\n"
     ]
    }
   ],
   "source": [
    "def target_encode_with_cv(train_df, test_df, cat_col, target_col, n_splits=5, smoothing=10):\n",
    "    \"\"\"\n",
    "    Target encoding z CV żeby uniknąć leakage\n",
    "    \"\"\"\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    \n",
    "    # Inicjalizacja\n",
    "    train_df[f\"{cat_col}_target_enc\"] = global_mean\n",
    "    \n",
    "    # K-Fold CV dla train\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(train_df):\n",
    "        train_fold = train_df.iloc[train_idx]\n",
    "        \n",
    "        # Oblicz statystyki na foldzie treningowym\n",
    "        agg = train_fold.groupby(cat_col)[target_col].agg(['mean', 'count'])\n",
    "        \n",
    "        # Smoothing (Bayesian average)\n",
    "        smoothed = (agg['count'] * agg['mean'] + smoothing * global_mean) / (agg['count'] + smoothing)\n",
    "        \n",
    "        # Mapuj na fold walidacyjny\n",
    "        train_df.loc[val_idx, f\"{cat_col}_target_enc\"] = train_df.loc[val_idx, cat_col].map(smoothed).fillna(global_mean)\n",
    "    \n",
    "    # Dla test użyj całego train\n",
    "    agg_full = train_df.groupby(cat_col)[target_col].agg(['mean', 'count'])\n",
    "    smoothed_full = (agg_full['count'] * agg_full['mean'] + smoothing * global_mean) / (agg_full['count'] + smoothing)\n",
    "    test_df[f\"{cat_col}_target_enc\"] = test_df[cat_col].map(smoothed_full).fillna(global_mean)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Zastosuj target encoding\n",
    "print(\"Target encoding w toku (może potrwać ~1min)...\")\n",
    "\n",
    "for col in [\"Podcast_Name\", \"Genre\"]:\n",
    "    train, test = target_encode_with_cv(train, test, col, \"Listening_Time_minutes\", smoothing=10)\n",
    "    print(f\"✓ {col} zakończony\")\n",
    "\n",
    "print(f\"\\nTarget encoding zakończony! Train shape: {train.shape}\")\n",
    "\n",
    "# po co to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21cdc72",
   "metadata": {},
   "source": [
    "Konwersja object -> string (dla AutoGluon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b5195d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konwersja typów zakończona!\n",
      "\n",
      "Train dtypes:\n",
      "float64           46\n",
      "int64             19\n",
      "string[python]     6\n",
      "datetime64[ns]     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in train.select_dtypes(include=[\"object\"]).columns:\n",
    "    if col in train.columns:\n",
    "        train[col] = train[col].astype(\"string\")\n",
    "    if col in test.columns:\n",
    "        test[col] = test[col].astype(\"string\")\n",
    "\n",
    "print(\"Konwersja typów zakończona!\")\n",
    "print(f\"\\nTrain dtypes:\\n{train.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb7f60",
   "metadata": {},
   "source": [
    "Wypełnienie ewentualnych NaN powstałych przy mergowaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bc8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train final missing: 3749995\n",
      "Test final missing: 1250000\n"
     ]
    }
   ],
   "source": [
    "# Dla nowych podcastów/gatunków w test, które nie były w train\n",
    "fill_cols = [col for col in train.columns if 'podcast_' in col or 'genre_' in col]\n",
    "\n",
    "for col in fill_cols:\n",
    "    if col in train.columns and col in test.columns:\n",
    "        train[col].fillna(0, inplace=True)\n",
    "        test[col].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Train final missing: {train.isnull().sum().sum()}\")\n",
    "print(f\"Test final missing: {test.isnull().sum().sum()}\")\n",
    "\n",
    "# d;aczego tak duzo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb46b25",
   "metadata": {},
   "source": [
    "Podsumowanie finalnych features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f8b6e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINALNE STATYSTYKI:\n",
      "Train shape: (749999, 72)\n",
      "Test shape: (250000, 71)\n",
      "\n",
      "Liczba features: 71\n",
      "\n",
      "Nowe features (przykłady):\n",
      "Dodano 60 nowych features\n",
      "\n",
      "Przykładowe nowe features:\n",
      "  - Episode_Length_missing\n",
      "  - Guest_Pop_missing\n",
      "  - pub_datetime\n",
      "  - day_of_week\n",
      "  - hour\n",
      "  - month\n",
      "  - day_of_month\n",
      "  - is_weekend\n",
      "  - is_primetime\n",
      "  - is_morning\n",
      "  - is_night\n",
      "  - ads_per_minute\n",
      "  - total_popularity\n",
      "  - popularity_ratio\n",
      "  - popularity_diff\n"
     ]
    }
   ],
   "source": [
    "print(f\"FINALNE STATYSTYKI:\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nLiczba features: {train.shape[1] - 1}\")  # -1 bo target\n",
    "print(f\"\\nNowe features (przykłady):\")\n",
    "new_features = [col for col in train.columns if col not in pd.read_csv(TRAIN_PATH).columns]\n",
    "print(f\"Dodano {len(new_features)} nowych features\")\n",
    "print(\"\\nPrzykładowe nowe features:\")\n",
    "for feat in new_features[:15]:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2defb",
   "metadata": {},
   "source": [
    "Zapisanie przetworzonych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17a1a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pliki zapisane:\n",
      "Train: data-generated/train_advanced_features.csv\n",
      "Test: data-generated/test_advanced_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Zapisz\n",
    "output_train = Path(\"data-generated/train_advanced_features.csv\")\n",
    "output_test = Path(\"data-generated/test_advanced_features.csv\")\n",
    "\n",
    "train.to_csv(output_train, index=False)\n",
    "test.to_csv(output_test, index=False)\n",
    "\n",
    "print(f\"Pliki zapisane:\")\n",
    "print(f\"Train: {output_train}\")\n",
    "print(f\"Test: {output_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a23a1",
   "metadata": {},
   "source": [
    "Quick check - preview finalnych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c5d8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN PREVIEW ===\n",
      "   id     Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n",
      "0   0  Mystery Matters    Episode 98                   63.84  True Crime   \n",
      "1   1    Joke Junction    Episode 26                  118.89      Comedy   \n",
      "2   2   Study Sessions    Episode 16                   73.90   Education   \n",
      "\n",
      "   Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
      "0                       74.81        Thursday            Night   \n",
      "1                       66.95        Saturday        Afternoon   \n",
      "2                       69.97         Tuesday          Evening   \n",
      "\n",
      "   Guest_Popularity_percentage  Number_of_Ads  ... genre_avg_guest_pop  \\\n",
      "0                    52.236453            0.0  ...           52.626684   \n",
      "1                    75.950000            2.0  ...           53.156679   \n",
      "2                     8.970000            0.0  ...           52.298693   \n",
      "\n",
      "   genre_avg_length  length_vs_podcast_avg  ads_vs_podcast_avg  \\\n",
      "0         64.459676               0.977233            0.000000   \n",
      "1         63.365055               1.897952            0.861125   \n",
      "2         64.843738               1.107653            0.000000   \n",
      "\n",
      "  guest_pop_vs_podcast_avg  length_vs_genre_avg  above_podcast_avg_length  \\\n",
      "0                 0.981147             0.975257                         0   \n",
      "1                 1.405893             1.847120                         1   \n",
      "2                 0.167194             1.122354                         1   \n",
      "\n",
      "   above_genre_avg_length  Podcast_Name_target_enc  Genre_target_enc  \n",
      "0                       0                45.976073         46.056395  \n",
      "1                       1                42.609257         44.478896  \n",
      "2                       1                47.093688         45.749755  \n",
      "\n",
      "[3 rows x 72 columns]\n",
      "\n",
      "=== TRAIN INFO ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 749999 entries, 0 to 749998\n",
      "Data columns (total 72 columns):\n",
      " #   Column                          Non-Null Count   Dtype         \n",
      "---  ------                          --------------   -----         \n",
      " 0   id                              749999 non-null  int64         \n",
      " 1   Podcast_Name                    749999 non-null  string        \n",
      " 2   Episode_Title                   749999 non-null  string        \n",
      " 3   Episode_Length_minutes          749999 non-null  float64       \n",
      " 4   Genre                           749999 non-null  string        \n",
      " 5   Host_Popularity_percentage      749999 non-null  float64       \n",
      " 6   Publication_Day                 749999 non-null  string        \n",
      " 7   Publication_Time                749999 non-null  string        \n",
      " 8   Guest_Popularity_percentage     749999 non-null  float64       \n",
      " 9   Number_of_Ads                   749999 non-null  float64       \n",
      " 10  Episode_Sentiment               749999 non-null  string        \n",
      " 11  Listening_Time_minutes          749999 non-null  float64       \n",
      " 12  Episode_Length_missing          749999 non-null  int64         \n",
      " 13  Guest_Pop_missing               749999 non-null  int64         \n",
      " 14  pub_datetime                    0 non-null       datetime64[ns]\n",
      " 15  day_of_week                     0 non-null       float64       \n",
      " 16  hour                            0 non-null       float64       \n",
      " 17  month                           0 non-null       float64       \n",
      " 18  day_of_month                    0 non-null       float64       \n",
      " 19  is_weekend                      749999 non-null  int64         \n",
      " 20  is_primetime                    749999 non-null  int64         \n",
      " 21  is_morning                      749999 non-null  int64         \n",
      " 22  is_night                        749999 non-null  int64         \n",
      " 23  ads_per_minute                  749999 non-null  float64       \n",
      " 24  total_popularity                749999 non-null  float64       \n",
      " 25  popularity_ratio                749999 non-null  float64       \n",
      " 26  popularity_diff                 749999 non-null  float64       \n",
      " 27  missing_guest_x_host_pop        749999 non-null  float64       \n",
      " 28  missing_length_x_ads            749999 non-null  float64       \n",
      " 29  missing_guest_x_episode_length  749999 non-null  float64       \n",
      " 30  weekend_x_primetime             749999 non-null  int64         \n",
      " 31  weekend_x_ads                   749999 non-null  float64       \n",
      " 32  sentiment_numeric               749999 non-null  float64       \n",
      " 33  sentiment_x_guest_pop           749999 non-null  float64       \n",
      " 34  sentiment_x_host_pop            749999 non-null  float64       \n",
      " 35  negative_sentiment_x_ads        749999 non-null  float64       \n",
      " 36  episode_title_len               749999 non-null  int64         \n",
      " 37  podcast_name_len                749999 non-null  int64         \n",
      " 38  episode_title_words             749999 non-null  int64         \n",
      " 39  podcast_name_words              749999 non-null  int64         \n",
      " 40  title_has_numbers               749999 non-null  int64         \n",
      " 41  title_has_special               749999 non-null  int64         \n",
      " 42  is_special_episode              749999 non-null  int64         \n",
      " 43  title_uppercase_ratio           749999 non-null  float64       \n",
      " 44  podcast_frequency               749999 non-null  int64         \n",
      " 45  genre_frequency                 749999 non-null  int64         \n",
      " 46  podcast_frequency_norm          749999 non-null  float64       \n",
      " 47  genre_frequency_norm            749999 non-null  float64       \n",
      " 48  podcast_avg_listening           749999 non-null  float64       \n",
      " 49  podcast_med_listening           749999 non-null  float64       \n",
      " 50  podcast_std_listening           749999 non-null  float64       \n",
      " 51  podcast_min_listening           749999 non-null  float64       \n",
      " 52  podcast_max_listening           749999 non-null  float64       \n",
      " 53  podcast_avg_length              749999 non-null  float64       \n",
      " 54  podcast_med_length              749999 non-null  float64       \n",
      " 55  podcast_avg_ads                 749999 non-null  float64       \n",
      " 56  podcast_med_ads                 749999 non-null  float64       \n",
      " 57  podcast_host_pop                749999 non-null  float64       \n",
      " 58  podcast_avg_guest_pop           749999 non-null  float64       \n",
      " 59  genre_avg_listening             749999 non-null  float64       \n",
      " 60  genre_med_listening             749999 non-null  float64       \n",
      " 61  genre_std_listening             749999 non-null  float64       \n",
      " 62  genre_avg_guest_pop             749999 non-null  float64       \n",
      " 63  genre_avg_length                749999 non-null  float64       \n",
      " 64  length_vs_podcast_avg           749999 non-null  float64       \n",
      " 65  ads_vs_podcast_avg              749999 non-null  float64       \n",
      " 66  guest_pop_vs_podcast_avg        749999 non-null  float64       \n",
      " 67  length_vs_genre_avg             749999 non-null  float64       \n",
      " 68  above_podcast_avg_length        749999 non-null  int64         \n",
      " 69  above_genre_avg_length          749999 non-null  int64         \n",
      " 70  Podcast_Name_target_enc         749999 non-null  float64       \n",
      " 71  Genre_target_enc                749999 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(46), int64(19), string(6)\n",
      "memory usage: 412.0 MB\n",
      "None\n",
      "\n",
      "=== BASIC STATS ===\n",
      "                  id  Episode_Length_minutes  Host_Popularity_percentage  \\\n",
      "count  749999.000000           749999.000000               749999.000000   \n",
      "mean   374999.670440               64.428395                   59.859944   \n",
      "min         0.000000                6.780000                    1.300000   \n",
      "25%    187499.500000               39.420000                   39.410000   \n",
      "50%    375000.000000               63.840000                   60.050000   \n",
      "75%    562499.500000               90.310000                   79.530000   \n",
      "max    749999.000000              118.890000                  119.460000   \n",
      "std    216506.589306               30.972535                   22.873082   \n",
      "\n",
      "       Guest_Popularity_percentage  Number_of_Ads  Listening_Time_minutes  \\\n",
      "count                749999.000000  749999.000000           749999.000000   \n",
      "mean                     52.236453       1.347904               45.437435   \n",
      "min                       0.000000       0.000000                0.000000   \n",
      "25%                      34.550000       0.000000               23.178350   \n",
      "50%                      52.236453       1.000000               43.379460   \n",
      "75%                      71.040000       2.000000               64.811580   \n",
      "max                     119.910000       3.000000              119.970000   \n",
      "std                      25.531627       1.110976               27.138313   \n",
      "\n",
      "       Episode_Length_missing  Guest_Pop_missing pub_datetime  day_of_week  \\\n",
      "count           749999.000000      749999.000000            0          0.0   \n",
      "mean                 0.116124           0.194707          NaT          NaN   \n",
      "min                  0.000000           0.000000          NaT          NaN   \n",
      "25%                  0.000000           0.000000          NaT          NaN   \n",
      "50%                  0.000000           0.000000          NaT          NaN   \n",
      "75%                  0.000000           0.000000          NaT          NaN   \n",
      "max                  1.000000           1.000000          NaT          NaN   \n",
      "std                  0.320374           0.395975          NaN          NaN   \n",
      "\n",
      "       ...  genre_avg_guest_pop  genre_avg_length  length_vs_podcast_avg  \\\n",
      "count  ...        749999.000000     749999.000000          749999.000000   \n",
      "mean   ...            52.236453         64.428395               0.984709   \n",
      "min    ...            51.433338         63.365055               0.099079   \n",
      "25%    ...            51.845216         64.011668               0.602185   \n",
      "50%    ...            52.086649         64.458545               0.975222   \n",
      "75%    ...            52.626684         64.725454               1.378168   \n",
      "max    ...            53.156679         65.746383               1.911928   \n",
      "std    ...             0.550997          0.633601               0.473270   \n",
      "\n",
      "       ads_vs_podcast_avg  guest_pop_vs_podcast_avg  length_vs_genre_avg  \\\n",
      "count       749999.000000             749999.000000        749999.000000   \n",
      "mean             0.573857                  0.981213             0.984715   \n",
      "min              0.000000                  0.000000             0.101579   \n",
      "25%              0.000000                  0.648984             0.602506   \n",
      "50%              0.434266                  0.982244             0.975274   \n",
      "75%              0.869941                  1.332389             1.379906   \n",
      "max              1.345118                  2.285994             1.847120   \n",
      "std              0.472874                  0.479553             0.473329   \n",
      "\n",
      "       above_podcast_avg_length  above_genre_avg_length  \\\n",
      "count             749999.000000           749999.000000   \n",
      "mean                   0.475450                0.459335   \n",
      "min                    0.000000                0.000000   \n",
      "25%                    0.000000                0.000000   \n",
      "50%                    0.000000                0.000000   \n",
      "75%                    1.000000                1.000000   \n",
      "max                    1.000000                1.000000   \n",
      "std                    0.499397                0.498344   \n",
      "\n",
      "       Podcast_Name_target_enc  Genre_target_enc  \n",
      "count            749999.000000     749999.000000  \n",
      "mean                 45.437293         45.437522  \n",
      "min                  41.658287         44.350059  \n",
      "25%                  44.483944         44.933337  \n",
      "50%                  45.575176         45.577619  \n",
      "75%                  46.409055         45.760232  \n",
      "max                  48.207141         46.649101  \n",
      "std                   1.395066          0.635703  \n",
      "\n",
      "[8 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN PREVIEW ===\")\n",
    "print(train.head(3))\n",
    "print(\"\\n=== TRAIN INFO ===\")\n",
    "print(train.info())\n",
    "print(\"\\n=== BASIC STATS ===\")\n",
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2549188",
   "metadata": {},
   "source": [
    "Trening w AutoGluon z optymalizacją"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c81b6e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (749999, 72)\n",
      "Test shape: (250000, 71)\n",
      "\n",
      "Train po usunięciu: (749999, 68)\n",
      "Test po usunięciu: (250000, 67)\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "\n",
    "# Wczytaj dane\n",
    "train = pd.read_csv(\"data-generated/train_advanced_features.csv\")\n",
    "test = pd.read_csv(\"data-generated/test_advanced_features.csv\")\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Usuń kolumny, które nie powinny być w treningu\n",
    "drop_cols = [\"id\", \"pub_datetime\", \"Publication_Day\", \"Publication_Time\"]\n",
    "train_features = train.drop(columns=[col for col in drop_cols if col in train.columns])\n",
    "test_features = test.drop(columns=[col for col in drop_cols if col in test.columns])\n",
    "\n",
    "print(f\"\\nTrain po usunięciu: {train_features.shape}\")\n",
    "print(f\"Test po usunięciu: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988ac4a",
   "metadata": {},
   "source": [
    "Konfiguracja AutoGluon z zaawansowanymi parametrami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb64f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #32~20.04.1-Ubuntu SMP Mon Jan 9 18:02:08 UTC 2023\n",
      "CPU Count:          4\n",
      "Memory Avail:       11.09 GB / 15.44 GB (71.8%)\n",
      "Disk Space Avail:   89.56 GB / 131.62 GB (68.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "2025-10-20 17:33:44,885\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-10-20 17:33:58,206\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/jbiedrzy/workspace/dsc-pjatk-audio-engagement/models/autogluon_advanced/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Beginning AutoGluon training ... Time limit = 881s\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m AutoGluon will save models to \"/home/jbiedrzy/workspace/dsc-pjatk-audio-engagement/models/autogluon_advanced/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Train Data Rows:    666665\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Train Data Columns: 67\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Label Column:       Listening_Time_minutes\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tAvailable Memory:                    10542.58 MB\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tTrain Data (Original)  Memory Usage: 489.41 MB (4.6% of available memory)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tUseless Original Features (Count: 21): ['day_of_week', 'hour', 'month', 'day_of_month', 'is_weekend', 'is_primetime', 'is_morning', 'is_night', 'weekend_x_primetime', 'weekend_x_ads', 'sentiment_numeric', 'sentiment_x_guest_pop', 'sentiment_x_host_pop', 'negative_sentiment_x_ads', 'episode_title_words', 'title_has_numbers', 'title_has_special', 'is_special_episode', 'podcast_min_listening', 'podcast_med_length', 'podcast_med_ads']\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t('float', [])  : 33 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'ads_per_minute', ...]\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t('int', [])    :  9 | ['Episode_Length_missing', 'Guest_Pop_missing', 'episode_title_len', 'podcast_name_len', 'podcast_name_words', ...]\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t('object', []) :  4 | ['Podcast_Name', 'Episode_Title', 'Genre', 'Episode_Sentiment']\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t('category', [])  :  4 | ['Podcast_Name', 'Episode_Title', 'Genre', 'Episode_Sentiment']\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t('float', [])     : 33 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'ads_per_minute', ...]\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t('int', [])       :  5 | ['episode_title_len', 'podcast_name_len', 'podcast_name_words', 'podcast_frequency', 'genre_frequency']\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t\t('int', ['bool']) :  4 | ['Episode_Length_missing', 'Guest_Pop_missing', 'above_podcast_avg_length', 'above_genre_avg_length']\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t5.4s = Fit runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t46 features in original data used to generate 46 features in processed data.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tTrain Data (Processed) Memory Usage: 198.37 MB (1.9% of available memory)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Data preprocessing and feature engineering runtime = 6.44s ...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'ag_args': {'name_suffix': 'Custom'}}],\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t'CAT': [{}],\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t'XGB': [{}],\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t'RF': [{'criterion': 'squared_error', 'max_depth': 20, 'ag_args': {'name_suffix': 'Deep'}}],\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t'XT': [{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE'}}],\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Excluded models: [] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting 7 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 582.83s of the 874.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=12.97%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5224)\u001b[0m [1000]\tvalid_set's rmse: 13.0175\n",
      "\u001b[36m(_ray_fit pid=5222)\u001b[0m [1000]\tvalid_set's rmse: 13.1137\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5221)\u001b[0m \tRan out of time, early stopping on iteration 1611. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=5221)\u001b[0m \t[1594]\tvalid_set's rmse: 13.0182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5438)\u001b[0m [1000]\tvalid_set's rmse: 13.0658\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5438)\u001b[0m [2000]\tvalid_set's rmse: 13.0602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5438)\u001b[0m \tRan out of time, early stopping on iteration 2561. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5438)\u001b[0m \t[2280]\tvalid_set's rmse: 13.0586\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t-13.0526\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t474.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t157.29s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 33.38s of the 325.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=13.33%)\n",
      "\u001b[36m(_ray_fit pid=5665)\u001b[0m \tRan out of time, early stopping on iteration 28. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=5665)\u001b[0m \t[28]\tvalid_set's rmse: 14.3565\n",
      "\u001b[36m(_ray_fit pid=5799)\u001b[0m \tRan out of time, early stopping on iteration 50. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5799)\u001b[0m \t[50]\tvalid_set's rmse: 13.2832\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t-14.2685\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t29.65s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t1.84s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 290.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.957, 'LightGBM_BAG_L1': 0.043}\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t-13.0499\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t0.43s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Excluded models: [] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting 7 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 289.92s of the 289.81s of remaining time.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=13.88%)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t-13.0691\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t113.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t16.82s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 170.16s of the 170.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=13.93%)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t-13.0364\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t85.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t6.93s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting model: LightGBMCustom_BAG_L2 ... Training model for up to 78.92s of the 78.81s of remaining time.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=14.31%)\n",
      "\u001b[36m(_ray_fit pid=6564)\u001b[0m \tRan out of time, early stopping on iteration 54. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=6564)\u001b[0m \t[54]\tvalid_set's rmse: 13.8211\n",
      "\u001b[36m(_ray_fit pid=6710)\u001b[0m \tRan out of time, early stopping on iteration 117. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6710)\u001b[0m \t[117]\tvalid_set's rmse: 13.0823\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t-13.6357\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t66.82s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t4.75s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 5.89s of remaining time.\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.625, 'LightGBMXT_BAG_L1': 0.25, 'LightGBMXT_BAG_L2': 0.125}\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t-13.033\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t1.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m AutoGluon training complete, total runtime = 876.56s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 729.1 rows/s (133333 batch size)\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jbiedrzy/workspace/dsc-pjatk-audio-engagement/models/autogluon_advanced/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=4889)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                   model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        LightGBM_BAG_L2     -12.946405 -13.036432  root_mean_squared_error       42.531507     166.064896  589.961512                 1.968496                6.934007          85.742411            2       True          5\n",
      "1    WeightedEnsemble_L3     -12.951284 -13.033011  root_mean_squared_error       47.512689     182.893026  703.983140                 0.005736                0.012690           1.011373            3       True          7\n",
      "2      LightGBMXT_BAG_L1     -12.971997 -13.052579  root_mean_squared_error       40.017139     157.287746  474.566272                40.017139              157.287746         474.566272            1       True          1\n",
      "3    WeightedEnsemble_L2     -12.972776 -13.049920  root_mean_squared_error       40.567482     159.142755  504.647736                 0.004470                0.011866           0.428635            2       True          3\n",
      "4      LightGBMXT_BAG_L2     -12.984319 -13.069060  root_mean_squared_error       45.538458     175.946329  617.229356                 4.975446               16.815440         113.010255            2       True          4\n",
      "5  LightGBMCustom_BAG_L2     -13.489134 -13.635746  root_mean_squared_error       42.233001     163.885112  571.037806                 1.669990                4.754223          66.818705            2       True          6\n",
      "6        LightGBM_BAG_L1     -14.134257 -14.268535  root_mean_squared_error        0.545873       1.843143   29.652829                 0.545873                1.843143          29.652829            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t950s\t = DyStack   runtime |\t2650s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2650s\n",
      "AutoGluon will save models to \"/home/jbiedrzy/workspace/dsc-pjatk-audio-engagement/models/autogluon_advanced\"\n",
      "Train Data Rows:    749999\n",
      "Train Data Columns: 67\n",
      "Label Column:       Listening_Time_minutes\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10482.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 550.57 MB (5.3% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 5.3% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['day_of_week', 'hour', 'month', 'day_of_month', 'is_weekend', 'is_primetime', 'is_morning', 'is_night', 'weekend_x_primetime', 'weekend_x_ads', 'sentiment_numeric', 'sentiment_x_guest_pop', 'sentiment_x_host_pop', 'negative_sentiment_x_ads', 'episode_title_words', 'title_has_numbers', 'title_has_special', 'is_special_episode', 'podcast_min_listening', 'podcast_med_length', 'podcast_med_ads']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 33 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'ads_per_minute', ...]\n",
      "\t\t('int', [])    :  9 | ['Episode_Length_missing', 'Guest_Pop_missing', 'episode_title_len', 'podcast_name_len', 'podcast_name_words', ...]\n",
      "\t\t('object', []) :  4 | ['Podcast_Name', 'Episode_Title', 'Genre', 'Episode_Sentiment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  4 | ['Podcast_Name', 'Episode_Title', 'Genre', 'Episode_Sentiment']\n",
      "\t\t('float', [])     : 33 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'ads_per_minute', ...]\n",
      "\t\t('int', [])       :  5 | ['episode_title_len', 'podcast_name_len', 'podcast_name_words', 'podcast_frequency', 'genre_frequency']\n",
      "\t\t('int', ['bool']) :  4 | ['Episode_Length_missing', 'Guest_Pop_missing', 'above_podcast_avg_length', 'above_genre_avg_length']\n",
      "\t5.8s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 223.16 MB (2.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 6.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'ag_args': {'name_suffix': 'Custom'}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'RF': [{'criterion': 'squared_error', 'max_depth': 20, 'ag_args': {'name_suffix': 'Deep'}}],\n",
      "\t'XT': [{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: [] (Specified by `excluded_model_types`)\n",
      "Fitting 7 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1761.71s of the 2643.22s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=13.68%)\n",
      "\t-13.0308\t = Validation score   (-root_mean_squared_error)\n",
      "\t748.05s\t = Training   runtime\n",
      "\t209.43s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 970.50s of the 1852.01s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=13.69%)\n",
      "\t-12.9907\t = Validation score   (-root_mean_squared_error)\n",
      "\t711.49s\t = Training   runtime\n",
      "\t238.35s\t = Validation runtime\n",
      "Fitting model: LightGBMCustom_BAG_L1 ... Training model for up to 223.52s of the 1105.03s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=14.05%)\n",
      "\t-13.0032\t = Validation score   (-root_mean_squared_error)\n",
      "\t186.82s\t = Training   runtime\n",
      "\t26.06s\t = Validation runtime\n",
      "Fitting model: RandomForestDeep_BAG_L1 ... Training model for up to 25.86s of the 907.37s of remaining time.\n",
      "\tWarning: Model is expected to require 2222.1s to train, which exceeds the maximum time limit of 24.7s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestDeep_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 875.10s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.5, 'LightGBMCustom_BAG_L1': 0.4, 'LightGBMXT_BAG_L1': 0.1}\n",
      "\t-12.9557\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Excluded models: [] (Specified by `excluded_model_types`)\n",
      "Fitting 7 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 874.34s of the 874.21s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=14.82%)\n",
      "\t-12.9957\t = Validation score   (-root_mean_squared_error)\n",
      "\t100.29s\t = Training   runtime\n",
      "\t11.95s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 767.12s of the 766.99s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=14.83%)\n",
      "\t-12.9607\t = Validation score   (-root_mean_squared_error)\n",
      "\t76.26s\t = Training   runtime\n",
      "\t4.99s\t = Validation runtime\n",
      "Fitting model: LightGBMCustom_BAG_L2 ... Training model for up to 685.59s of the 685.45s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=15.20%)\n",
      "\t-12.9704\t = Validation score   (-root_mean_squared_error)\n",
      "\t146.21s\t = Training   runtime\n",
      "\t13.89s\t = Validation runtime\n",
      "Fitting model: RandomForestDeep_BAG_L2 ... Training model for up to 532.23s of the 532.10s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 50 due to low time. Expected time usage reduced from 3156.9s -> 530.8s...\n",
      "\t-13.0675\t = Validation score   (-root_mean_squared_error)\n",
      "\t537.08s\t = Training   runtime\n",
      "\t11.34s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -17.34s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.261, 'LightGBMCustom_BAG_L2': 0.261, 'LightGBMCustom_BAG_L1': 0.217, 'RandomForestDeep_BAG_L2': 0.174, 'LightGBMXT_BAG_L1': 0.043, 'LightGBM_BAG_L2': 0.043}\n",
      "\t-12.9444\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2668.97s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 303.0 rows/s (150000 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jbiedrzy/workspace/dsc-pjatk-audio-engagement/models/autogluon_advanced\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trening zakończony (EZZZZ????)\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=\"Listening_Time_minutes\",\n",
    "    eval_metric=\"root_mean_squared_error\",\n",
    "    problem_type=\"regression\",\n",
    "    path=\"models/autogluon_advanced\"  # Zapisz modele\n",
    ").fit(\n",
    "    train_data=train_features,\n",
    "    # time_limit=3600,  # 1 godzina\n",
    "    presets=\"best_quality\",\n",
    "    num_bag_folds=5,  # K-fold bagging dla stabilności\n",
    "    num_bag_sets=1,\n",
    "    num_stack_levels=1,  # Stacking dla lepszych predykcji\n",
    "    hyperparameters={\n",
    "        'GBM': [\n",
    "            {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n",
    "            {},  # Default LightGBM\n",
    "            {'learning_rate': 0.03, 'num_leaves': 128, 'ag_args': {'name_suffix': 'Custom'}},\n",
    "        ],\n",
    "        'CAT': {},  # CatBoost\n",
    "        'XGB': {},  # XGBoost\n",
    "        'RF': [\n",
    "            {'criterion': 'squared_error', 'max_depth': 20, 'ag_args': {'name_suffix': 'Deep'}},\n",
    "        ],\n",
    "        'XT': [  # Extra Trees\n",
    "            {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE'}},\n",
    "        ],\n",
    "    },\n",
    "    excluded_model_types=['KNN', 'NN_TORCH'],  # Usuń słabe modele dla regresji\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# moze da sie jakos lepiej dobrac parametry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d7727",
   "metadata": {},
   "source": [
    "Analiza modelu - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0439a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['day_of_week', 'hour', 'month', 'day_of_month', 'is_weekend', 'is_primetime', 'is_morning', 'is_night', 'weekend_x_primetime', 'weekend_x_ads', 'sentiment_numeric', 'sentiment_x_guest_pop', 'sentiment_x_host_pop', 'negative_sentiment_x_ads', 'episode_title_words', 'title_has_numbers', 'title_has_special', 'is_special_episode', 'podcast_min_listening', 'podcast_med_length', 'podcast_med_ads']\n",
      "Computing feature importance via permutation shuffling for 46 features using 5000 rows with 5 shuffle sets...\n",
      "\t1606.22s\t= Expected runtime (321.24s per shuffle set)\n",
      "\t1368.28s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP 20 NAJWAŻNIEJSZYCH FEATURES ===\n",
      "                                importance    stddev       p_value  n  \\\n",
      "Episode_Length_minutes           15.413678  0.224082  5.358714e-09  5   \n",
      "length_vs_genre_avg               1.551974  0.062060  3.061643e-07  5   \n",
      "Episode_Title                     1.129664  0.076337  2.487102e-06  5   \n",
      "Podcast_Name                      0.822746  0.069175  5.940585e-06  5   \n",
      "ads_per_minute                    0.710582  0.100078  4.599167e-05  5   \n",
      "Host_Popularity_percentage        0.549408  0.042835  4.398192e-06  5   \n",
      "Episode_Length_missing            0.521614  0.050632  1.052093e-05  5   \n",
      "length_vs_podcast_avg             0.383378  0.028407  3.590806e-06  5   \n",
      "Guest_Popularity_percentage       0.231754  0.023376  1.225415e-05  5   \n",
      "Episode_Sentiment                 0.131353  0.024774  1.449097e-04  5   \n",
      "guest_pop_vs_podcast_avg          0.126991  0.011756  8.712472e-06  5   \n",
      "popularity_diff                   0.104842  0.006421  1.680308e-06  5   \n",
      "total_popularity                  0.104469  0.010963  1.434019e-05  5   \n",
      "missing_length_x_ads              0.104253  0.010229  1.098206e-05  5   \n",
      "above_genre_avg_length            0.094784  0.015078  7.431866e-05  5   \n",
      "ads_vs_podcast_avg                0.078687  0.015396  1.672593e-04  5   \n",
      "popularity_ratio                  0.072353  0.006195  6.388731e-06  5   \n",
      "missing_guest_x_episode_length    0.056848  0.005189  8.235907e-06  5   \n",
      "Podcast_Name_target_enc           0.044755  0.010820  3.799188e-04  5   \n",
      "Genre_target_enc                  0.043961  0.007933  1.218898e-04  5   \n",
      "\n",
      "                                 p99_high    p99_low  \n",
      "Episode_Length_minutes          15.875066  14.952291  \n",
      "length_vs_genre_avg              1.679755   1.424192  \n",
      "Episode_Title                    1.286844   0.972484  \n",
      "Podcast_Name                     0.965178   0.680314  \n",
      "ads_per_minute                   0.916644   0.504520  \n",
      "Host_Popularity_percentage       0.637606   0.461211  \n",
      "Episode_Length_missing           0.625867   0.417362  \n",
      "length_vs_podcast_avg            0.441868   0.324888  \n",
      "Guest_Popularity_percentage      0.279885   0.183622  \n",
      "Episode_Sentiment                0.182363   0.080342  \n",
      "guest_pop_vs_podcast_avg         0.151196   0.102786  \n",
      "popularity_diff                  0.118064   0.091621  \n",
      "total_popularity                 0.127041   0.081897  \n",
      "missing_length_x_ads             0.125316   0.083190  \n",
      "above_genre_avg_length           0.125830   0.063739  \n",
      "ads_vs_podcast_avg               0.110388   0.046986  \n",
      "popularity_ratio                 0.085109   0.059596  \n",
      "missing_guest_x_episode_length   0.067532   0.046165  \n",
      "Podcast_Name_target_enc          0.067034   0.022475  \n",
      "Genre_target_enc                 0.060294   0.027627  \n",
      "Feature importance zapisane do: feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "importance = predictor.feature_importance(train_features)\n",
    "print(\"\\n=== TOP 20 NAJWAŻNIEJSZYCH FEATURES ===\")\n",
    "print(importance.head(20))\n",
    "\n",
    "# Zapisz importance\n",
    "importance.to_csv(\"feature_importance.csv\")\n",
    "print(\"Feature importance zapisane do: feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22f148",
   "metadata": {},
   "source": [
    "24. Leaderboard modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ef552ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LEADERBOARD MODELI ===\n",
      "                     model  score_test  score_val              eval_metric  \\\n",
      "0          LightGBM_BAG_L1  -11.569711 -12.990736  root_mean_squared_error   \n",
      "1  RandomForestDeep_BAG_L2  -11.867125 -13.067457  root_mean_squared_error   \n",
      "2      WeightedEnsemble_L3  -11.910536 -12.944424  root_mean_squared_error   \n",
      "3    LightGBMCustom_BAG_L2  -11.934366 -12.970425  root_mean_squared_error   \n",
      "4      WeightedEnsemble_L2  -11.939326 -12.955695  root_mean_squared_error   \n",
      "5          LightGBM_BAG_L2  -11.994666 -12.960717  root_mean_squared_error   \n",
      "6        LightGBMXT_BAG_L1  -12.075988 -13.030786  root_mean_squared_error   \n",
      "7        LightGBMXT_BAG_L2  -12.176237 -12.995737  root_mean_squared_error   \n",
      "8    LightGBMCustom_BAG_L1  -12.443026 -13.003249  root_mean_squared_error   \n",
      "\n",
      "   pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
      "0      367.475880     238.353383   711.494490               367.475880   \n",
      "1      803.108906     485.183036  2183.444448                 4.112515   \n",
      "2      847.427173     504.070153  2407.248533                 0.042484   \n",
      "3      831.629162     487.731365  1792.575594                32.632771   \n",
      "4      799.019236     473.860394  1647.040182                 0.022845   \n",
      "5      810.639403     478.832776  1722.621936                11.643012   \n",
      "6      378.298349     209.429559   748.051417               378.298349   \n",
      "7      828.558017     485.790218  1746.658064                29.561625   \n",
      "8       53.222162      26.062049   186.817472                53.222162   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0              238.353383         711.494490            1       True   \n",
      "1               11.338046         537.081070            2       True   \n",
      "2                0.012956           1.333310            3       True   \n",
      "3               13.886375         146.212216            2       True   \n",
      "4                0.015404           0.676804            2       True   \n",
      "5                4.987785          76.258558            2       True   \n",
      "6              209.429559         748.051417            1       True   \n",
      "7               11.945228         100.294685            2       True   \n",
      "8               26.062049         186.817472            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          2  \n",
      "1          8  \n",
      "2          9  \n",
      "3          7  \n",
      "4          4  \n",
      "5          6  \n",
      "6          1  \n",
      "7          5  \n",
      "8          3  \n",
      "RMSE najlepszego modelu: -12.9907\n"
     ]
    }
   ],
   "source": [
    "leaderboard = predictor.leaderboard(train_features, silent=True)\n",
    "print(\"\\n=== LEADERBOARD MODELI ===\")\n",
    "print(leaderboard)\n",
    "\n",
    "# print(f\"\\nNajlepszy model: {predictor.get_model_best()}\")\n",
    "print(f\"RMSE najlepszego modelu: {leaderboard['score_val'].iloc[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476d7db",
   "metadata": {},
   "source": [
    "Predykcja na test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "373e451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STATYSTYKI PREDYKCJI ===\n",
      "Min: 0.51\n",
      "Max: 113.51\n",
      "Mean: 45.46\n",
      "Median: 44.62\n"
     ]
    }
   ],
   "source": [
    "# Predykcja\n",
    "predictions = predictor.predict(test_features)\n",
    "\n",
    "# Sprawdź statystyki predykcji\n",
    "print(\"\\n=== STATYSTYKI PREDYKCJI ===\")\n",
    "print(f\"Min: {predictions.min():.2f}\")\n",
    "print(f\"Max: {predictions.max():.2f}\")\n",
    "print(f\"Mean: {predictions.mean():.2f}\")\n",
    "print(f\"Median: {predictions.median():.2f}\")\n",
    "\n",
    "# Sprawdź czy są wartości ujemne (błąd!)\n",
    "if (predictions < 0).any():\n",
    "    print(f\"UWAGA: {(predictions < 0).sum()} predykcji ujemnych! Clipowanie do 0...\")\n",
    "    predictions = predictions.clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9459a8d",
   "metadata": {},
   "source": [
    "Zapisanie submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a9c5008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission zapisany do: submission.csv\n",
      "Shape: (250000, 2)\n",
      "\n",
      "Pierwsze 5 wierszy:\n",
      "       id  Listening_Time_minutes\n",
      "0  750000               53.896751\n",
      "1  750001               18.823473\n",
      "2  750002               49.512531\n",
      "3  750003               81.293648\n",
      "4  750004               47.252350\n"
     ]
    }
   ],
   "source": [
    "# Stwórz submission\n",
    "submission = test[[\"id\"]].copy()\n",
    "submission[\"Listening_Time_minutes\"] = predictions\n",
    "\n",
    "# Zapisz\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nSubmission zapisany do: submission.csv\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(\"\\nPierwsze 5 wierszy:\")\n",
    "print(submission.head())\n",
    "\n",
    "#hopefully malutki rmse (plsplspls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff263b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdz:\n",
    "# jaki jest rozklad danych?\n",
    "# w sensie jak sie rozkladaja wszystkie waerosci w kolumnach\n",
    "\n",
    "# dodalbym zamiane tytułu podcastu na jakies embeddingi, zeby model mial wiecej informacji o tresci\n",
    "# proces embeddingow:\n",
    "# 1. SentenceTransformers zeby zamienic tkst na wektor\n",
    "# 2. sprawdzic czy są w jakims stopniu podobne (np cosine similarity)\n",
    "# 3. cluterowanie na podstawie similarity threshhold np: pary z similarity > 0.8 sa w tym samym clusterze\n",
    "# 4. dodanie do modelu jako feature, np cluster i potem caly embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
