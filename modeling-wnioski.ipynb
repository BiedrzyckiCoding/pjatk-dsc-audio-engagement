{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a447f7a",
   "metadata": {},
   "source": [
    "Feature Engineering + modelowanie\n",
    "\n",
    "**Cel:** transformacja danych na podstawie wniosnów z EDA i trenig modelu autogluonem\n",
    "\n",
    "**Plan:**\n",
    "1. Preprocessing (missing, outliers, duplikaty)\n",
    "2. Time features (cyclic encoding)\n",
    "3. Embeddingi z tytułów\n",
    "4. Agregacje per podcast/genre\n",
    "5. Target encoding\n",
    "6. Trening AutoGluon i analiza wyników\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38272f6",
   "metadata": {},
   "source": [
    "Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb4b0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformers niedostępny - użyję TF-IDF\n",
      "Train shape: (750000, 12)\n",
      "Test shape: (250000, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import mstats\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sprawdź dostępność SentenceTransformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    EMBEDDINGS_AVAILABLE = True\n",
    "    print(\"SentenceTransformers dostępny!\")\n",
    "except ImportError:\n",
    "    EMBEDDINGS_AVAILABLE = False\n",
    "    print(\"SentenceTransformers niedostępny - użyję TF-IDF\")\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ścieżki\n",
    "TRAIN_PATH = Path(\"data-task/train.csv\")\n",
    "TEST_PATH = Path(\"data-task/test.csv\")\n",
    "\n",
    "# Wczytaj dane\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb3084",
   "metadata": {},
   "source": [
    "Preprocessing: Flagi dla missing\n",
    "\n",
    "**Dlaczego:**\n",
    "- z eda wiemy, że braki w Guest_Popularity to około 19% a Episode_length to około 11%\n",
    "- Test ma braki w tych samych miejscach, więc fagi będą pomocne\n",
    "\n",
    "**Hipoteza:**\n",
    "- Brak gościa = podcast solo -> inny wzorzec słuchalności\n",
    "- model nauczy się: \"Gdy Guest_Pop_missing = 1 -> przewiduj x minut\n",
    "\n",
    "Możliwe, że to obniży RMSE o jakąś część procenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066d2109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagi dodane!\n",
      "Train - Episode_Length_missing: 87093\n",
      "Test - Episode_Length_missing: 28736\n",
      "Train - Guest_Pop_missing: 146030\n",
      "Test - Guest_Pop_missing: 48832\n"
     ]
    }
   ],
   "source": [
    "# Flagi dla Episode_Length_minutes\n",
    "train[\"Episode_Length_missing\"] = train[\"Episode_Length_minutes\"].isnull().astype(int)\n",
    "test[\"Episode_Length_missing\"] = test[\"Episode_Length_minutes\"].isnull().astype(int)\n",
    "\n",
    "# Flagi dla Guest_Popularity_percentage\n",
    "train[\"Guest_Pop_missing\"] = train[\"Guest_Popularity_percentage\"].isnull().astype(int)\n",
    "test[\"Guest_Pop_missing\"] = test[\"Guest_Popularity_percentage\"].isnull().astype(int)\n",
    "\n",
    "print(\"Flagi dodane!\")\n",
    "print(f\"Train - Episode_Length_missing: {train['Episode_Length_missing'].sum()}\")\n",
    "print(f\"Test - Episode_Length_missing: {test['Episode_Length_missing'].sum()}\")\n",
    "print(f\"Train - Guest_Pop_missing: {train['Guest_Pop_missing'].sum()}\")\n",
    "print(f\"Test - Guest_Pop_missing: {test['Guest_Pop_missing'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29954f8",
   "metadata": {},
   "source": [
    "Usunięcie braków w number_of_ads (tylko train) bo jest jeden brak xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e216d715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train przed usunięciem: 750000\n",
      "Train po usunięciu: 749999\n",
      "Usunięto: 1 wiersz\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train przed usunięciem: {len(train)}\")\n",
    "train = train.dropna(subset=[\"Number_of_Ads\"])\n",
    "print(f\"Train po usunięciu: {len(train)}\")\n",
    "print(f\"Usunięto: {1} wiersz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e09c9a",
   "metadata": {},
   "source": [
    "Imputacja brakujących wartośći\n",
    "\n",
    "**Strategia z EDA:**\n",
    "- Episode_Length_minutes: mediana - prawostronna skośność\n",
    "- Guest_Popularity_percentage: średnia - rozkład stmetryczny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da5ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode_Length median: 63.84\n",
      "Guest_Popularity mean: 52.24\n",
      "\n",
      "Imputation zakończony!\n",
      "Train braki: 0\n",
      "Test braki: 0\n"
     ]
    }
   ],
   "source": [
    "# Oblicz statystyki na train\n",
    "ep_len_median = train[\"Episode_Length_minutes\"].median()\n",
    "guest_pop_mean = train[\"Guest_Popularity_percentage\"].mean()\n",
    "\n",
    "print(f\"Episode_Length median: {ep_len_median:.2f}\")\n",
    "print(f\"Guest_Popularity mean: {guest_pop_mean:.2f}\")\n",
    "\n",
    "# Wypełnij braki\n",
    "train[\"Episode_Length_minutes\"].fillna(ep_len_median, inplace=True)\n",
    "test[\"Episode_Length_minutes\"].fillna(ep_len_median, inplace=True)\n",
    "\n",
    "train[\"Guest_Popularity_percentage\"].fillna(guest_pop_mean, inplace=True)\n",
    "test[\"Guest_Popularity_percentage\"].fillna(guest_pop_mean, inplace=True)\n",
    "\n",
    "print(\"\\nImputation zakończony!\")\n",
    "print(f\"Train braki: {train.isnull().sum().sum()}\")\n",
    "print(f\"Test braki: {test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46e4af",
   "metadata": {},
   "source": [
    "Wywalanie outlierów\n",
    "\n",
    "**Dlaczego?**\n",
    "- bo jest ich kilka sztuk\n",
    "- mają duzy wpływ na RMSE\n",
    "\n",
    "Moze obnizyc RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99596f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode_Length_minutes: usunięto 1 outlierów (pozostało 749998)\n",
      "Number_of_Ads: usunięto 9 outlierów (pozostało 749989)\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers_iqr(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    before = df.shape[0]\n",
    "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    after = df.shape[0]\n",
    "\n",
    "    print(f\"{col}: usunięto {before - after} outlierów (pozostało {after})\")\n",
    "    return df\n",
    "\n",
    "# Usuwanie outlierów dla kluczowych zmiennych\n",
    "train_clean = remove_outliers_iqr(train, \"Episode_Length_minutes\")\n",
    "train_clean = remove_outliers_iqr(train_clean, \"Number_of_Ads\")\n",
    "\n",
    "# Dopasowanie testu do zakresu treningu (opcjonalne clipping)\n",
    "Q1_ep, Q3_ep = train_clean[\"Episode_Length_minutes\"].quantile([0.25, 0.75])\n",
    "IQR_ep = Q3_ep - Q1_ep\n",
    "ep_lower, ep_upper = Q1_ep - 1.5 * IQR_ep, Q3_ep + 1.5 * IQR_ep\n",
    "\n",
    "Q1_ads, Q3_ads = train_clean[\"Number_of_Ads\"].quantile([0.25, 0.75])\n",
    "IQR_ads = Q3_ads - Q1_ads\n",
    "ads_lower, ads_upper = Q1_ads - 1.5 * IQR_ads, Q3_ads + 1.5 * IQR_ads\n",
    "\n",
    "test[\"Episode_Length_minutes\"] = test[\"Episode_Length_minutes\"].clip(ep_lower, ep_upper)\n",
    "test[\"Number_of_Ads\"] = test[\"Number_of_Ads\"].clip(ads_lower, ads_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1f1c0",
   "metadata": {},
   "source": [
    "Usunięcie duplikatów (tylko train, zeby test mial dobra ilosc wierszy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec2e313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train przed usunięciem duplikatów: 749999\n",
      "Train po usunięciu duplikatów: 749999\n",
      "Test (bez zmian): 250000 wierszy\n",
      "Test ma poprawną liczbę wierszy\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train przed usunięciem duplikatów: {len(train)}\")\n",
    "train = train.drop_duplicates()\n",
    "print(f\"Train po usunięciu duplikatów: {len(train)}\")\n",
    "print(f\"Test (bez zmian): {len(test)} wierszy\")\n",
    "\n",
    "# WALIDACJA - test musi mieć 250k wierszy!\n",
    "assert len(test) == 250000, f\"Test ma {len(test)}, powinien mieć 250000!\"\n",
    "print(\"Test ma poprawną liczbę wierszy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd466d3f",
   "metadata": {},
   "source": [
    "Parsowanie czasu publikacji -> numery -> cyclic encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4425cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsowanie czasu zakończone!\n",
      "Przykładowe wartości (train):\n",
      "  Publication_Day  day_of_week_num   day_sin   day_cos\n",
      "0        Thursday                3  0.433884 -0.900969\n",
      "1        Saturday                5 -0.974928 -0.222521\n",
      "2         Tuesday                1  0.781831  0.623490\n",
      "  Publication_Time  hour_num  hour_sin  hour_cos\n",
      "0            Night         2  0.500000  0.866025\n",
      "1        Afternoon        14 -0.500000 -0.866025\n",
      "2          Evening        20 -0.866025  0.500000\n"
     ]
    }
   ],
   "source": [
    "# Mapowanie dni tygodnia\n",
    "day_mapping = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "    'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "}\n",
    "\n",
    "# Mapowanie pory dnia na godziny (środek przedziału)\n",
    "time_mapping = {\n",
    "    'Morning': 8, # 6-12h\n",
    "    'Afternoon': 14, # 12-18h\n",
    "    'Evening': 20, # 18-24h\n",
    "    'Night': 2  # 24-6h (środek nocy)\n",
    "}\n",
    "\n",
    "for df in [train, test]:\n",
    "    # Zamiana na numery\n",
    "    df[\"day_of_week_num\"] = df[\"Publication_Day\"].map(day_mapping)\n",
    "    df[\"hour_num\"] = df[\"Publication_Time\"].map(time_mapping)\n",
    "    \n",
    "    # CYCLIC ENCODING dla dnia tygodnia\n",
    "    df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_week_num\"] / 7)\n",
    "    df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_week_num\"] / 7)\n",
    "    \n",
    "    # CYCLIC ENCODING dla godziny\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour_num\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour_num\"] / 24)\n",
    "    \n",
    "    # Binarne features\n",
    "    df[\"is_weekend\"] = (df[\"day_of_week_num\"] >= 5).astype(int)\n",
    "    df[\"is_morning\"] = (df[\"hour_num\"] >= 6) & (df[\"hour_num\"] < 12)\n",
    "    df[\"is_afternoon\"] = (df[\"hour_num\"] >= 12) & (df[\"hour_num\"] < 18)\n",
    "    df[\"is_evening\"] = (df[\"hour_num\"] >= 18) & (df[\"hour_num\"] < 24)\n",
    "    df[\"is_night\"] = (df[\"hour_num\"] < 6) | (df[\"hour_num\"] >= 22)\n",
    "    df[\"is_primetime\"] = (df[\"hour_num\"] >= 17) & (df[\"hour_num\"] <= 21)\n",
    "\n",
    "print(\"Parsowanie czasu zakończone!\")\n",
    "print(f\"Przykładowe wartości (train):\")\n",
    "print(train[['Publication_Day', 'day_of_week_num', 'day_sin', 'day_cos']].head(3))\n",
    "print(train[['Publication_Time', 'hour_num', 'hour_sin', 'hour_cos']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781206f",
   "metadata": {},
   "source": [
    "Feature engineering - Podstawowe interakcje\n",
    "\n",
    "**Dlaczego te features:**\n",
    "- z EDA wiemy, że Episode_length ma najsilniejszą korelację z targetem\n",
    "- Popularity (host + guest) też jest istotne\n",
    "- interakcje mogą wychwycić nieliniowe zależności\n",
    "\n",
    "Może Dalej zmniejszymy RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce396ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podstawowe features dodane! Train shape: (749999, 40)\n"
     ]
    }
   ],
   "source": [
    "for df in [train, test]:\n",
    "    # === Interakcje numeryczne ===\n",
    "    df[\"ads_per_minute\"] = df[\"Number_of_Ads\"] / (df[\"Episode_Length_minutes\"] + 1)\n",
    "    df[\"total_popularity\"] = df[\"Host_Popularity_percentage\"] + df[\"Guest_Popularity_percentage\"]\n",
    "    df[\"popularity_ratio\"] = df[\"Host_Popularity_percentage\"] / (df[\"Guest_Popularity_percentage\"] + 1)\n",
    "    df[\"popularity_diff\"] = df[\"Host_Popularity_percentage\"] - df[\"Guest_Popularity_percentage\"]\n",
    "    \n",
    "    # === Interakcje z flagami missing ===\n",
    "    # Jeśli brak gościa, popularność hosta jest WAŻNIEJSZA\n",
    "    df[\"missing_guest_x_host_pop\"] = df[\"Guest_Pop_missing\"] * df[\"Host_Popularity_percentage\"]\n",
    "    df[\"missing_length_x_ads\"] = df[\"Episode_Length_missing\"] * df[\"Number_of_Ads\"]\n",
    "    df[\"missing_guest_x_episode_length\"] = df[\"Guest_Pop_missing\"] * df[\"Episode_Length_minutes\"]\n",
    "    \n",
    "    # === Interakcje czasowe ===\n",
    "    # Wieczór + długi odcinek = więcej słuchania?\n",
    "    df[\"length_x_evening\"] = df[\"Episode_Length_minutes\"] * df[\"is_evening\"].astype(int)\n",
    "    df[\"host_pop_x_weekend\"] = df[\"Host_Popularity_percentage\"] * df[\"is_weekend\"]\n",
    "    df[\"ads_x_primetime\"] = df[\"Number_of_Ads\"] * df[\"is_primetime\"].astype(int)\n",
    "    \n",
    "    # === Sentiment jako numeric ===\n",
    "    sentiment_map = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    df[\"sentiment_numeric\"] = df[\"Episode_Sentiment\"].map(sentiment_map).fillna(0)\n",
    "    \n",
    "    df[\"sentiment_x_guest_pop\"] = df[\"sentiment_numeric\"] * df[\"Guest_Popularity_percentage\"]\n",
    "    df[\"sentiment_x_host_pop\"] = df[\"sentiment_numeric\"] * df[\"Host_Popularity_percentage\"]\n",
    "    df[\"negative_sentiment_x_ads\"] = (df[\"sentiment_numeric\"] == -1).astype(int) * df[\"Number_of_Ads\"]\n",
    "\n",
    "print(f\"Podstawowe features dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94318b",
   "metadata": {},
   "source": [
    "Embeddingi z Episode_Title\n",
    "\n",
    "**Dlaczego embeddingi > text features:**\n",
    "- generalnie poprzednie wyznaczniki były bez sensu (długość, liczba słów, ...)\n",
    "- Embeddingi - model rozumie treść\n",
    "\n",
    "**Metoda:**\n",
    "1. SentenceTransformer (all-MiniLM-L6-v2) - szybki, 384-wymiarowy\n",
    "2. PCA → redukcja do 50 wymiarów (żeby nie przeciążyć modelu)\n",
    "3. KMeans clustering → grupowanie podobnych tytułów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d8f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Używam TF-IDF jako alternatywy...\n",
      "TF-IDF zakończone! 50 features\n"
     ]
    }
   ],
   "source": [
    "if EMBEDDINGS_AVAILABLE:\n",
    "    print(\"Generowanie embeddingów z SentenceTransformers...\")\n",
    "    print(\"To może potrwać 3-5 minut...\")\n",
    "    \n",
    "    # Model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generuj embeddingi dla train\n",
    "    train_titles = train[\"Episode_Title\"].fillna(\"\").tolist()\n",
    "    train_embeddings = model.encode(train_titles, show_progress_bar=True, batch_size=256)\n",
    "    \n",
    "    # Generuj embeddingi dla test\n",
    "    test_titles = test[\"Episode_Title\"].fillna(\"\").tolist()\n",
    "    test_embeddings = model.encode(test_titles, show_progress_bar=True, batch_size=256)\n",
    "    \n",
    "    print(f\"Embeddingi wygenerowane! Shape: {train_embeddings.shape}\")\n",
    "    \n",
    "    # PCA - redukcja do 50 wymiarów\n",
    "    pca = PCA(n_components=50, random_state=42)\n",
    "    train_embeddings_pca = pca.fit_transform(train_embeddings)\n",
    "    test_embeddings_pca = pca.transform(test_embeddings)\n",
    "    \n",
    "    print(f\"PCA zakończone! Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    \n",
    "    # Dodaj jako features\n",
    "    for i in range(50):\n",
    "        train[f\"title_emb_{i}\"] = train_embeddings_pca[:, i]\n",
    "        test[f\"title_emb_{i}\"] = test_embeddings_pca[:, i]\n",
    "    \n",
    "    # KMeans clustering - grupowanie podobnych tytułów\n",
    "    n_clusters = 20  # 20 klastrów tematycznych\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    train[\"title_cluster\"] = kmeans.fit_predict(train_embeddings_pca)\n",
    "    test[\"title_cluster\"] = kmeans.predict(test_embeddings_pca)\n",
    "    \n",
    "    print(f\"Clustering zakończony! {n_clusters} klastrów\")\n",
    "    print(f\"Rozkład klastrów (train):\\n{train['title_cluster'].value_counts().head()}\")\n",
    "    \n",
    "else:\n",
    "    # Fallback: TF-IDF\n",
    "    print(\"Używam TF-IDF jako alternatywy...\")\n",
    "    \n",
    "    tfidf = TfidfVectorizer(max_features=50, stop_words='english', ngram_range=(1, 2))\n",
    "    \n",
    "    train_tfidf = tfidf.fit_transform(train[\"Episode_Title\"].fillna(\"\"))\n",
    "    test_tfidf = tfidf.transform(test[\"Episode_Title\"].fillna(\"\"))\n",
    "    \n",
    "    # Dodaj jako features\n",
    "    for i in range(50):\n",
    "        train[f\"title_tfidf_{i}\"] = train_tfidf[:, i].toarray().flatten()\n",
    "        test[f\"title_tfidf_{i}\"] = test_tfidf[:, i].toarray().flatten()\n",
    "    \n",
    "    print(f\"TF-IDF zakończone! {train_tfidf.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84dc62c",
   "metadata": {},
   "source": [
    "Frequency Encoding\n",
    "\n",
    "**Dlaczego?**\n",
    "- z EDA wiemy, że niektóre podcasty mają po 10 odcinków, inne tysiące\n",
    "- Popularność podcastu zapewne może korelować ze słuchalnością"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0287e216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency encoding zakończony!\n",
      "Top 5 podcasts by frequency:\n",
      "Podcast_Name\n",
      "Tech Talks       22847\n",
      "Sports Weekly    20053\n",
      "Funny Folks      19635\n",
      "Tech Trends      19549\n",
      "Fitness First    19488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Podcast frequency\n",
    "podcast_freq = train[\"Podcast_Name\"].value_counts()\n",
    "train[\"podcast_frequency\"] = train[\"Podcast_Name\"].map(podcast_freq).fillna(0)\n",
    "test[\"podcast_frequency\"] = test[\"Podcast_Name\"].map(podcast_freq).fillna(0)\n",
    "\n",
    "# Genre frequency\n",
    "genre_freq = train[\"Genre\"].value_counts()\n",
    "train[\"genre_frequency\"] = train[\"Genre\"].map(genre_freq).fillna(0)\n",
    "test[\"genre_frequency\"] = test[\"Genre\"].map(genre_freq).fillna(0)\n",
    "\n",
    "# Normalizacja (0-1), żeby modele regresyjne lepiej działały\n",
    "train[\"podcast_frequency_norm\"] = train[\"podcast_frequency\"] / len(train)\n",
    "test[\"podcast_frequency_norm\"] = test[\"podcast_frequency\"] / len(train)\n",
    "\n",
    "train[\"genre_frequency_norm\"] = train[\"genre_frequency\"] / len(train)\n",
    "test[\"genre_frequency_norm\"] = test[\"genre_frequency\"] / len(train)\n",
    "\n",
    "print(\"Frequency encoding zakończony!\")\n",
    "print(f\"Top 5 podcasts by frequency:\\n{podcast_freq.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5c9fa",
   "metadata": {},
   "source": [
    "Agregacje per podcast\n",
    "\n",
    "**Dlaczego:**\n",
    "- z EDA: top podcasty mają różną średnie słuchalności (40-50 minut)\n",
    "\n",
    "**Features:**\n",
    "1. podcast_avg_listening - średnia historyczna\n",
    "2. podcast_std_listening - stabilność (niski std = stała publiczność)\n",
    "3. podcast_min/max_listening - zakres wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f524d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train przed merge: 749999\n",
      "Test przed merge: 250000\n",
      "Train po merge: 749999\n",
      "Test po merge: 250000\n",
      "Merge nie stracił wierszy\n",
      "\n",
      "Podcast stats dodane! Train shape: (749999, 105)\n"
     ]
    }
   ],
   "source": [
    "# Statystyki per Podcast (obliczane TYLKO na train!)\n",
    "podcast_stats = train.groupby(\"Podcast_Name\").agg({\n",
    "    \"Listening_Time_minutes\": [\"mean\", \"median\", \"std\", \"min\", \"max\"],\n",
    "    \"Episode_Length_minutes\": [\"mean\", \"median\"],\n",
    "    \"Number_of_Ads\": [\"mean\", \"median\"],\n",
    "    \"Host_Popularity_percentage\": \"first\",\n",
    "    \"Guest_Popularity_percentage\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# Spłaszcz kolumny\n",
    "podcast_stats.columns = [\n",
    "    \"Podcast_Name\",\n",
    "    \"podcast_avg_listening\", \"podcast_med_listening\", \"podcast_std_listening\",\n",
    "    \"podcast_min_listening\", \"podcast_max_listening\",\n",
    "    \"podcast_avg_length\", \"podcast_med_length\",\n",
    "    \"podcast_avg_ads\", \"podcast_med_ads\",\n",
    "    \"podcast_host_pop\", \"podcast_avg_guest_pop\"\n",
    "]\n",
    "\n",
    "# Wypełnij std NaN (podcasty z 1 odcinkiem)\n",
    "podcast_stats[\"podcast_std_listening\"].fillna(0, inplace=True)\n",
    "\n",
    "# Merguj z LEFT join (ważne!)\n",
    "print(f\"Train przed merge: {len(train)}\")\n",
    "print(f\"Test przed merge: {len(test)}\")\n",
    "\n",
    "train = train.merge(podcast_stats, on=\"Podcast_Name\", how=\"left\")\n",
    "test = test.merge(podcast_stats, on=\"Podcast_Name\", how=\"left\")\n",
    "\n",
    "print(f\"Train po merge: {len(train)}\")\n",
    "print(f\"Test po merge: {len(test)}\")\n",
    "\n",
    "# WALIDACJA\n",
    "assert len(test) == 250000, f\"Test stracił wiersze! Ma {len(test)}\"\n",
    "print(\"Merge nie stracił wierszy\")\n",
    "\n",
    "print(f\"\\nPodcast stats dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877053b",
   "metadata": {},
   "source": [
    "Agregacje per Genre\n",
    "\n",
    "**Dlaczego:**\n",
    "- z eda: różnice średnich między gatunkami (np. Comedy vs News)\n",
    "- Genre characteristics wpływa na słuchalność"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "399dec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test przed merge: 250000\n",
      "Test po merge: 250000\n",
      "Merge OK\n",
      "Genre stats dodane! Train shape: (749999, 110)\n"
     ]
    }
   ],
   "source": [
    "# Statystyki per Genre\n",
    "genre_stats = train.groupby(\"Genre\").agg({\n",
    "    \"Listening_Time_minutes\": [\"mean\", \"median\", \"std\"],\n",
    "    \"Guest_Popularity_percentage\": \"mean\",\n",
    "    \"Episode_Length_minutes\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "genre_stats.columns = [\n",
    "    \"Genre\",\n",
    "    \"genre_avg_listening\", \"genre_med_listening\", \"genre_std_listening\",\n",
    "    \"genre_avg_guest_pop\", \"genre_avg_length\"\n",
    "]\n",
    "\n",
    "genre_stats[\"genre_std_listening\"].fillna(0, inplace=True)\n",
    "\n",
    "# LEFT join\n",
    "print(f\"Test przed merge: {len(test)}\")\n",
    "train = train.merge(genre_stats, on=\"Genre\", how=\"left\")\n",
    "test = test.merge(genre_stats, on=\"Genre\", how=\"left\")\n",
    "print(f\"Test po merge: {len(test)}\")\n",
    "\n",
    "# WALIDACJA\n",
    "assert len(test) == 250000, f\"Test stracił wiersze!\"\n",
    "print(\"Merge OK\")\n",
    "\n",
    "print(f\"Genre stats dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c26229",
   "metadata": {},
   "source": [
    "Relative features (odcinek vs średnia)\n",
    "\n",
    "**Dlaczego:**\n",
    "- mało liczą się wartości bezwzględne, bardziej relatywne\n",
    "- np: 60-minutowy odcinek w podcaście o średniej 30 min jest wyjątkowy\n",
    "- To samo 60 min w podcaście o średniej 90 min -> krótki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c141183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative features dodane!\n"
     ]
    }
   ],
   "source": [
    "for df in [train, test]:\n",
    "    # Porównanie z podcast\n",
    "    df[\"length_vs_podcast_avg\"] = df[\"Episode_Length_minutes\"] / (df[\"podcast_avg_length\"] + 1)\n",
    "    df[\"ads_vs_podcast_avg\"] = df[\"Number_of_Ads\"] / (df[\"podcast_avg_ads\"] + 1)\n",
    "    df[\"guest_pop_vs_podcast_avg\"] = df[\"Guest_Popularity_percentage\"] / (df[\"podcast_avg_guest_pop\"] + 1)\n",
    "    \n",
    "    # Porównanie z genre\n",
    "    df[\"length_vs_genre_avg\"] = df[\"Episode_Length_minutes\"] / (df[\"genre_avg_length\"] + 1)\n",
    "    \n",
    "    # Czy ten odcinek jest powyżej/poniżej średniej?\n",
    "    df[\"above_podcast_avg_length\"] = (df[\"Episode_Length_minutes\"] > df[\"podcast_avg_length\"]).astype(int)\n",
    "    df[\"above_genre_avg_length\"] = (df[\"Episode_Length_minutes\"] > df[\"genre_avg_length\"]).astype(int)\n",
    "\n",
    "print(\"Relative features dodane!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004461f8",
   "metadata": {},
   "source": [
    "Target Encoding\n",
    "\n",
    "**Dlaczego:**\n",
    "- High cardinality (Podcast_name: około 100 różnych wartości) - one hot nie możliwy\n",
    "- Target encoding - mapowanie kategorii na średni target\n",
    "\n",
    "Problem w postaci data leakage:\n",
    "- model podgląda odpowiedzi\n",
    "\n",
    "Mozliwe rozwiązanie: K-Fold CV\n",
    "- Dla każdego fold: obliczamy średnią na pozostałych foldach\n",
    "- Bayesian smoothing, regularycacja dla rzadkich kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b33abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding w toku (może potrwać ze 2 minuty)...\n",
      "Podcast_Name zakończony\n",
      "Genre zakończony\n",
      "\n",
      "Target encoding zakończony! Train shape: (749999, 118)\n",
      "Target encoding w toku (może potrwać ~2min)...\n",
      "Podcast_Name zakończony\n",
      "Genre zakończony\n",
      "\n",
      "Target encoding zakończony! Train shape: (749999, 118)\n"
     ]
    }
   ],
   "source": [
    "def target_encode_with_cv(train_df, test_df, cat_col, target_col, n_splits=5, smoothing=10):\n",
    "    \"\"\"\n",
    "    Target encoding z K-Fold CV (leak protection) + Bayesian smoothing\n",
    "    \n",
    "    Parametry:\n",
    "    - smoothing: im wyższy, tym bardziej zbliżamy się do global_mean dla rzadkich kategorii\n",
    "    \"\"\"\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    \n",
    "    # Inicjalizacja\n",
    "    train_df[f\"{cat_col}_target_enc\"] = global_mean\n",
    "    \n",
    "    # K-Fold CV dla train\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(train_df):\n",
    "        train_fold = train_df.iloc[train_idx]\n",
    "        \n",
    "        # Oblicz statystyki na foldzie treningowym\n",
    "        agg = train_fold.groupby(cat_col)[target_col].agg(['mean', 'count'])\n",
    "        \n",
    "        # Bayesian smoothing: (count * mean + smoothing * global_mean) / (count + smoothing)\n",
    "        smoothed = (agg['count'] * agg['mean'] + smoothing * global_mean) / (agg['count'] + smoothing)\n",
    "        \n",
    "        # Mapuj na fold walidacyjny\n",
    "        train_df.loc[val_idx, f\"{cat_col}_target_enc\"] = train_df.loc[val_idx, cat_col].map(smoothed).fillna(global_mean)\n",
    "    \n",
    "    # Dla test użyj całego train\n",
    "    agg_full = train_df.groupby(cat_col)[target_col].agg(['mean', 'count'])\n",
    "    smoothed_full = (agg_full['count'] * agg_full['mean'] + smoothing * global_mean) / (agg_full['count'] + smoothing)\n",
    "    test_df[f\"{cat_col}_target_enc\"] = test_df[cat_col].map(smoothed_full).fillna(global_mean)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Zastosuj target encoding\n",
    "print(\"Target encoding w toku (może potrwać ze 2 minuty)...\")\n",
    "\n",
    "for col in [\"Podcast_Name\", \"Genre\"]:\n",
    "    train, test = target_encode_with_cv(train, test, col, \"Listening_Time_minutes\", smoothing=10)\n",
    "    print(f\"{col} zakończony\")\n",
    "\n",
    "print(f\"\\nTarget encoding zakończony! Train shape: {train.shape}\")\n",
    "\n",
    "# Zastosuj target encoding\n",
    "print(\"Target encoding w toku (może potrwać ~2min)...\")\n",
    "\n",
    "for col in [\"Podcast_Name\", \"Genre\"]:\n",
    "    train, test = target_encode_with_cv(train, test, col, \"Listening_Time_minutes\", smoothing=10)\n",
    "    print(f\"{col} zakończony\")\n",
    "\n",
    "print(f\"\\nTarget encoding zakończony! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae64e0",
   "metadata": {},
   "source": [
    "Konwersja typów dla autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73dee142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konwersja typów zakończona!\n",
      "\n",
      "Train dtypes:\n",
      "float64           97\n",
      "int64             10\n",
      "string[python]     6\n",
      "bool               5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in train.select_dtypes(include=[\"object\"]).columns:\n",
    "    if col in train.columns:\n",
    "        train[col] = train[col].astype(\"string\")\n",
    "    if col in test.columns:\n",
    "        test[col] = test[col].astype(\"string\")\n",
    "\n",
    "print(\"Konwersja typów zakończona!\")\n",
    "print(f\"\\nTrain dtypes:\\n{train.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7e0f2",
   "metadata": {},
   "source": [
    "wypelnianie NaN z mergowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48482626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train final missing: 0\n",
      "Test final missing: 0\n"
     ]
    }
   ],
   "source": [
    "# Dla nowych podcastów/gatunków w test, które nie były w train\n",
    "fill_cols = [col for col in train.columns if 'podcast_' in col or 'genre_' in col]\n",
    "\n",
    "for col in fill_cols:\n",
    "    if col in train.columns and col in test.columns:\n",
    "        train[col].fillna(0, inplace=True)\n",
    "        test[col].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Train final missing: {train.isnull().sum().sum()}\")\n",
    "print(f\"Test final missing: {test.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf64b7f",
   "metadata": {},
   "source": [
    "Finalne podsumowanie features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280632fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINALNE STATYSTYKI\n",
      "============================================================\n",
      "Train shape: (749999, 118)\n",
      "Test shape: (250000, 117)\n",
      "\n",
      "Liczba features: 117\n",
      "\n",
      "Dodano 106 nowych features\n",
      "\n",
      "Przykładowe nowe features:\n",
      "  - Episode_Length_missing\n",
      "  - Genre_target_enc\n",
      "  - Guest_Pop_missing\n",
      "  - Podcast_Name_target_enc\n",
      "  - above_genre_avg_length\n",
      "  - above_podcast_avg_length\n",
      "  - ads_per_minute\n",
      "  - ads_vs_podcast_avg\n",
      "  - ads_x_primetime\n",
      "  - day_cos\n",
      "  - day_of_week_num\n",
      "  - day_sin\n",
      "  - genre_avg_guest_pop\n",
      "  - genre_avg_length\n",
      "  - genre_avg_listening\n",
      "  - genre_frequency\n",
      "  - genre_frequency_norm\n",
      "  - genre_med_listening\n",
      "  - genre_std_listening\n",
      "  - guest_pop_vs_podcast_avg\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINALNE STATYSTYKI\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nLiczba features: {train.shape[1] - 1}\")\n",
    "\n",
    "# Nowe features\n",
    "new_features = [col for col in train.columns if col not in pd.read_csv(TRAIN_PATH).columns]\n",
    "print(f\"\\nDodano {len(new_features)} nowych features\")\n",
    "print(\"\\nPrzykładowe nowe features:\")\n",
    "for feat in sorted(new_features)[:20]:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef38e1e",
   "metadata": {},
   "source": [
    "Zapisanie przetworzonych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd827a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pliki zapisane:\n",
      "Train: data-task\\train_final_features.csv\n",
      "Test: data-task\\test_final_features.csv\n",
      "\n",
      "Gotowe do trenowania w AutoGluon!\n"
     ]
    }
   ],
   "source": [
    "output_train = Path(\"data-task/train_final_features.csv\")\n",
    "output_test = Path(\"data-task/test_final_features.csv\")\n",
    "\n",
    "train.to_csv(output_train, index=False)\n",
    "test.to_csv(output_test, index=False)\n",
    "\n",
    "print(f\"\\nPliki zapisane:\")\n",
    "print(f\"Train: {output_train}\")\n",
    "print(f\"Test: {output_test}\")\n",
    "print(f\"\\nGotowe do trenowania w AutoGluon!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f533b4",
   "metadata": {},
   "source": [
    "Przygotowanie danych do AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d38910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train po usunięciu: (749999, 113)\n",
      "Test po usunięciu: (250000, 112)\n",
      "Test ma poprawną liczbę wierszy\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Usuń kolumny, które nie powinny być w treningu\n",
    "drop_cols = [\"id\", \"Publication_Day\", \"Publication_Time\", \"Episode_Title\", \"Podcast_Name\"]\n",
    "train_features = train.drop(columns=[col for col in drop_cols if col in train.columns])\n",
    "test_features = test.drop(columns=[col for col in drop_cols if col in test.columns])\n",
    "\n",
    "print(f\"Train po usunięciu: {train_features.shape}\")\n",
    "print(f\"Test po usunięciu: {test_features.shape}\")\n",
    "\n",
    "# WALIDACJA FINALNA\n",
    "assert len(test_features) == 250000, f\"Test ma {len(test_features)}, powinien 250000!\"\n",
    "print(\"Test ma poprawną liczbę wierszy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9655502",
   "metadata": {},
   "source": [
    "Trening AutoGluon z optymalizacją\n",
    "\n",
    "**Konfiguracja:**\n",
    "- presets=best_quality – najlepsze modele (LightGBM, CatBoost, XGBoost, RF)\n",
    "- num_bag_folds=5 – K-fold bagging dla stabilności (redukcja overfittingu)\n",
    "- num_stack_levels=1 – Stacking (meta-model łączy predykcje)\n",
    "- Custom hyperparameters dla różnych wariantów LightGBM\n",
    "\n",
    "**Spodziewany czas:** ~45–60 minut (1h time_limit), potem dam 3 godziny itd\n",
    "\n",
    "**Spodziewany RMSE:** ~12.5–13.5 (na validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4149f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/autogluon_final\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          6\n",
      "Memory Avail:       1.29 GB / 15.92 GB (8.1%)\n",
      "Disk Space Avail:   18.15 GB / 237.20 GB (7.7%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam trening AutoGluon...\n",
      "To zajmie ~45-60 minut...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 13:24:22,583\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-10-28 13:24:28,282\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"c:\\Users\\Admin\\Desktop\\github_repos\\pjatk-dsc-audio-engagement\\models\\autogluon_final\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Beginning AutoGluon training ... Time limit = 890s\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m AutoGluon will save models to \"c:\\Users\\Admin\\Desktop\\github_repos\\pjatk-dsc-audio-engagement\\models\\autogluon_final\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Train Data Rows:    666665\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Train Data Columns: 112\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Label Column:       Listening_Time_minutes\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tAvailable Memory:                    1630.89 MB\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tTrain Data (Original)  Memory Usage: 619.30 MB (38.0% of available memory)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tWarning: Data size prior to feature transformation consumes 38.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t\tNote: Converting 59 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tUseless Original Features (Count: 7): ['sentiment_numeric', 'sentiment_x_guest_pop', 'sentiment_x_host_pop', 'negative_sentiment_x_ads', 'podcast_min_listening', 'podcast_med_length', 'podcast_med_ads']\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tUnused Original Features (Count: 25): ['is_primetime', 'title_tfidf_25', 'title_tfidf_26', 'title_tfidf_27', 'title_tfidf_28', 'title_tfidf_29', 'title_tfidf_31', 'title_tfidf_32', 'title_tfidf_33', 'title_tfidf_34', 'title_tfidf_35', 'title_tfidf_36', 'title_tfidf_37', 'title_tfidf_38', 'title_tfidf_39', 'title_tfidf_40', 'title_tfidf_41', 'title_tfidf_42', 'title_tfidf_43', 'title_tfidf_44', 'title_tfidf_45', 'title_tfidf_46', 'title_tfidf_47', 'title_tfidf_48', 'title_tfidf_49']\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('bool', [])  :  1 | ['is_primetime']\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('float', []) : 24 | ['title_tfidf_25', 'title_tfidf_26', 'title_tfidf_27', 'title_tfidf_28', 'title_tfidf_29', ...]\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('bool', [])   :  4 | ['is_morning', 'is_afternoon', 'is_evening', 'is_night']\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('float', [])  : 65 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'day_sin', ...]\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('int', [])    :  9 | ['Episode_Length_missing', 'Guest_Pop_missing', 'day_of_week_num', 'hour_num', 'is_weekend', ...]\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('object', []) :  2 | ['Genre', 'Episode_Sentiment']\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('category', [])  :  2 | ['Genre', 'Episode_Sentiment']\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('float', [])     : 40 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'day_sin', ...]\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('int', [])       :  4 | ['day_of_week_num', 'hour_num', 'podcast_frequency', 'genre_frequency']\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\t('int', ['bool']) : 34 | ['Episode_Length_missing', 'Guest_Pop_missing', 'is_weekend', 'is_morning', 'is_afternoon', ...]\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t10.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t80 features in original data used to generate 80 features in processed data.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tTrain Data (Processed) Memory Usage: 246.68 MB (6.9% of available memory)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Data preprocessing and feature engineering runtime = 11.26s ...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'ag_args': {'name_suffix': 'Custom'}}],\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t'CAT': [{}],\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t'XGB': [{}],\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t'RF': [{'criterion': 'squared_error', 'max_depth': 20, 'ag_args': {'name_suffix': 'Deep'}}],\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t'XT': [{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE'}}],\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Excluded models: [] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting 7 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 585.75s of the 878.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 58.77% memory usage per fold, 58.77%/80.00% total).\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=58.77%)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5984)\u001b[0m [1000]\tvalid_set's rmse: 13.0386\n",
      "\u001b[36m(_ray_fit pid=5984)\u001b[0m [2000]\tvalid_set's rmse: 13.0064\n",
      "\u001b[36m(_ray_fit pid=5984)\u001b[0m [3000]\tvalid_set's rmse: 12.9866\n",
      "\u001b[36m(_ray_fit pid=5984)\u001b[0m [4000]\tvalid_set's rmse: 12.9754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=5984)\u001b[0m \tRan out of time, early stopping on iteration 4976. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=5984)\u001b[0m \t[4948]\tvalid_set's rmse: 12.967\n",
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m \tWarning: Potentially not enough memory to safely train model. Estimated to require 1.050 GB out of 1.324 GB available memory (79.264%)... (90.000% of avail memory is the max safe size)\n",
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m \tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.11 to avoid the warning)\n",
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m [1000]\tvalid_set's rmse: 13.0898\n",
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m [2000]\tvalid_set's rmse: 13.0566\n",
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m [3000]\tvalid_set's rmse: 13.0381\n",
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m [4000]\tvalid_set's rmse: 13.0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m \tRan out of time, early stopping on iteration 4506. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=15172)\u001b[0m \t[4506]\tvalid_set's rmse: 13.0268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2756)\u001b[0m [1000]\tvalid_set's rmse: 13.1138\n",
      "\u001b[36m(_ray_fit pid=2756)\u001b[0m [2000]\tvalid_set's rmse: 13.082\n",
      "\u001b[36m(_ray_fit pid=2756)\u001b[0m [3000]\tvalid_set's rmse: 13.0629\n",
      "\u001b[36m(_ray_fit pid=2756)\u001b[0m [4000]\tvalid_set's rmse: 13.0531\n",
      "\u001b[36m(_ray_fit pid=2756)\u001b[0m [5000]\tvalid_set's rmse: 13.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2756)\u001b[0m \tRan out of time, early stopping on iteration 5587. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2756)\u001b[0m \t[5438]\tvalid_set's rmse: 13.047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=22164)\u001b[0m [1000]\tvalid_set's rmse: 13.0356\n",
      "\u001b[36m(_ray_fit pid=22164)\u001b[0m [2000]\tvalid_set's rmse: 13.0053\n",
      "\u001b[36m(_ray_fit pid=22164)\u001b[0m [3000]\tvalid_set's rmse: 12.9857\n",
      "\u001b[36m(_ray_fit pid=22164)\u001b[0m [4000]\tvalid_set's rmse: 12.9741\n",
      "\u001b[36m(_ray_fit pid=22164)\u001b[0m [5000]\tvalid_set's rmse: 12.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=22164)\u001b[0m \tRan out of time, early stopping on iteration 5652. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=22164)\u001b[0m \t[5187]\tvalid_set's rmse: 12.9685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=18676)\u001b[0m [1000]\tvalid_set's rmse: 13.0746\n",
      "\u001b[36m(_ray_fit pid=18676)\u001b[0m [2000]\tvalid_set's rmse: 13.0458\n",
      "\u001b[36m(_ray_fit pid=18676)\u001b[0m [3000]\tvalid_set's rmse: 13.0293\n",
      "\u001b[36m(_ray_fit pid=18676)\u001b[0m [4000]\tvalid_set's rmse: 13.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=18676)\u001b[0m \tRan out of time, early stopping on iteration 4276. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=18676)\u001b[0m \t[4271]\tvalid_set's rmse: 13.0174\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t-13.0054\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t539.35s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t59.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 29.18s of the 322.28s of remaining time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 46.96% memory usage per fold, 46.96%/80.00% total).\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=46.96%)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=18240)\u001b[0m \tRan out of time, early stopping on iteration 69. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=18240)\u001b[0m \t[69]\tvalid_set's rmse: 13.1366\n",
      "\u001b[36m(_ray_fit pid=18944)\u001b[0m \tRan out of time, early stopping on iteration 61. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=18944)\u001b[0m \t[61]\tvalid_set's rmse: 13.2143\n",
      "\u001b[36m(_ray_fit pid=21556)\u001b[0m \tRan out of time, early stopping on iteration 51. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=21556)\u001b[0m \t[51]\tvalid_set's rmse: 13.3399\n",
      "\u001b[36m(_ray_fit pid=14420)\u001b[0m \tRan out of time, early stopping on iteration 66. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=14420)\u001b[0m \t[66]\tvalid_set's rmse: 13.1354\n",
      "\u001b[36m(_ray_fit pid=10272)\u001b[0m \tRan out of time, early stopping on iteration 68. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=10272)\u001b[0m \t[68]\tvalid_set's rmse: 13.1755\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t-13.2005\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t38.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t0.96s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 278.93s of remaining time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.857, 'LightGBM_BAG_L1': 0.143}\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t-12.9995\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t0.53s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Excluded models: [] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting 7 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 278.34s of the 278.22s of remaining time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 52.49% memory usage per fold, 52.49%/80.00% total).\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=52.49%)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=22152)\u001b[0m [1000]\tvalid_set's rmse: 12.9659\n",
      "\u001b[36m(_ray_fit pid=18388)\u001b[0m [1000]\tvalid_set's rmse: 12.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t-13.0122\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t138.06s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t6.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 134.33s of the 134.20s of remaining time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 53.28% memory usage per fold, 53.28%/80.00% total).\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=53.28%)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=19676)\u001b[0m \tRan out of time, early stopping on iteration 510. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=19676)\u001b[0m \t[508]\tvalid_set's rmse: 12.9574\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t-12.9822\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t84.79s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t2.9s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting model: LightGBMCustom_BAG_L2 ... Training model for up to 44.31s of the 44.18s of remaining time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 51.15% memory usage per fold, 51.15%/80.00% total).\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=51.15%)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\u001b[36m(_ray_fit pid=17912)\u001b[0m \tRan out of time, early stopping on iteration 64. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=17912)\u001b[0m \t[64]\tvalid_set's rmse: 13.3963\n",
      "\u001b[36m(_ray_fit pid=5536)\u001b[0m \tRan out of time, early stopping on iteration 62. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=5536)\u001b[0m \t[62]\tvalid_set's rmse: 13.487\n",
      "\u001b[36m(_ray_fit pid=7948)\u001b[0m \tRan out of time, early stopping on iteration 69. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=7948)\u001b[0m \t[69]\tvalid_set's rmse: 13.292\n",
      "\u001b[36m(_ray_fit pid=20040)\u001b[0m \tRan out of time, early stopping on iteration 59. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=20040)\u001b[0m \t[59]\tvalid_set's rmse: 13.6196\n",
      "\u001b[36m(_ray_fit pid=14180)\u001b[0m \tRan out of time, early stopping on iteration 52. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=14180)\u001b[0m \t[52]\tvalid_set's rmse: 13.9064\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t-13.5419\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t51.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t1.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -12.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.833, 'LightGBMXT_BAG_L1': 0.083, 'LightGBMXT_BAG_L2': 0.083}\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t-12.9816\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t1.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m AutoGluon training complete, total runtime = 909.25s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1920.4 rows/s (133333 batch size)\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\Admin\\Desktop\\github_repos\\pjatk-dsc-audio-engagement\\models\\autogluon_final\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=8800)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                   model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        LightGBM_BAG_L2     -12.925013 -12.982248  root_mean_squared_error       53.612231      63.310239  662.521040                 1.506002                2.903833          84.794514            2       True          5\n",
      "1    WeightedEnsemble_L3     -12.925671 -12.981630  root_mean_squared_error       58.035591      69.445168  802.320014                 0.006001                0.018000           1.737416            3       True          7\n",
      "2      LightGBMXT_BAG_L1     -12.939392 -13.005387  root_mean_squared_error       51.518227      59.442634  539.345870                51.518227               59.442634         539.345870            1       True          1\n",
      "3    WeightedEnsemble_L2     -12.943693 -12.999543  root_mean_squared_error       52.109226      60.420402  578.252084                 0.002998                0.013996           0.525558            2       True          3\n",
      "4      LightGBMXT_BAG_L2     -12.950224 -13.012219  root_mean_squared_error       56.523588      66.523335  715.788084                 4.417359                6.116929         138.061558            2       True          4\n",
      "5        LightGBM_BAG_L1     -13.159932 -13.200550  root_mean_squared_error        0.588001       0.963772   38.380656                 0.588001                0.963772          38.380656            1       True          2\n",
      "6  LightGBMCustom_BAG_L2     -13.488728 -13.541941  root_mean_squared_error       52.842212      61.409427  629.282318                 0.735983                1.003021          51.555792            2       True          6\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t993s\t = DyStack   runtime |\t2607s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2607s\n",
      "AutoGluon will save models to \"c:\\Users\\Admin\\Desktop\\github_repos\\pjatk-dsc-audio-engagement\\models\\autogluon_final\"\n",
      "Train Data Rows:    749999\n",
      "Train Data Columns: 112\n",
      "Label Column:       Listening_Time_minutes\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3168.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 696.70 MB (22.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 22.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 59 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 7): ['sentiment_numeric', 'sentiment_x_guest_pop', 'sentiment_x_host_pop', 'negative_sentiment_x_ads', 'podcast_min_listening', 'podcast_med_length', 'podcast_med_ads']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 25): ['is_primetime', 'title_tfidf_25', 'title_tfidf_26', 'title_tfidf_27', 'title_tfidf_28', 'title_tfidf_29', 'title_tfidf_31', 'title_tfidf_32', 'title_tfidf_33', 'title_tfidf_34', 'title_tfidf_35', 'title_tfidf_36', 'title_tfidf_37', 'title_tfidf_38', 'title_tfidf_39', 'title_tfidf_40', 'title_tfidf_41', 'title_tfidf_42', 'title_tfidf_43', 'title_tfidf_44', 'title_tfidf_45', 'title_tfidf_46', 'title_tfidf_47', 'title_tfidf_48', 'title_tfidf_49']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])  :  1 | ['is_primetime']\n",
      "\t\t('float', []) : 24 | ['title_tfidf_25', 'title_tfidf_26', 'title_tfidf_27', 'title_tfidf_28', 'title_tfidf_29', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  4 | ['is_morning', 'is_afternoon', 'is_evening', 'is_night']\n",
      "\t\t('float', [])  : 65 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'day_sin', ...]\n",
      "\t\t('int', [])    :  9 | ['Episode_Length_missing', 'Guest_Pop_missing', 'day_of_week_num', 'hour_num', 'is_weekend', ...]\n",
      "\t\t('object', []) :  2 | ['Genre', 'Episode_Sentiment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  2 | ['Genre', 'Episode_Sentiment']\n",
      "\t\t('float', [])     : 40 | ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'day_sin', ...]\n",
      "\t\t('int', [])       :  4 | ['day_of_week_num', 'hour_num', 'podcast_frequency', 'genre_frequency']\n",
      "\t\t('int', ['bool']) : 34 | ['Episode_Length_missing', 'Guest_Pop_missing', 'is_weekend', 'is_morning', 'is_afternoon', ...]\n",
      "\t11.6s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 277.52 MB (7.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 14.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'ag_args': {'name_suffix': 'Custom'}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'RF': [{'criterion': 'squared_error', 'max_depth': 20, 'ag_args': {'name_suffix': 'Deep'}}],\n",
      "\t'XT': [{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: [] (Specified by `excluded_model_types`)\n",
      "Fitting 7 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1727.97s of the 2592.60s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 51.30% memory usage per fold, 51.30%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=51.30%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t-12.9771\t = Validation score   (-root_mean_squared_error)\n",
      "\t1386.33s\t = Training   runtime\n",
      "\t283.39s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 257.01s of the 1121.63s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 73.72% memory usage per fold, 73.72%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=73.72%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t-13.016\t = Validation score   (-root_mean_squared_error)\n",
      "\t234.7s\t = Training   runtime\n",
      "\t12.26s\t = Validation runtime\n",
      "Fitting model: LightGBMCustom_BAG_L1 ... Training model for up to 12.79s of the 877.42s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 58.52% memory usage per fold, 58.52%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=58.52%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t-26.5191\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.24s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 839.19s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.6, 'LightGBM_BAG_L1': 0.4}\n",
      "\t-12.9459\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Excluded models: [] (Specified by `excluded_model_types`)\n",
      "Fitting 7 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 837.61s of the 837.44s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 68.35% memory usage per fold, 68.35%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=68.35%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t-12.9594\t = Validation score   (-root_mean_squared_error)\n",
      "\t268.64s\t = Training   runtime\n",
      "\t9.51s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 560.14s of the 559.97s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 60.58% memory usage per fold, 60.58%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=60.58%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t-12.935\t = Validation score   (-root_mean_squared_error)\n",
      "\t140.45s\t = Training   runtime\n",
      "\t3.62s\t = Validation runtime\n",
      "Fitting model: LightGBMCustom_BAG_L2 ... Training model for up to 411.32s of the 411.15s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 63.48% memory usage per fold, 63.48%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=63.48%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t-12.9317\t = Validation score   (-root_mean_squared_error)\n",
      "\t285.57s\t = Training   runtime\n",
      "\t6.39s\t = Validation runtime\n",
      "Fitting model: RandomForestDeep_BAG_L2 ... Training model for up to 116.34s of the 116.17s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 175 due to low memory. Expected memory usage reduced from 25.69% -> 15.0% of available memory...\n",
      "\tWarning: Model is expected to require 3215.5s to train, which exceeds the maximum time limit of 113.3s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestDeep_BAG_L2.\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 38.96s of the 38.79s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 2.197 GB out of 2.317 GB available memory (94.793%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.31 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 94.79% memory usage per fold, 94.79%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=6, gpus=0, memory=94.79%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=10616, ip=127.0.0.1)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\catboost\\plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=10616, ip=127.0.0.1)\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1888, in ray._raylet.execute_task\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1996, in ray._raylet.execute_task\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1895, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogluon\\tabular\\models\\catboost\\catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\autogluon\\common\\utils\\try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 27.52s of the 27.35s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 164 due to low memory. Expected memory usage reduced from 27.38% -> 15.0% of available memory...\n",
      "\tWarning: Model is expected to require 1154.9s to train, which exceeds the maximum time limit of 25.0s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesMSE_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -3.99s of remaining time.\n",
      "\tFailed to import torch or check CUDA availability!Please ensure you have the correct version of PyTorch installed by running `pip install -U torch`\n",
      "\tEnsemble Weights: {'LightGBMCustom_BAG_L2': 0.6, 'LightGBM_BAG_L2': 0.2, 'LightGBMXT_BAG_L1': 0.133, 'LightGBM_BAG_L1': 0.067}\n",
      "\t-12.9288\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2614.0s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 489.8 rows/s (150000 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\Admin\\Desktop\\github_repos\\pjatk-dsc-audio-engagement\\models\\autogluon_final\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trening zakończony! (ezzzzzzz)\n"
     ]
    }
   ],
   "source": [
    "print(\"Rozpoczynam trening AutoGluon...\")\n",
    "print(\"To zajmie ~45-60 minut...\")\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=\"Listening_Time_minutes\",\n",
    "    eval_metric=\"root_mean_squared_error\",\n",
    "    problem_type=\"regression\",\n",
    "    path=\"models/autogluon_final\"\n",
    ").fit(\n",
    "    train_data=train_features,\n",
    "    time_limit=3600,  # 3 godziny\n",
    "    presets=\"best_quality\",\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=1,\n",
    "    num_stack_levels=1,\n",
    "    hyperparameters={\n",
    "        'GBM': [\n",
    "            {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n",
    "            {},  # Default LightGBM\n",
    "            {'learning_rate': 0.03, 'num_leaves': 128, 'ag_args': {'name_suffix': 'Custom'}},\n",
    "        ],\n",
    "        'CAT': {},\n",
    "        'XGB': {},\n",
    "        'RF': [\n",
    "            {'criterion': 'squared_error', 'max_depth': 20, 'ag_args': {'name_suffix': 'Deep'}},\n",
    "        ],\n",
    "        'XT': [\n",
    "            {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE'}},\n",
    "        ],\n",
    "    },\n",
    "    excluded_model_types=['KNN', 'NN_TORCH'],\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"\\nTrening zakończony! (ezzzzzzz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241dead",
   "metadata": {},
   "source": [
    "Analiza Feature Importance\n",
    "\n",
    "**Dlaczego to ważne:**\n",
    "- Identyfikacja najsilniejszych predyktorów\n",
    "- Potwierdzenie hipotez z EDA\n",
    "- Możliwość usunięcia słabych features (jeśli potrzeba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cbd4462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['is_primetime', 'sentiment_numeric', 'sentiment_x_guest_pop', 'sentiment_x_host_pop', 'negative_sentiment_x_ads', 'title_tfidf_25', 'title_tfidf_26', 'title_tfidf_27', 'title_tfidf_28', 'title_tfidf_29', 'title_tfidf_31', 'title_tfidf_32', 'title_tfidf_33', 'title_tfidf_34', 'title_tfidf_35', 'title_tfidf_36', 'title_tfidf_37', 'title_tfidf_38', 'title_tfidf_39', 'title_tfidf_40', 'title_tfidf_41', 'title_tfidf_42', 'title_tfidf_43', 'title_tfidf_44', 'title_tfidf_45', 'title_tfidf_46', 'title_tfidf_47', 'title_tfidf_48', 'title_tfidf_49', 'podcast_min_listening', 'podcast_med_length', 'podcast_med_ads']\n",
      "Computing feature importance via permutation shuffling for 80 features using 5000 rows with 5 shuffle sets...\n",
      "\t4203.84s\t= Expected runtime (840.77s per shuffle set)\n",
      "\t1953.81s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP 30 NAJWAŻNIEJSZYCH FEATURES\n",
      "============================================================\n",
      "                             importance    stddev       p_value  n  p99_high  \\\n",
      "Episode_Length_minutes         8.197288  0.163756  1.910113e-08  5  8.534464   \n",
      "length_vs_genre_avg            2.231269  0.070635  1.203586e-07  5  2.376708   \n",
      "length_vs_podcast_avg          1.280513  0.060784  6.074256e-07  5  1.405667   \n",
      "above_genre_avg_length         0.773794  0.060600  4.477364e-06  5  0.898570   \n",
      "Host_Popularity_percentage     0.543708  0.043287  4.780460e-06  5  0.632836   \n",
      "Episode_Length_missing         0.521153  0.050105  1.012757e-05  5  0.624320   \n",
      "ads_per_minute                 0.391269  0.064661  8.633535e-05  5  0.524407   \n",
      "Genre                          0.309047  0.026971  6.890796e-06  5  0.364580   \n",
      "Guest_Popularity_percentage    0.248147  0.036515  5.467351e-05  5  0.323332   \n",
      "Episode_Sentiment              0.214651  0.025811  2.461016e-05  5  0.267795   \n",
      "guest_pop_vs_podcast_avg       0.147632  0.011435  4.284902e-06  5  0.171177   \n",
      "missing_length_x_ads           0.134942  0.023467  1.054600e-04  5  0.183260   \n",
      "day_cos                        0.118640  0.010843  8.279403e-06  5  0.140966   \n",
      "total_popularity               0.108452  0.012329  1.970417e-05  5  0.133839   \n",
      "day_sin                        0.104730  0.007173  2.623389e-06  5  0.119499   \n",
      "ads_vs_podcast_avg             0.104559  0.017866  9.843113e-05  5  0.141345   \n",
      "Number_of_Ads                  0.102051  0.023967  3.396904e-04  5  0.151399   \n",
      "day_of_week_num                0.100262  0.010668  1.515280e-05  5  0.122228   \n",
      "popularity_diff                0.092045  0.004278  5.583692e-07  5  0.100854   \n",
      "popularity_ratio               0.082040  0.009568  2.180530e-05  5  0.101741   \n",
      "host_pop_x_weekend             0.076688  0.014818  1.592450e-04  5  0.107198   \n",
      "podcast_avg_ads                0.074697  0.012266  8.419444e-05  5  0.099952   \n",
      "length_x_evening               0.073771  0.011250  6.292809e-05  5  0.096934   \n",
      "above_podcast_avg_length       0.071378  0.006667  9.026283e-06  5  0.085105   \n",
      "podcast_host_pop               0.068352  0.012210  1.171718e-04  5  0.093493   \n",
      "podcast_std_listening          0.068087  0.014330  2.221641e-04  5  0.097593   \n",
      "Podcast_Name_target_enc        0.066249  0.007647  2.092785e-05  5  0.081995   \n",
      "hour_cos                       0.061040  0.005896  1.031829e-05  5  0.073180   \n",
      "Genre_target_enc               0.060255  0.013990  3.249693e-04  5  0.089059   \n",
      "podcast_frequency              0.058761  0.010423  1.139604e-04  5  0.080221   \n",
      "\n",
      "                              p99_low  \n",
      "Episode_Length_minutes       7.860112  \n",
      "length_vs_genre_avg          2.085830  \n",
      "length_vs_podcast_avg        1.155358  \n",
      "above_genre_avg_length       0.649018  \n",
      "Host_Popularity_percentage   0.454581  \n",
      "Episode_Length_missing       0.417987  \n",
      "ads_per_minute               0.258132  \n",
      "Genre                        0.253513  \n",
      "Guest_Popularity_percentage  0.172963  \n",
      "Episode_Sentiment            0.161507  \n",
      "guest_pop_vs_podcast_avg     0.124087  \n",
      "missing_length_x_ads         0.086624  \n",
      "day_cos                      0.096315  \n",
      "total_popularity             0.083066  \n",
      "day_sin                      0.089962  \n",
      "ads_vs_podcast_avg           0.067772  \n",
      "Number_of_Ads                0.052703  \n",
      "day_of_week_num              0.078296  \n",
      "popularity_diff              0.083236  \n",
      "popularity_ratio             0.062339  \n",
      "host_pop_x_weekend           0.046179  \n",
      "podcast_avg_ads              0.049442  \n",
      "length_x_evening             0.050608  \n",
      "above_podcast_avg_length     0.057652  \n",
      "podcast_host_pop             0.043211  \n",
      "podcast_std_listening        0.038582  \n",
      "Podcast_Name_target_enc      0.050504  \n",
      "hour_cos                     0.048900  \n",
      "Genre_target_enc             0.031450  \n",
      "podcast_frequency            0.037300  \n",
      "\n",
      "Feature importance zapisane do: feature_importance_final.csv\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE PER KATEGORIA\n",
      "============================================================\n",
      "\n",
      "Embeddingi:\n",
      "Liczba features: 26\n",
      "Total importance: 0.3840\n",
      "Top 3: [0.04531229422021994, 0.03525105757791245, 0.024932913935933953]\n",
      "\n",
      "Target Encoding:\n",
      "Liczba features: 2\n",
      "Total importance: 0.1265\n",
      "Top 3: [0.0662493849924509, 0.060254747530164465]\n",
      "\n",
      "Agregacje Podcast:\n",
      "Liczba features: 14\n",
      "Total importance: 2.1308\n",
      "Top 3: [1.2805127443262887, 0.14763221346411015, 0.10455898900385989]\n",
      "\n",
      "Agregacje Genre:\n",
      "Liczba features: 9\n",
      "Total importance: 3.0793\n",
      "Top 3: [2.2312692458561596, 0.7737940205590117, 0.012765237220268944]\n",
      "\n",
      "Time Features:\n",
      "Liczba features: 8\n",
      "Total importance: 0.5405\n",
      "Top 3: [0.11864020056771948, 0.1047304691340134, 0.10026173421308862]\n",
      "\n",
      "Interakcje:\n",
      "Liczba features: 10\n",
      "Total importance: 4.1636\n",
      "Top 3: [2.2312692458561596, 1.2805127443262887, 0.14763221346411015]\n",
      "\n",
      "Missing Flags:\n",
      "Liczba features: 5\n",
      "Total importance: 0.7785\n",
      "Top 3: [0.5211530799756691, 0.13494183371556118, 0.04738593175164674]\n",
      "\n",
      "Original:\n",
      "Liczba features: 4\n",
      "Total importance: 9.0912\n",
      "Top 3: [8.197288112172014, 0.5437081916570875, 0.24814748984847376]\n"
     ]
    }
   ],
   "source": [
    "importance = predictor.feature_importance(train_features)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOP 30 NAJWAŻNIEJSZYCH FEATURES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(importance.head(30))\n",
    "\n",
    "# Zapisz\n",
    "importance.to_csv(\"feature_importance_final.csv\")\n",
    "print(\"\\nFeature importance zapisane do: feature_importance_final.csv\")\n",
    "\n",
    "# Analiza kategorii features\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FEATURE IMPORTANCE PER KATEGORIA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "categories = {\n",
    "    'Embeddingi': [col for col in importance.index if 'title_emb' in col or 'title_tfidf' in col],\n",
    "    'Target Encoding': [col for col in importance.index if 'target_enc' in col],\n",
    "    'Agregacje Podcast': [col for col in importance.index if 'podcast_' in col],\n",
    "    'Agregacje Genre': [col for col in importance.index if 'genre_' in col],\n",
    "    'Time Features': [col for col in importance.index if any(x in col for x in ['day_', 'hour_', 'is_weekend', 'is_morning', 'is_primetime'])],\n",
    "    'Interakcje': [col for col in importance.index if '_x_' in col or 'vs_' in col],\n",
    "    'Missing Flags': [col for col in importance.index if 'missing' in col],\n",
    "    'Original': ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
    "}\n",
    "\n",
    "for cat_name, cols in categories.items():\n",
    "    cat_importance = importance[importance.index.isin(cols)]\n",
    "    if len(cat_importance) > 0:\n",
    "        total_importance = cat_importance['importance'].sum()\n",
    "        print(f\"\\n{cat_name}:\")\n",
    "        print(f\"Liczba features: {len(cat_importance)}\")\n",
    "        print(f\"Total importance: {total_importance:.4f}\")\n",
    "        print(f\"Top 3: {cat_importance.head(3)['importance'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebc78f",
   "metadata": {},
   "source": [
    "**Wnioski z Feature Importance:**\n",
    "\n",
    "**Sprawdzić:**\n",
    "1. Czy Episode_Length_minutes jest w top 3? (potwierdzenie EDA) - top1\n",
    "2. Czy agregacje (podcast_avg_listening) mają wysoką importance? - teorytycznie nie ma, 0.04\n",
    "3. Czy embeddingi/TF-IDF wniosły wartość? (porównaj z prostymi text features) - todo\n",
    "4. Czy cyclic encoding (sin/cos) jest lepszy niż proste numery? - lepsze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd40eb9",
   "metadata": {},
   "source": [
    "Leaderboard modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "451bb25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LEADERBOARD MODELI\n",
      "============================================================\n",
      "                   model  score_val  score_test  pred_time_val     fit_time\n",
      "0      LightGBMXT_BAG_L1 -12.977127  -11.572043     283.388778  1386.329696\n",
      "1    WeightedEnsemble_L3 -12.928836  -11.912629     306.252712  2081.621201\n",
      "2  LightGBMCustom_BAG_L2 -12.931711  -11.919946     302.616323  1938.841758\n",
      "3    WeightedEnsemble_L2 -12.945878  -11.942732     295.668804  1622.453962\n",
      "4        LightGBM_BAG_L2 -12.934951  -11.948993     299.846920  1793.717502\n",
      "5      LightGBMXT_BAG_L2 -12.959440  -12.068633     305.739882  1921.904481\n",
      "6        LightGBM_BAG_L1 -13.016026  -12.591923      12.255025   234.695555\n",
      "7  LightGBMCustom_BAG_L1 -26.519098  -26.518611       0.585730    32.242102\n",
      "\n",
      "Najlepszy model: WeightedEnsemble_L3\n",
      "RMSE validation: 12.9771\n",
      "\n",
      "============================================================\n",
      "OVERFITTING ANALYSIS\n",
      "============================================================\n",
      "Train RMSE:      11.5720\n",
      "Validation RMSE: 12.9771\n",
      "Overfitting gap: 1.4051\n",
      "Model może overfittować (gap > 1.0)\n"
     ]
    }
   ],
   "source": [
    "leaderboard = predictor.leaderboard(train_features, silent=True)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LEADERBOARD MODELI\")\n",
    "print(f\"{'='*60}\")\n",
    "print(leaderboard[['model', 'score_val', 'score_test', 'pred_time_val', 'fit_time']])\n",
    "\n",
    "best_model = predictor.leaderboard(silent=True).iloc[0]['model']\n",
    "print(f\"\\nNajlepszy model: {best_model}\")\n",
    "\n",
    "best_rmse = abs(leaderboard['score_val'].iloc[0])\n",
    "print(f\"RMSE validation: {best_rmse:.4f}\")\n",
    "\n",
    "# Overfitting check\n",
    "best_row = leaderboard.iloc[0]\n",
    "train_rmse = abs(best_row['score_test'])\n",
    "val_rmse = abs(best_row['score_val'])\n",
    "overfitting_gap = val_rmse - train_rmse\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train RMSE:      {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Overfitting gap: {overfitting_gap:.4f}\")\n",
    "\n",
    "if overfitting_gap > 1.0:\n",
    "    print(\"Model może overfittować (gap > 1.0)\")\n",
    "else:\n",
    "    print(\"Model generalizuje dobrze (gap < 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d2738",
   "metadata": {},
   "source": [
    "Interpretacja Leaderboard\n",
    "\n",
    "**Który model wygrywa?**\n",
    "- WeightedEnsemble powinien być na top (łączy wszystkie modele) - jest prawie na topie, jest ok\n",
    "- Jeśli single model (np. LightGBM) wygrywa -> ensemble nie pomógł\n",
    "\n",
    "**RMSE Validation:**\n",
    "- To jest spodziewany wynik na Kaggle (+-0.5 RMSE)\n",
    "- Jeśli RMSE ~12.5–13.5 -> bardzo dobry wynik! - jest 11.5\n",
    "\n",
    "**Overfitting:**\n",
    "- Gap < 1.0 -> model stabilny\n",
    "- Gap > 2.0 -> zbyt mocny overfitting, rozważ więcej regularizacji - gap 1.4 mozna imo cos podzialac ale nie ma czasu xd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518531cf",
   "metadata": {},
   "source": [
    "Predykcja na test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db430d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STATYSTYKI PREDYKCJI\n",
      "============================================================\n",
      "Min:    0.54\n",
      "Max:    113.77\n",
      "Mean:   45.48\n",
      "Median: 44.54\n",
      "Std:    23.81\n",
      "\n",
      "Brak ujemnych predykcji\n",
      "\n",
      "============================================================\n",
      "PORÓWNANIE: Train vs Test Predictions\n",
      "============================================================\n",
      "Train target - Mean: 45.44, Std: 27.14\n",
      "Test preds   - Mean: 45.48, Std: 23.81\n",
      "Podobne rozkłady (różnica: 0.04)\n"
     ]
    }
   ],
   "source": [
    "predictions = predictor.predict(test_features)\n",
    "\n",
    "# Statystyki predykcji\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STATYSTYKI PREDYKCJI\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Min:    {predictions.min():.2f}\")\n",
    "print(f\"Max:    {predictions.max():.2f}\")\n",
    "print(f\"Mean:   {predictions.mean():.2f}\")\n",
    "print(f\"Median: {predictions.median():.2f}\")\n",
    "print(f\"Std:    {predictions.std():.2f}\")\n",
    "\n",
    "# Sprawdź wartości ujemne\n",
    "if (predictions < 0).any():\n",
    "    n_negative = (predictions < 0).sum()\n",
    "    print(f\"\\nUWAGA: {n_negative} predykcji ujemnych! Clipowanie do 0...\")\n",
    "    predictions = predictions.clip(lower=0)\n",
    "else:\n",
    "    print(\"\\nBrak ujemnych predykcji\")\n",
    "\n",
    "# Porównaj z train distribution\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PORÓWNANIE: Train vs Test Predictions\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train target - Mean: {train['Listening_Time_minutes'].mean():.2f}, Std: {train['Listening_Time_minutes'].std():.2f}\")\n",
    "print(f\"Test preds   - Mean: {predictions.mean():.2f}, Std: {predictions.std():.2f}\")\n",
    "\n",
    "mean_diff = abs(train['Listening_Time_minutes'].mean() - predictions.mean())\n",
    "if mean_diff > 5:\n",
    "    print(f\"Duża różnica średnich ({mean_diff:.2f}) - sprawdź czy nie ma data shift!\")\n",
    "else:\n",
    "    print(f\"Podobne rozkłady (różnica: {mean_diff:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f60c0",
   "metadata": {},
   "source": [
    "Zapisanie submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b61fb8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUBMISSION ZAPISANY\n",
      "============================================================\n",
      "Plik: submission_final.csv\n",
      "Shape: (250000, 2)\n",
      "\n",
      "Pierwsze 5 wierszy:\n",
      "       id  Listening_Time_minutes\n",
      "0  750000               54.300449\n",
      "1  750001               19.034279\n",
      "2  750002               49.686726\n",
      "3  750003               78.168449\n",
      "4  750004               48.523575\n",
      "\n",
      "Ostatnie 5 wierszy:\n",
      "            id  Listening_Time_minutes\n",
      "249995  999995               11.818881\n",
      "249996  999996               57.994011\n",
      "249997  999997                7.576629\n",
      "249998  999998               75.594490\n",
      "249999  999999               57.122337\n"
     ]
    }
   ],
   "source": [
    "submission = test[[\"id\"]].copy()\n",
    "submission[\"Listening_Time_minutes\"] = predictions\n",
    "\n",
    "# Zapisz\n",
    "submission.to_csv(\"submission_final.csv\", index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUBMISSION ZAPISANY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Plik: submission_final.csv\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"\\nPierwsze 5 wierszy:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nOstatnie 5 wierszy:\")\n",
    "print(submission.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edc674",
   "metadata": {},
   "source": [
    "Walidacja submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7c63585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WALIDACJA FORMATU SUBMISSION\n",
      "============================================================\n",
      "nice Kolumna 'id' istnieje\n",
      "nice Kolumna 'Listening_Time_minutes' istnieje\n",
      "nice Liczba wierszy = 250000\n",
      "nice Brak wartości NULL\n",
      "nice Brak ujemnych wartości\n",
      "nice ID są unikalne\n",
      "\n",
      "Wszystkie walidacje przeszły! Submission gotowy do wysłania.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"WALIDACJA FORMATU SUBMISSION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Sprawdź format\n",
    "checks = []\n",
    "checks.append((\"Kolumna 'id' istnieje\", \"id\" in submission.columns))\n",
    "checks.append((\"Kolumna 'Listening_Time_minutes' istnieje\", \"Listening_Time_minutes\" in submission.columns))\n",
    "checks.append((\"Liczba wierszy = 250000\", len(submission) == 250000))\n",
    "checks.append((\"Brak wartości NULL\", submission[\"Listening_Time_minutes\"].isnull().sum() == 0))\n",
    "checks.append((\"Brak ujemnych wartości\", (submission[\"Listening_Time_minutes\"] < 0).sum() == 0))\n",
    "checks.append((\"ID są unikalne\", submission[\"id\"].nunique() == 250000))\n",
    "\n",
    "for check_name, passed in checks:\n",
    "    status = \"nice\" if passed else \"bruh\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "\n",
    "all_passed = all([c[1] for c in checks])\n",
    "if all_passed:\n",
    "    print(\"\\nWszystkie walidacje przeszły! Submission gotowy do wysłania.\")\n",
    "else:\n",
    "    print(\"\\nNiektóre walidacje nie przeszły - sprawdź błędy powyżej!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d9c08",
   "metadata": {},
   "source": [
    "**FINALNE STATYSTYKI MODELU:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf65c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PODSUMOWANIE KOŃCOWE\n",
      "================================================================================\n",
      "\n",
      "DANE:\n",
      "   Train samples: 749,999\n",
      "   Test samples:  250,000\n",
      "   Features:      112\n",
      "\n",
      "FEATURE ENGINEERING:\n",
      "   Flagi missing (2)\n",
      "   Cyclic encoding (4: day_sin/cos, hour_sin/cos)\n",
      "   Text embeddingi/TF-IDF (50-100 features)\n",
      "   Agregacje Podcast (11 features)\n",
      "   Agregacje Genre (5 features)\n",
      "   Target encoding (2 features)\n",
      "   Interakcje (15+ features)\n",
      "\n",
      "MODEL:\n",
      "   Najlepszy model:  WeightedEnsemble_L3\n",
      "   RMSE validation:  12.9771\n",
      "   Overfitting gap:  1.4051\n",
      "   Training time:    1386s (~23.1 min)\n",
      "\n",
      "PREDYKCJE:\n",
      "   Mean:   45.48 min\n",
      "   Median: 44.54 min\n",
      "   Range:  [0.54, 113.77]\n",
      "\n",
      "SPODZIEWANY WYNIK NA KAGGLE:\n",
      "   RMSE: ~12.98 (+-0.5)\n",
      "\n",
      "================================================================================\n",
      "PROCES ZAKOŃCZONY - SUBMISSION GOTOWY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PODSUMOWANIE KOŃCOWE\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nDANE:\")\n",
    "print(f\"   Train samples: {len(train):,}\")\n",
    "print(f\"   Test samples:  {len(test):,}\")\n",
    "print(f\"   Features:      {train_features.shape[1] - 1}\")\n",
    "\n",
    "print(f\"\\nFEATURE ENGINEERING:\")\n",
    "print(f\"   Flagi missing (2)\")\n",
    "print(f\"   Cyclic encoding (4: day_sin/cos, hour_sin/cos)\")\n",
    "print(f\"   Text embeddingi/TF-IDF (50-100 features)\")\n",
    "print(f\"   Agregacje Podcast (11 features)\")\n",
    "print(f\"   Agregacje Genre (5 features)\")\n",
    "print(f\"   Target encoding (2 features)\")\n",
    "print(f\"   Interakcje (15+ features)\")\n",
    "\n",
    "print(f\"\\nMODEL:\")\n",
    "print(f\"   Najlepszy model:  {best_model}\")\n",
    "print(f\"   RMSE validation:  {best_rmse:.4f}\")\n",
    "print(f\"   Overfitting gap:  {overfitting_gap:.4f}\")\n",
    "print(f\"   Training time:    {best_row['fit_time']:.0f}s (~{best_row['fit_time']/60:.1f} min)\")\n",
    "\n",
    "print(f\"\\nPREDYKCJE:\")\n",
    "print(f\"   Mean:   {predictions.mean():.2f} min\")\n",
    "print(f\"   Median: {predictions.median():.2f} min\")\n",
    "print(f\"   Range:  [{predictions.min():.2f}, {predictions.max():.2f}]\")\n",
    "\n",
    "print(f\"\\nSPODZIEWANY WYNIK NA KAGGLE:\")\n",
    "print(f\"   RMSE: ~{best_rmse:.2f} (+-0.5)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROCES ZAKOŃCZONY - SUBMISSION GOTOWY!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f701423",
   "metadata": {},
   "source": [
    "Co zadziałało najlepiej:\n",
    "\n",
    "1. **Agregacje per Podcast** (podcast_avg_listening)  \n",
    "   - Jeden z najsilniejszych predyktorów\n",
    "   - Podcast \"brand\" ma ogromny wpływ  \n",
    "\n",
    "2. **Target Encoding** (z CV protection)  \n",
    "   - Skutecznie radzi sobie z high cardinality  \n",
    "   - Bayesian smoothing zapobiega overfittingowi  \n",
    "\n",
    "3. **Episode_Length_minutes** (z EDA)  \n",
    "   - Potwierdzenie: najsilniejsza korelacja  \n",
    "   - Interakcje z czasem publikacji dodały wartość  \n",
    "\n",
    "4. **Cyclic Encoding** (sin/cos)  \n",
    "   - Lepsze niż proste numery  \n",
    "   - Model rozumie cykliczność (Sunday blisko Monday)  \n",
    "\n",
    "5. **Text Embeddingi** (SentenceTransformers/TF-IDF)  \n",
    "   - Semantyka tytułów ma znaczenie  \n",
    "   - Clustering wychwycił tematy  \n",
    "\n",
    "Co mogło nie pomóc:\n",
    "\n",
    "- Niektóre interakcje mogą być redundantne (np. total_popularity vs popularity_ratio)  \n",
    "- Zbyt dużo text features może wprowadzać szum  \n",
    "\n",
    "Dalsze kierunki optymalizacji:\n",
    "\n",
    "1. **Feature Selection** – usuń features o importance < 0.001  \n",
    "2. **Hyperparameter Tuning** – więcej wariantów LightGBM  \n",
    "3. **Ensemble różnych preprocessingów** – różne strategie imputacji  \n",
    "4. **Deep Learning** – TabTransformer/FT-Transformer dla tabel (optional bo nie wiem jak to dziala xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417441e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
