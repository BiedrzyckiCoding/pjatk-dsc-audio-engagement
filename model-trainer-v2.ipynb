{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a447f7a",
   "metadata": {},
   "source": [
    "Feature Engineering + modelowanie\n",
    "\n",
    "**Cel:** transformacja danych na podstawie wniosnów z EDA i trenig modelu autogluonem\n",
    "\n",
    "**Plan:**\n",
    "1. Preprocessing (missing, outliers, duplikaty)\n",
    "2. Time features (cyclic encoding)\n",
    "3. Embeddingi z tytułów\n",
    "4. Agregacje per podcast/genre\n",
    "5. Target encoding\n",
    "6. Trening AutoGluon i analiza wyników\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38272f6",
   "metadata": {},
   "source": [
    "Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import mstats\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sprawdź dostępność SentenceTransformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    EMBEDDINGS_AVAILABLE = True\n",
    "    print(\"SentenceTransformers dostępny!\")\n",
    "except ImportError:\n",
    "    EMBEDDINGS_AVAILABLE = False\n",
    "    print(\"SentenceTransformers niedostępny - użyję TF-IDF\")\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ścieżki\n",
    "TRAIN_PATH = Path(\"data/train.csv\")\n",
    "TEST_PATH = Path(\"data/test.csv\")\n",
    "\n",
    "# Wczytaj dane\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb3084",
   "metadata": {},
   "source": [
    "Preprocessing: Flagi dla missing\n",
    "\n",
    "**Dlaczego:**\n",
    "- z eda wiemy, że braki w Guest_Popularity to około 19% a Episode_length to około 11%\n",
    "- Test ma braki w tych samych miejscach, więc fagi będą pomocne\n",
    "\n",
    "**Hipoteza:**\n",
    "- Brak gościa = podcast solo -> inny wzorzec słuchalności\n",
    "- model nauczy się: \"Gdy Guest_Pop_missing = 1 -> przewiduj x minut\n",
    "\n",
    "Możliwe, że to obniży RMSE o jakąś część procenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flagi dla Episode_Length_minutes\n",
    "train[\"Episode_Length_missing\"] = train[\"Episode_Length_minutes\"].isnull().astype(int)\n",
    "test[\"Episode_Length_missing\"] = test[\"Episode_Length_minutes\"].isnull().astype(int)\n",
    "\n",
    "# Flagi dla Guest_Popularity_percentage\n",
    "train[\"Guest_Pop_missing\"] = train[\"Guest_Popularity_percentage\"].isnull().astype(int)\n",
    "test[\"Guest_Pop_missing\"] = test[\"Guest_Popularity_percentage\"].isnull().astype(int)\n",
    "\n",
    "print(\"Flagi dodane!\")\n",
    "print(f\"Train - Episode_Length_missing: {train['Episode_Length_missing'].sum()}\")\n",
    "print(f\"Test - Episode_Length_missing: {test['Episode_Length_missing'].sum()}\")\n",
    "print(f\"Train - Guest_Pop_missing: {train['Guest_Pop_missing'].sum()}\")\n",
    "print(f\"Test - Guest_Pop_missing: {test['Guest_Pop_missing'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29954f8",
   "metadata": {},
   "source": [
    "Usunięcie braków w number_of_ads (tylko train) bo jest jeden brak xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train przed usunięciem: {len(train)}\")\n",
    "train = train.dropna(subset=[\"Number_of_Ads\"])\n",
    "print(f\"Train po usunięciu: {len(train)}\")\n",
    "print(f\"Usunięto: {1} wiersz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e09c9a",
   "metadata": {},
   "source": [
    "Imputacja brakujących wartośći\n",
    "\n",
    "**Strategia z EDA:**\n",
    "- Episode_Length_minutes: mediana - prawostronna skośność\n",
    "- Guest_Popularity_percentage: średnia - rozkład stmetryczny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oblicz statystyki na train\n",
    "ep_len_median = train[\"Episode_Length_minutes\"].median()\n",
    "guest_pop_mean = train[\"Guest_Popularity_percentage\"].mean()\n",
    "\n",
    "print(f\"Episode_Length median: {ep_len_median:.2f}\")\n",
    "print(f\"Guest_Popularity mean: {guest_pop_mean:.2f}\")\n",
    "\n",
    "# Wypełnij braki\n",
    "train[\"Episode_Length_minutes\"].fillna(ep_len_median, inplace=True)\n",
    "test[\"Episode_Length_minutes\"].fillna(ep_len_median, inplace=True)\n",
    "\n",
    "train[\"Guest_Popularity_percentage\"].fillna(guest_pop_mean, inplace=True)\n",
    "test[\"Guest_Popularity_percentage\"].fillna(guest_pop_mean, inplace=True)\n",
    "\n",
    "print(\"\\nImputation zakończony!\")\n",
    "print(f\"Train braki: {train.isnull().sum().sum()}\")\n",
    "print(f\"Test braki: {test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46e4af",
   "metadata": {},
   "source": [
    "Wywalanie outlierów\n",
    "\n",
    "**Dlaczego?**\n",
    "- bo jest ich kilka sztuk\n",
    "- mają duzy wpływ na RMSE\n",
    "\n",
    "Moze obnizyc RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99596f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    before = df.shape[0]\n",
    "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    after = df.shape[0]\n",
    "\n",
    "    print(f\"{col}: usunięto {before - after} outlierów (pozostało {after})\")\n",
    "    return df\n",
    "\n",
    "# Usuwanie outlierów dla kluczowych zmiennych\n",
    "train_clean = remove_outliers_iqr(train, \"Episode_Length_minutes\")\n",
    "train_clean = remove_outliers_iqr(train_clean, \"Number_of_Ads\")\n",
    "\n",
    "# Dopasowanie testu do zakresu treningu (opcjonalne clipping)\n",
    "Q1_ep, Q3_ep = train_clean[\"Episode_Length_minutes\"].quantile([0.25, 0.75])\n",
    "IQR_ep = Q3_ep - Q1_ep\n",
    "ep_lower, ep_upper = Q1_ep - 1.5 * IQR_ep, Q3_ep + 1.5 * IQR_ep\n",
    "\n",
    "Q1_ads, Q3_ads = train_clean[\"Number_of_Ads\"].quantile([0.25, 0.75])\n",
    "IQR_ads = Q3_ads - Q1_ads\n",
    "ads_lower, ads_upper = Q1_ads - 1.5 * IQR_ads, Q3_ads + 1.5 * IQR_ads\n",
    "\n",
    "test[\"Episode_Length_minutes\"] = test[\"Episode_Length_minutes\"].clip(ep_lower, ep_upper)\n",
    "test[\"Number_of_Ads\"] = test[\"Number_of_Ads\"].clip(ads_lower, ads_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1f1c0",
   "metadata": {},
   "source": [
    "Usunięcie duplikatów (tylko train, zeby test mial dobra ilosc wierszy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train przed usunięciem duplikatów: {len(train)}\")\n",
    "train = train.drop_duplicates()\n",
    "print(f\"Train po usunięciu duplikatów: {len(train)}\")\n",
    "print(f\"Test (bez zmian): {len(test)} wierszy\")\n",
    "\n",
    "# WALIDACJA - test musi mieć 250k wierszy!\n",
    "assert len(test) == 250000, f\"Test ma {len(test)}, powinien mieć 250000!\"\n",
    "print(\"Test ma poprawną liczbę wierszy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd466d3f",
   "metadata": {},
   "source": [
    "Parsowanie czasu publikacji -> numery -> cyclic encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4425cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapowanie dni tygodnia\n",
    "day_mapping = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "    'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "}\n",
    "\n",
    "# Mapowanie pory dnia na godziny (środek przedziału)\n",
    "time_mapping = {\n",
    "    'Morning': 8, # 6-12h\n",
    "    'Afternoon': 14, # 12-18h\n",
    "    'Evening': 20, # 18-24h\n",
    "    'Night': 2  # 24-6h (środek nocy)\n",
    "}\n",
    "\n",
    "for df in [train, test]:\n",
    "    # Zamiana na numery\n",
    "    df[\"day_of_week_num\"] = df[\"Publication_Day\"].map(day_mapping)\n",
    "    df[\"hour_num\"] = df[\"Publication_Time\"].map(time_mapping)\n",
    "    \n",
    "    # CYCLIC ENCODING dla dnia tygodnia\n",
    "    df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_week_num\"] / 7)\n",
    "    df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_week_num\"] / 7)\n",
    "    \n",
    "    # CYCLIC ENCODING dla godziny\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour_num\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour_num\"] / 24)\n",
    "    \n",
    "    # Binarne features\n",
    "    df[\"is_weekend\"] = (df[\"day_of_week_num\"] >= 5).astype(int)\n",
    "    df[\"is_morning\"] = (df[\"hour_num\"] >= 6) & (df[\"hour_num\"] < 12)\n",
    "    df[\"is_afternoon\"] = (df[\"hour_num\"] >= 12) & (df[\"hour_num\"] < 18)\n",
    "    df[\"is_evening\"] = (df[\"hour_num\"] >= 18) & (df[\"hour_num\"] < 24)\n",
    "    df[\"is_night\"] = (df[\"hour_num\"] < 6) | (df[\"hour_num\"] >= 22)\n",
    "    df[\"is_primetime\"] = (df[\"hour_num\"] >= 17) & (df[\"hour_num\"] <= 21)\n",
    "\n",
    "print(\"Parsowanie czasu zakończone!\")\n",
    "print(f\"Przykładowe wartości (train):\")\n",
    "print(train[['Publication_Day', 'day_of_week_num', 'day_sin', 'day_cos']].head(3))\n",
    "print(train[['Publication_Time', 'hour_num', 'hour_sin', 'hour_cos']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781206f",
   "metadata": {},
   "source": [
    "Feature engineering - Podstawowe interakcje\n",
    "\n",
    "**Dlaczego te features:**\n",
    "- z EDA wiemy, że Episode_length ma najsilniejszą korelację z targetem\n",
    "- Popularity (host + guest) też jest istotne\n",
    "- interakcje mogą wychwycić nieliniowe zależności\n",
    "\n",
    "Może Dalej zmniejszymy RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce396ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    # === Interakcje numeryczne ===\n",
    "    df[\"ads_per_minute\"] = df[\"Number_of_Ads\"] / (df[\"Episode_Length_minutes\"] + 1)\n",
    "    df[\"total_popularity\"] = df[\"Host_Popularity_percentage\"] + df[\"Guest_Popularity_percentage\"]\n",
    "    df[\"popularity_ratio\"] = df[\"Host_Popularity_percentage\"] / (df[\"Guest_Popularity_percentage\"] + 1)\n",
    "    df[\"popularity_diff\"] = df[\"Host_Popularity_percentage\"] - df[\"Guest_Popularity_percentage\"]\n",
    "    \n",
    "    # === Interakcje z flagami missing ===\n",
    "    # Jeśli brak gościa, popularność hosta jest WAŻNIEJSZA\n",
    "    df[\"missing_guest_x_host_pop\"] = df[\"Guest_Pop_missing\"] * df[\"Host_Popularity_percentage\"]\n",
    "    df[\"missing_length_x_ads\"] = df[\"Episode_Length_missing\"] * df[\"Number_of_Ads\"]\n",
    "    df[\"missing_guest_x_episode_length\"] = df[\"Guest_Pop_missing\"] * df[\"Episode_Length_minutes\"]\n",
    "    \n",
    "    # === Interakcje czasowe ===\n",
    "    # Wieczór + długi odcinek = więcej słuchania?\n",
    "    df[\"length_x_evening\"] = df[\"Episode_Length_minutes\"] * df[\"is_evening\"].astype(int)\n",
    "    df[\"host_pop_x_weekend\"] = df[\"Host_Popularity_percentage\"] * df[\"is_weekend\"]\n",
    "    df[\"ads_x_primetime\"] = df[\"Number_of_Ads\"] * df[\"is_primetime\"].astype(int)\n",
    "    \n",
    "    # === Sentiment jako numeric ===\n",
    "    sentiment_map = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    df[\"sentiment_numeric\"] = df[\"Episode_Sentiment\"].map(sentiment_map).fillna(0)\n",
    "    \n",
    "    df[\"sentiment_x_guest_pop\"] = df[\"sentiment_numeric\"] * df[\"Guest_Popularity_percentage\"]\n",
    "    df[\"sentiment_x_host_pop\"] = df[\"sentiment_numeric\"] * df[\"Host_Popularity_percentage\"]\n",
    "    df[\"negative_sentiment_x_ads\"] = (df[\"sentiment_numeric\"] == -1).astype(int) * df[\"Number_of_Ads\"]\n",
    "\n",
    "print(f\"Podstawowe features dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94318b",
   "metadata": {},
   "source": [
    "Embeddingi z Episode_Title\n",
    "\n",
    "**Dlaczego embeddingi > text features:**\n",
    "- generalnie poprzednie wyznaczniki były bez sensu (długość, liczba słów, ...)\n",
    "- Embeddingi - model rozumie treść\n",
    "\n",
    "**Metoda:**\n",
    "1. SentenceTransformer (all-MiniLM-L6-v2) - szybki, 384-wymiarowy\n",
    "2. PCA → redukcja do 50 wymiarów (żeby nie przeciążyć modelu)\n",
    "3. KMeans clustering → grupowanie podobnych tytułów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDINGS_AVAILABLE:\n",
    "    print(\"Generowanie embeddingów z SentenceTransformers...\")\n",
    "    print(\"To może potrwać 3-5 minut...\")\n",
    "    \n",
    "    # Model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generuj embeddingi dla train\n",
    "    train_titles = train[\"Episode_Title\"].fillna(\"\").tolist()\n",
    "    train_embeddings = model.encode(train_titles, show_progress_bar=True, batch_size=256)\n",
    "    \n",
    "    # Generuj embeddingi dla test\n",
    "    test_titles = test[\"Episode_Title\"].fillna(\"\").tolist()\n",
    "    test_embeddings = model.encode(test_titles, show_progress_bar=True, batch_size=256)\n",
    "    \n",
    "    print(f\"Embeddingi wygenerowane! Shape: {train_embeddings.shape}\")\n",
    "    \n",
    "    # PCA - redukcja do 50 wymiarów\n",
    "    pca = PCA(n_components=50, random_state=42)\n",
    "    train_embeddings_pca = pca.fit_transform(train_embeddings)\n",
    "    test_embeddings_pca = pca.transform(test_embeddings)\n",
    "    \n",
    "    print(f\"PCA zakończone! Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    \n",
    "    # Dodaj jako features\n",
    "    for i in range(50):\n",
    "        train[f\"title_emb_{i}\"] = train_embeddings_pca[:, i]\n",
    "        test[f\"title_emb_{i}\"] = test_embeddings_pca[:, i]\n",
    "    \n",
    "    # KMeans clustering - grupowanie podobnych tytułów\n",
    "    n_clusters = 20  # 20 klastrów tematycznych\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    train[\"title_cluster\"] = kmeans.fit_predict(train_embeddings_pca)\n",
    "    test[\"title_cluster\"] = kmeans.predict(test_embeddings_pca)\n",
    "    \n",
    "    print(f\"Clustering zakończony! {n_clusters} klastrów\")\n",
    "    print(f\"Rozkład klastrów (train):\\n{train['title_cluster'].value_counts().head()}\")\n",
    "    \n",
    "else:\n",
    "    # Fallback: TF-IDF\n",
    "    print(\"Używam TF-IDF jako alternatywy...\")\n",
    "    \n",
    "    tfidf = TfidfVectorizer(max_features=50, stop_words='english', ngram_range=(1, 2))\n",
    "    \n",
    "    train_tfidf = tfidf.fit_transform(train[\"Episode_Title\"].fillna(\"\"))\n",
    "    test_tfidf = tfidf.transform(test[\"Episode_Title\"].fillna(\"\"))\n",
    "    \n",
    "    # Dodaj jako features\n",
    "    for i in range(50):\n",
    "        train[f\"title_tfidf_{i}\"] = train_tfidf[:, i].toarray().flatten()\n",
    "        test[f\"title_tfidf_{i}\"] = test_tfidf[:, i].toarray().flatten()\n",
    "    \n",
    "    print(f\"TF-IDF zakończone! {train_tfidf.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84dc62c",
   "metadata": {},
   "source": [
    "Frequency Encoding\n",
    "\n",
    "**Dlaczego?**\n",
    "- z EDA wiemy, że niektóre podcasty mają po 10 odcinków, inne tysiące\n",
    "- Popularność podcastu zapewne może korelować ze słuchalnością"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podcast frequency\n",
    "podcast_freq = train[\"Podcast_Name\"].value_counts()\n",
    "train[\"podcast_frequency\"] = train[\"Podcast_Name\"].map(podcast_freq).fillna(0)\n",
    "test[\"podcast_frequency\"] = test[\"Podcast_Name\"].map(podcast_freq).fillna(0)\n",
    "\n",
    "# Genre frequency\n",
    "genre_freq = train[\"Genre\"].value_counts()\n",
    "train[\"genre_frequency\"] = train[\"Genre\"].map(genre_freq).fillna(0)\n",
    "test[\"genre_frequency\"] = test[\"Genre\"].map(genre_freq).fillna(0)\n",
    "\n",
    "# Normalizacja (0-1), żeby modele regresyjne lepiej działały\n",
    "train[\"podcast_frequency_norm\"] = train[\"podcast_frequency\"] / len(train)\n",
    "test[\"podcast_frequency_norm\"] = test[\"podcast_frequency\"] / len(train)\n",
    "\n",
    "train[\"genre_frequency_norm\"] = train[\"genre_frequency\"] / len(train)\n",
    "test[\"genre_frequency_norm\"] = test[\"genre_frequency\"] / len(train)\n",
    "\n",
    "print(\"Frequency encoding zakończony!\")\n",
    "print(f\"Top 5 podcasts by frequency:\\n{podcast_freq.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5c9fa",
   "metadata": {},
   "source": [
    "Agregacje per podcast\n",
    "\n",
    "**Dlaczego:**\n",
    "- z EDA: top podcasty mają różną średnie słuchalności (40-50 minut)\n",
    "\n",
    "**Features:**\n",
    "1. podcast_avg_listening - średnia historyczna\n",
    "2. podcast_std_listening - stabilność (niski std = stała publiczność)\n",
    "3. podcast_min/max_listening - zakres wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statystyki per Podcast (obliczane TYLKO na train!)\n",
    "podcast_stats = train.groupby(\"Podcast_Name\").agg({\n",
    "    \"Listening_Time_minutes\": [\"mean\", \"median\", \"std\", \"min\", \"max\"],\n",
    "    \"Episode_Length_minutes\": [\"mean\", \"median\"],\n",
    "    \"Number_of_Ads\": [\"mean\", \"median\"],\n",
    "    \"Host_Popularity_percentage\": \"first\",\n",
    "    \"Guest_Popularity_percentage\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# Spłaszcz kolumny\n",
    "podcast_stats.columns = [\n",
    "    \"Podcast_Name\",\n",
    "    \"podcast_avg_listening\", \"podcast_med_listening\", \"podcast_std_listening\",\n",
    "    \"podcast_min_listening\", \"podcast_max_listening\",\n",
    "    \"podcast_avg_length\", \"podcast_med_length\",\n",
    "    \"podcast_avg_ads\", \"podcast_med_ads\",\n",
    "    \"podcast_host_pop\", \"podcast_avg_guest_pop\"\n",
    "]\n",
    "\n",
    "# Wypełnij std NaN (podcasty z 1 odcinkiem)\n",
    "podcast_stats[\"podcast_std_listening\"].fillna(0, inplace=True)\n",
    "\n",
    "# Merguj z LEFT join (ważne!)\n",
    "print(f\"Train przed merge: {len(train)}\")\n",
    "print(f\"Test przed merge: {len(test)}\")\n",
    "\n",
    "train = train.merge(podcast_stats, on=\"Podcast_Name\", how=\"left\")\n",
    "test = test.merge(podcast_stats, on=\"Podcast_Name\", how=\"left\")\n",
    "\n",
    "print(f\"Train po merge: {len(train)}\")\n",
    "print(f\"Test po merge: {len(test)}\")\n",
    "\n",
    "# WALIDACJA\n",
    "assert len(test) == 250000, f\"Test stracił wiersze! Ma {len(test)}\"\n",
    "print(\"Merge nie stracił wierszy\")\n",
    "\n",
    "print(f\"\\nPodcast stats dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877053b",
   "metadata": {},
   "source": [
    "Agregacje per Genre\n",
    "\n",
    "**Dlaczego:**\n",
    "- z eda: różnice średnich między gatunkami (np. Comedy vs News)\n",
    "- Genre characteristics wpływa na słuchalność"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399dec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statystyki per Genre\n",
    "genre_stats = train.groupby(\"Genre\").agg({\n",
    "    \"Listening_Time_minutes\": [\"mean\", \"median\", \"std\"],\n",
    "    \"Guest_Popularity_percentage\": \"mean\",\n",
    "    \"Episode_Length_minutes\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "genre_stats.columns = [\n",
    "    \"Genre\",\n",
    "    \"genre_avg_listening\", \"genre_med_listening\", \"genre_std_listening\",\n",
    "    \"genre_avg_guest_pop\", \"genre_avg_length\"\n",
    "]\n",
    "\n",
    "genre_stats[\"genre_std_listening\"].fillna(0, inplace=True)\n",
    "\n",
    "# LEFT join\n",
    "print(f\"Test przed merge: {len(test)}\")\n",
    "train = train.merge(genre_stats, on=\"Genre\", how=\"left\")\n",
    "test = test.merge(genre_stats, on=\"Genre\", how=\"left\")\n",
    "print(f\"Test po merge: {len(test)}\")\n",
    "\n",
    "# WALIDACJA\n",
    "assert len(test) == 250000, f\"Test stracił wiersze!\"\n",
    "print(\"Merge OK\")\n",
    "\n",
    "print(f\"Genre stats dodane! Train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c26229",
   "metadata": {},
   "source": [
    "Relative features (odcinek vs średnia)\n",
    "\n",
    "**Dlaczego:**\n",
    "- mało liczą się wartości bezwzględne, bardziej relatywne\n",
    "- np: 60-minutowy odcinek w podcaście o średniej 30 min jest wyjątkowy\n",
    "- To samo 60 min w podcaście o średniej 90 min -> krótki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    # Porównanie z podcast\n",
    "    df[\"length_vs_podcast_avg\"] = df[\"Episode_Length_minutes\"] / (df[\"podcast_avg_length\"] + 1)\n",
    "    df[\"ads_vs_podcast_avg\"] = df[\"Number_of_Ads\"] / (df[\"podcast_avg_ads\"] + 1)\n",
    "    df[\"guest_pop_vs_podcast_avg\"] = df[\"Guest_Popularity_percentage\"] / (df[\"podcast_avg_guest_pop\"] + 1)\n",
    "    \n",
    "    # Porównanie z genre\n",
    "    df[\"length_vs_genre_avg\"] = df[\"Episode_Length_minutes\"] / (df[\"genre_avg_length\"] + 1)\n",
    "    \n",
    "    # Czy ten odcinek jest powyżej/poniżej średniej?\n",
    "    df[\"above_podcast_avg_length\"] = (df[\"Episode_Length_minutes\"] > df[\"podcast_avg_length\"]).astype(int)\n",
    "    df[\"above_genre_avg_length\"] = (df[\"Episode_Length_minutes\"] > df[\"genre_avg_length\"]).astype(int)\n",
    "\n",
    "print(\"Relative features dodane!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004461f8",
   "metadata": {},
   "source": [
    "Target Encoding\n",
    "\n",
    "**Dlaczego:**\n",
    "- High cardinality (Podcast_name: około 100 różnych wartości) - one hot nie możliwy\n",
    "- Target encoding - mapowanie kategorii na średni target\n",
    "\n",
    "Problem w postaci data leakage:\n",
    "- model podgląda odpowiedzi\n",
    "\n",
    "Mozliwe rozwiązanie: K-Fold CV\n",
    "- Dla każdego fold: obliczamy średnią na pozostałych foldach\n",
    "- Bayesian smoothing, regularycacja dla rzadkich kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode_with_cv(train_df, test_df, cat_col, target_col, n_splits=5, smoothing=10):\n",
    "    \"\"\"\n",
    "    Target encoding z K-Fold CV (leak protection) + Bayesian smoothing\n",
    "    \n",
    "    Parametry:\n",
    "    - smoothing: im wyższy, tym bardziej zbliżamy się do global_mean dla rzadkich kategorii\n",
    "    \"\"\"\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    \n",
    "    # Inicjalizacja\n",
    "    train_df[f\"{cat_col}_target_enc\"] = global_mean\n",
    "    \n",
    "    # K-Fold CV dla train\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(train_df):\n",
    "        train_fold = train_df.iloc[train_idx]\n",
    "        \n",
    "        # Oblicz statystyki na foldzie treningowym\n",
    "        agg = train_fold.groupby(cat_col)[target_col].agg(['mean', 'count'])\n",
    "        \n",
    "        # Bayesian smoothing: (count * mean + smoothing * global_mean) / (count + smoothing)\n",
    "        smoothed = (agg['count'] * agg['mean'] + smoothing * global_mean) / (agg['count'] + smoothing)\n",
    "        \n",
    "        # Mapuj na fold walidacyjny\n",
    "        train_df.loc[val_idx, f\"{cat_col}_target_enc\"] = train_df.loc[val_idx, cat_col].map(smoothed).fillna(global_mean)\n",
    "    \n",
    "    # Dla test użyj całego train\n",
    "    agg_full = train_df.groupby(cat_col)[target_col].agg(['mean', 'count'])\n",
    "    smoothed_full = (agg_full['count'] * agg_full['mean'] + smoothing * global_mean) / (agg_full['count'] + smoothing)\n",
    "    test_df[f\"{cat_col}_target_enc\"] = test_df[cat_col].map(smoothed_full).fillna(global_mean)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Zastosuj target encoding\n",
    "print(\"Target encoding w toku (może potrwać ze 2 minuty)...\")\n",
    "\n",
    "for col in [\"Podcast_Name\", \"Genre\"]:\n",
    "    train, test = target_encode_with_cv(train, test, col, \"Listening_Time_minutes\", smoothing=10)\n",
    "    print(f\"{col} zakończony\")\n",
    "\n",
    "print(f\"\\nTarget encoding zakończony! Train shape: {train.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
