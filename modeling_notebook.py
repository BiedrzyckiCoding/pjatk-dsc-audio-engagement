# %% [markdown]
# # üöÄ Feature Engineering + Modelowanie
# 
# **Cel:** Transformacja danych na podstawie wniosk√≥w z EDA i trening modelu AutoGluon.
# 
# **Plan:**
# 1. Preprocessing (missing, outliers, duplikaty)
# 2. Time features (cyclic encoding)
# 3. Embeddingi z tytu≈Ç√≥w (SentenceTransformers)
# 4. Agregacje per Podcast/Genre
# 5. Target encoding
# 6. Trening AutoGluon + analiza wynik√≥w

# %% [markdown]
# ## 1Ô∏è‚É£ Import bibliotek

# %%
import pandas as pd
import numpy as np
from pathlib import Path
from scipy.stats import mstats
from sklearn.model_selection import KFold
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# Sprawd≈∫ dostƒôpno≈õƒá SentenceTransformers
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
    print("‚úÖ SentenceTransformers dostƒôpny!")
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("‚ö†Ô∏è SentenceTransformers niedostƒôpny - u≈ºyjƒô TF-IDF")
    from sklearn.feature_extraction.text import TfidfVectorizer

# ≈öcie≈ºki
TRAIN_PATH = Path("data/train.csv")
TEST_PATH = Path("data/test.csv")

# Wczytaj dane
train = pd.read_csv(TRAIN_PATH)
test = pd.read_csv(TEST_PATH)

print(f"üìå Train shape: {train.shape}")
print(f"üìå Test shape: {test.shape}")

# %% [markdown]
# ## 2Ô∏è‚É£ Preprocessing: Flagi dla brakujƒÖcych danych
# 
# **Dlaczego:**
# - Z EDA wiemy, ≈ºe braki w `Guest_Popularity` (~19.5%) i `Episode_Length` (~11.5%)
# - Test ma braki w tych samych miejscach ‚Üí flagi bƒôdƒÖ informatywne!
# 
# **Hipoteza:**
# - Brak go≈õcia = podcast solo ‚Üí **inny wzorzec s≈Çuchalno≈õci**
# - Model nauczy siƒô: "Gdy `Guest_Pop_missing=1` ‚Üí przewiduj X minut"
# 
# **Spodziewany efekt:** ‚Üì 1-2% RMSE

# %%
# Flagi dla Episode_Length_minutes
train["Episode_Length_missing"] = train["Episode_Length_minutes"].isnull().astype(int)
test["Episode_Length_missing"] = test["Episode_Length_minutes"].isnull().astype(int)

# Flagi dla Guest_Popularity_percentage
train["Guest_Pop_missing"] = train["Guest_Popularity_percentage"].isnull().astype(int)
test["Guest_Pop_missing"] = test["Guest_Popularity_percentage"].isnull().astype(int)

print("‚úÖ Flagi dodane!")
print(f"Train - Episode_Length_missing: {train['Episode_Length_missing'].sum()}")
print(f"Test - Episode_Length_missing: {test['Episode_Length_missing'].sum()}")
print(f"Train - Guest_Pop_missing: {train['Guest_Pop_missing'].sum()}")
print(f"Test - Guest_Pop_missing: {test['Guest_Pop_missing'].sum()}")

# %% [markdown]
# ## 3Ô∏è‚É£ Usuniƒôcie brak√≥w w Number_of_Ads (tylko train)
# 
# **Dlaczego:**
# - Tylko 1 brak w train (0.001%) ‚Üí nie jest informatywny
# - Test ma 0 brak√≥w ‚Üí bezpiecznie dropujemy
# 
# **Spodziewany efekt:** Brak wp≈Çywu na RMSE

# %%
print(f"Train przed usuniƒôciem: {len(train)}")
train = train.dropna(subset=["Number_of_Ads"])
print(f"Train po usuniƒôciu: {len(train)}")
print(f"Usuniƒôto: {1} wiersz")

# %% [markdown]
# ## 4Ô∏è‚É£ Imputacja brakujƒÖcych warto≈õci
# 
# **Strategia (z EDA):**
# - `Episode_Length_minutes`: **median** (rozk≈Çad prawostronna sko≈õno≈õƒá)
# - `Guest_Popularity_percentage`: **mean** (rozk≈Çad symetryczny)
# 
# **Dlaczego obliczamy TYLKO na train:**
# - Unikamy data leakage (test nie mo≈ºe wp≈Çywaƒá na statystyki treningu)
# 
# **Spodziewany efekt:** Neutralny (ale flagi dodajƒÖ warto≈õƒá)

# %%
# Oblicz statystyki na train
ep_len_median = train["Episode_Length_minutes"].median()
guest_pop_mean = train["Guest_Popularity_percentage"].mean()

print(f"üìä Episode_Length median: {ep_len_median:.2f}")
print(f"üìä Guest_Popularity mean: {guest_pop_mean:.2f}")

# Wype≈Çnij braki
train["Episode_Length_minutes"].fillna(ep_len_median, inplace=True)
test["Episode_Length_minutes"].fillna(ep_len_median, inplace=True)

train["Guest_Popularity_percentage"].fillna(guest_pop_mean, inplace=True)
test["Guest_Popularity_percentage"].fillna(guest_pop_mean, inplace=True)

print("\n‚úÖ Imputation zako≈Ñczony!")
print(f"Train braki: {train.isnull().sum().sum()}")
print(f"Test braki: {test.isnull().sum().sum()}")

# %% [markdown]
# ## 5Ô∏è‚É£ Winsorization outlier√≥w
# 
# **Dlaczego NIE usuwamy outlier√≥w:**
# - Z EDA: ~5-10% outlier√≥w w `Episode_Length` i `Number_of_Ads`
# - **Usuniƒôcie = strata danych** ‚Üí gorszy model
# - **Winsorization = clip do 1/99 percentyla** ‚Üí zachowujemy wszystkie sample
# 
# **Spodziewany efekt:** ‚Üì 2-3% RMSE vs usuwanie outlier√≥w

# %%
# Winsorization dla Episode_Length_minutes
ep_len_winsorized = mstats.winsorize(train["Episode_Length_minutes"], limits=[0.01, 0.01])
ep_lower, ep_upper = ep_len_winsorized.min(), ep_len_winsorized.max()

print(f"üìä Episode_Length bounds: [{ep_lower:.2f}, {ep_upper:.2f}]")

train["Episode_Length_minutes"] = train["Episode_Length_minutes"].clip(ep_lower, ep_upper)
test["Episode_Length_minutes"] = test["Episode_Length_minutes"].clip(ep_lower, ep_upper)

# Winsorization dla Number_of_Ads
ads_winsorized = mstats.winsorize(train["Number_of_Ads"], limits=[0.01, 0.01])
ads_lower, ads_upper = ads_winsorized.min(), ads_winsorized.max()

print(f"üìä Number_of_Ads bounds: [{ads_lower:.2f}, {ads_upper:.2f}]")

train["Number_of_Ads"] = train["Number_of_Ads"].clip(ads_lower, ads_upper)
test["Number_of_Ads"] = test["Number_of_Ads"].clip(ads_lower, ads_upper)

print("\n‚úÖ Winsorization zako≈Ñczony!")

# %% [markdown]
# ## 6Ô∏è‚É£ Usuniƒôcie duplikat√≥w (TYLKO train)
# 
# **Dlaczego:**
# - Duplikaty mogƒÖ byƒá artefaktem zbierania danych
# - **Test NIE usuwamy** - ka≈ºdy wiersz to osobna predykcja (mo≈ºe byƒá duplikat!)
# 
# **Spodziewany efekt:** ‚Üì 0-1% RMSE (mniejszy overfitting)

# %%
print(f"Train przed usuniƒôciem duplikat√≥w: {len(train)}")
train = train.drop_duplicates()
print(f"Train po usuniƒôciu duplikat√≥w: {len(train)}")
print(f"Test (bez zmian): {len(test)} wierszy")

# WALIDACJA - test musi mieƒá 250k wierszy!
assert len(test) == 250000, f"‚ùå Test ma {len(test)}, powinien mieƒá 250000!"
print("‚úÖ Test ma poprawnƒÖ liczbƒô wierszy")

# %% [markdown]
# ## 7Ô∏è‚É£ Parsowanie czasu publikacji ‚Üí numery + cyclic encoding
# 
# **Problem z poprzednim kodem:**
# ```python
# df["pub_datetime"] = pd.to_datetime(df["Publication_Day"] + " " + df["Publication_Time"])
# # ‚Üë To dawa≈Ço NaN bo "Thursday Night" to nie jest prawdziwa data!
# ```
# 
# **Nowe podej≈õcie:**
# 1. Zamiana kategorii na **numery**:
#    - Publication_Day: Monday=0, ..., Sunday=6
#    - Publication_Time: Morning=6, Afternoon=14, Evening=18, Night=22 (godziny)
# 
# 2. **Cyclic encoding (sin/cos)**:
#    - Model wie, ≈ºe Sunday (6) jest blisko Monday (0)
#    - Godzina 23 jest blisko godziny 0
# 
# **Spodziewany efekt:** ‚Üì 2-4% RMSE (czas publikacji jest istotny z EDA)

# %%
# Mapowanie dni tygodnia
day_mapping = {
    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,
    'Friday': 4, 'Saturday': 5, 'Sunday': 6
}

# Mapowanie pory dnia na godziny (≈õrodek przedzia≈Çu)
time_mapping = {
    'Morning': 8,    # 6-12h
    'Afternoon': 14, # 12-18h
    'Evening': 20,   # 18-24h
    'Night': 2       # 24-6h (≈õrodek nocy)
}

for df in [train, test]:
    # Zamiana na numery
    df["day_of_week_num"] = df["Publication_Day"].map(day_mapping)
    df["hour_num"] = df["Publication_Time"].map(time_mapping)
    
    # CYCLIC ENCODING dla dnia tygodnia
    df["day_sin"] = np.sin(2 * np.pi * df["day_of_week_num"] / 7)
    df["day_cos"] = np.cos(2 * np.pi * df["day_of_week_num"] / 7)
    
    # CYCLIC ENCODING dla godziny
    df["hour_sin"] = np.sin(2 * np.pi * df["hour_num"] / 24)
    df["hour_cos"] = np.cos(2 * np.pi * df["hour_num"] / 24)
    
    # Binarne features
    df["is_weekend"] = (df["day_of_week_num"] >= 5).astype(int)
    df["is_morning"] = (df["hour_num"] >= 6) & (df["hour_num"] < 12)
    df["is_afternoon"] = (df["hour_num"] >= 12) & (df["hour_num"] < 18)
    df["is_evening"] = (df["hour_num"] >= 18) & (df["hour_num"] < 24)
    df["is_night"] = (df["hour_num"] < 6) | (df["hour_num"] >= 22)
    df["is_primetime"] = (df["hour_num"] >= 17) & (df["hour_num"] <= 21)

print("‚úÖ Parsowanie czasu zako≈Ñczone!")
print(f"Przyk≈Çadowe warto≈õci (train):")
print(train[['Publication_Day', 'day_of_week_num', 'day_sin', 'day_cos']].head(3))
print(train[['Publication_Time', 'hour_num', 'hour_sin', 'hour_cos']].head(3))

# %% [markdown]
# ## 8Ô∏è‚É£ Feature Engineering - Podstawowe interakcje
# 
# **Dlaczego te features:**
# - Z EDA wiemy, ≈ºe `Episode_Length` ma najsilniejszƒÖ korelacjƒô z targetem
# - Popularity (host + guest) te≈º jest istotne
# - **Interakcje** mogƒÖ wychwyciƒá nieliniowe zale≈ºno≈õci
# 
# **Spodziewany efekt:** ‚Üì 1-3% RMSE

# %%
for df in [train, test]:
    # === Interakcje numeryczne ===
    df["ads_per_minute"] = df["Number_of_Ads"] / (df["Episode_Length_minutes"] + 1)
    df["total_popularity"] = df["Host_Popularity_percentage"] + df["Guest_Popularity_percentage"]
    df["popularity_ratio"] = df["Host_Popularity_percentage"] / (df["Guest_Popularity_percentage"] + 1)
    df["popularity_diff"] = df["Host_Popularity_percentage"] - df["Guest_Popularity_percentage"]
    
    # === Interakcje z flagami missing ===
    # Je≈õli brak go≈õcia, popularno≈õƒá hosta jest WA≈ªNIEJSZA
    df["missing_guest_x_host_pop"] = df["Guest_Pop_missing"] * df["Host_Popularity_percentage"]
    df["missing_length_x_ads"] = df["Episode_Length_missing"] * df["Number_of_Ads"]
    df["missing_guest_x_episode_length"] = df["Guest_Pop_missing"] * df["Episode_Length_minutes"]
    
    # === Interakcje czasowe ===
    # Wiecz√≥r + d≈Çugi odcinek = wiƒôcej s≈Çuchania?
    df["length_x_evening"] = df["Episode_Length_minutes"] * df["is_evening"].astype(int)
    df["host_pop_x_weekend"] = df["Host_Popularity_percentage"] * df["is_weekend"]
    df["ads_x_primetime"] = df["Number_of_Ads"] * df["is_primetime"].astype(int)
    
    # === Sentiment jako numeric ===
    sentiment_map = {"positive": 1, "neutral": 0, "negative": -1}
    df["sentiment_numeric"] = df["Episode_Sentiment"].map(sentiment_map).fillna(0)
    
    df["sentiment_x_guest_pop"] = df["sentiment_numeric"] * df["Guest_Popularity_percentage"]
    df["sentiment_x_host_pop"] = df["sentiment_numeric"] * df["Host_Popularity_percentage"]
    df["negative_sentiment_x_ads"] = (df["sentiment_numeric"] == -1).astype(int) * df["Number_of_Ads"]

print(f"‚úÖ Podstawowe features dodane! Train shape: {train.shape}")

# %% [markdown]
# ## 9Ô∏è‚É£ Text Features - EMBEDDINGI z Episode_Title
# 
# **Dlaczego embeddingi > proste text features:**
# - Proste (d≈Çugo≈õƒá, liczba s≈Ç√≥w) ‚Üí brak semantyki
# - **Embeddingi** ‚Üí model rozumie tre≈õƒá (np. "Interview with CEO" vs "Music Session")
# 
# **Metoda:**
# 1. SentenceTransformer ('all-MiniLM-L6-v2') - szybki, 384-wymiarowy
# 2. PCA ‚Üí redukcja do 50 wymiar√≥w (≈ºeby nie przeciƒÖ≈ºyƒá modelu)
# 3. KMeans clustering ‚Üí grupowanie podobnych tytu≈Ç√≥w
# 
# **Alternatywa (je≈õli brak SentenceTransformers):** TF-IDF (top 50 s≈Ç√≥w)
# 
# **Spodziewany efekt:** ‚Üì 3-5% RMSE (tytu≈Çy sƒÖ informatywne!)

# %%
if EMBEDDINGS_AVAILABLE:
    print("üî• Generowanie embedding√≥w z SentenceTransformers...")
    print("‚è≥ To mo≈ºe potrwaƒá 3-5 minut...")
    
    # Model
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Generuj embeddingi dla train
    train_titles = train["Episode_Title"].fillna("").tolist()
    train_embeddings = model.encode(train_titles, show_progress_bar=True, batch_size=256)
    
    # Generuj embeddingi dla test
    test_titles = test["Episode_Title"].fillna("").tolist()
    test_embeddings = model.encode(test_titles, show_progress_bar=True, batch_size=256)
    
    print(f"‚úÖ Embeddingi wygenerowane! Shape: {train_embeddings.shape}")
    
    # PCA - redukcja do 50 wymiar√≥w
    pca = PCA(n_components=50, random_state=42)
    train_embeddings_pca = pca.fit_transform(train_embeddings)
    test_embeddings_pca = pca.transform(test_embeddings)
    
    print(f"‚úÖ PCA zako≈Ñczone! Explained variance: {pca.explained_variance_ratio_.sum():.2%}")
    
    # Dodaj jako features
    for i in range(50):
        train[f"title_emb_{i}"] = train_embeddings_pca[:, i]
        test[f"title_emb_{i}"] = test_embeddings_pca[:, i]
    
    # KMeans clustering - grupowanie podobnych tytu≈Ç√≥w
    n_clusters = 20  # 20 klastr√≥w tematycznych
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    train["title_cluster"] = kmeans.fit_predict(train_embeddings_pca)
    test["title_cluster"] = kmeans.predict(test_embeddings_pca)
    
    print(f"‚úÖ Clustering zako≈Ñczony! {n_clusters} klastr√≥w")
    print(f"Rozk≈Çad klastr√≥w (train):\n{train['title_cluster'].value_counts().head()}")
    
else:
    # Fallback: TF-IDF
    print("üìä U≈ºywam TF-IDF jako alternatywy...")
    
    tfidf = TfidfVectorizer(max_features=50, stop_words='english', ngram_range=(1, 2))
    
    train_tfidf = tfidf.fit_transform(train["Episode_Title"].fillna(""))
    test_tfidf = tfidf.transform(test["Episode_Title"].fillna(""))
    
    # Dodaj jako features
    for i in range(50):
        train[f"title_tfidf_{i}"] = train_tfidf[:, i].toarray().flatten()
        test[f"title_tfidf_{i}"] = test_tfidf[:, i].toarray().flatten()
    
    print(f"‚úÖ TF-IDF zako≈Ñczone! {train_tfidf.shape[1]} features")

# %% [markdown]
# ## üîü Frequency Encoding
# 
# **Dlaczego:**
# - Z EDA: niekt√≥re podcasty majƒÖ 20k+ odcink√≥w, inne <10
# - **Popularno≈õƒá podcastu** (frequency) mo≈ºe korelowaƒá ze s≈Çuchalno≈õciƒÖ
# 
# **Spodziewany efekt:** ‚Üì 1-2% RMSE

# %%
# Podcast frequency
podcast_freq = train["Podcast_Name"].value_counts()
train["podcast_frequency"] = train["Podcast_Name"].map(podcast_freq).fillna(0)
test["podcast_frequency"] = test["Podcast_Name"].map(podcast_freq).fillna(0)

# Genre frequency
genre_freq = train["Genre"].value_counts()
train["genre_frequency"] = train["Genre"].map(genre_freq).fillna(0)
test["genre_frequency"] = test["Genre"].map(genre_freq).fillna(0)

# Normalizacja (0-1)
train["podcast_frequency_norm"] = train["podcast_frequency"] / len(train)
test["podcast_frequency_norm"] = test["podcast_frequency"] / len(train)

train["genre_frequency_norm"] = train["genre_frequency"] / len(train)
test["genre_frequency_norm"] = test["genre_frequency"] / len(train)

print("‚úÖ Frequency encoding zako≈Ñczony!")
print(f"Top 5 podcasts by frequency:\n{podcast_freq.head()}")

# %% [markdown]
# ## 1Ô∏è‚É£1Ô∏è‚É£ Agregacje per Podcast
# 
# **Dlaczego:**
# - Z EDA: Top podcasty majƒÖ r√≥≈ºne ≈õrednie s≈Çuchalno≈õci (40-50 min)
# - **Podcast "brand"** ma silny wp≈Çyw na s≈Çuchalno≈õƒá
# 
# **Features:**
# - `podcast_avg_listening` - ≈õrednia historyczna (silny predyktor!)
# - `podcast_std_listening` - stabilno≈õƒá (niski std = sta≈Ça publiczno≈õƒá)
# - `podcast_min/max_listening` - zakres warto≈õci
# 
# **Spodziewany efekt:** ‚Üì 3-5% RMSE (jeden z najmocniejszych features!)

# %%
# Statystyki per Podcast (obliczane TYLKO na train!)
podcast_stats = train.groupby("Podcast_Name").agg({
    "Listening_Time_minutes": ["mean", "median", "std", "min", "max"],
    "Episode_Length_minutes": ["mean", "median"],
    "Number_of_Ads": ["mean", "median"],
    "Host_Popularity_percentage": "first",
    "Guest_Popularity_percentage": "mean"
}).reset_index()

# Sp≈Çaszcz kolumny
podcast_stats.columns = [
    "Podcast_Name",
    "podcast_avg_listening", "podcast_med_listening", "podcast_std_listening",
    "podcast_min_listening", "podcast_max_listening",
    "podcast_avg_length", "podcast_med_length",
    "podcast_avg_ads", "podcast_med_ads",
    "podcast_host_pop", "podcast_avg_guest_pop"
]

# Wype≈Çnij std NaN (podcasty z 1 odcinkiem)
podcast_stats["podcast_std_listening"].fillna(0, inplace=True)

# Merguj z LEFT join (wa≈ºne!)
print(f"Train przed merge: {len(train)}")
print(f"Test przed merge: {len(test)}")

train = train.merge(podcast_stats, on="Podcast_Name", how="left")
test = test.merge(podcast_stats, on="Podcast_Name", how="left")

print(f"Train po merge: {len(train)}")
print(f"Test po merge: {len(test)}")

# WALIDACJA
assert len(test) == 250000, f"‚ùå Test straci≈Ç wiersze! Ma {len(test)}"
print("‚úÖ Merge nie straci≈Ç wierszy")

print(f"\n‚úÖ Podcast stats dodane! Train shape: {train.shape}")

# %% [markdown]
# ## 1Ô∏è‚É£2Ô∏è‚É£ Agregacje per Genre
# 
# **Dlaczego:**
# - Z EDA: r√≥≈ºnice ≈õrednich miƒôdzy gatunkami (np. Comedy vs News)
# - **Genre characteristics** wp≈Çywa na s≈Çuchalno≈õƒá
# 
# **Spodziewany efekt:** ‚Üì 1-2% RMSE

# %%
# Statystyki per Genre
genre_stats = train.groupby("Genre").agg({
    "Listening_Time_minutes": ["mean", "median", "std"],
    "Guest_Popularity_percentage": "mean",
    "Episode_Length_minutes": "mean"
}).reset_index()

genre_stats.columns = [
    "Genre",
    "genre_avg_listening", "genre_med_listening", "genre_std_listening",
    "genre_avg_guest_pop", "genre_avg_length"
]

genre_stats["genre_std_listening"].fillna(0, inplace=True)

# LEFT join
print(f"Test przed merge: {len(test)}")
train = train.merge(genre_stats, on="Genre", how="left")
test = test.merge(genre_stats, on="Genre", how="left")
print(f"Test po merge: {len(test)}")

# WALIDACJA
assert len(test) == 250000, f"‚ùå Test straci≈Ç wiersze!"
print("‚úÖ Merge OK")

print(f"‚úÖ Genre stats dodane! Train shape: {train.shape}")

# %% [markdown]
# ## 1Ô∏è‚É£3Ô∏è‚É£ Relative Features (odcinek vs ≈õrednia)
# 
# **Dlaczego:**
# - **Nie liczƒÖ siƒô warto≈õci bezwzglƒôdne, ale relatywne!**
# - Przyk≈Çad: 60-minutowy odcinek w podca≈õcie o ≈õredniej 30 min ‚Üí **wyjƒÖtkowy!**
# - To samo 60 min w podca≈õcie o ≈õredniej 90 min ‚Üí **kr√≥tki**
# 
# **Spodziewany efekt:** ‚Üì 2-3% RMSE (model nauczy siƒô kontekstu)

# %%
for df in [train, test]:
    # Por√≥wnanie z podcast
    df["length_vs_podcast_avg"] = df["Episode_Length_minutes"] / (df["podcast_avg_length"] + 1)
    df["ads_vs_podcast_avg"] = df["Number_of_Ads"] / (df["podcast_avg_ads"] + 1)
    df["guest_pop_vs_podcast_avg"] = df["Guest_Popularity_percentage"] / (df["podcast_avg_guest_pop"] + 1)
    
    # Por√≥wnanie z genre
    df["length_vs_genre_avg"] = df["Episode_Length_minutes"] / (df["genre_avg_length"] + 1)
    
    # Czy ten odcinek jest powy≈ºej/poni≈ºej ≈õredniej?
    df["above_podcast_avg_length"] = (df["Episode_Length_minutes"] > df["podcast_avg_length"]).astype(int)
    df["above_genre_avg_length"] = (df["Episode_Length_minutes"] > df["genre_avg_length"]).astype(int)

print("‚úÖ Relative features dodane!")

# %% [markdown]
# ## 1Ô∏è‚É£4Ô∏è‚É£ Target Encoding (z CV leak protection!)
# 
# **Dlaczego:**
# - High cardinality (Podcast_Name: ~50-100 warto≈õci) ‚Üí one-hot niemo≈ºliwy
# - **Target encoding** = mapowanie kategorii na ≈õredniƒÖ target
# 
# **Problem: Data leakage!**
# - Je≈õli u≈ºyjemy ca≈Çego train ‚Üí model "podglƒÖda" odpowiedzi
# 
# **RozwiƒÖzanie: K-Fold CV**
# - Dla ka≈ºdego fold: oblicz ≈õredniƒÖ na pozosta≈Çych foldach
# - **Bayesian smoothing** ‚Üí regularyzacja dla rzadkich kategorii
# 
# **Spodziewany efekt:** ‚Üì 3-8% RMSE (najsilniejszy trick!)

# %%
def target_encode_with_cv(train_df, test_df, cat_col, target_col, n_splits=5, smoothing=10):
    """
    Target encoding z K-Fold CV (leak protection) + Bayesian smoothing
    
    Parametry:
    - smoothing: im wy≈ºszy, tym bardziej zbli≈ºamy siƒô do global_mean dla rzadkich kategorii
    """
    global_mean = train_df[target_col].mean()
    
    # Inicjalizacja
    train_df[f"{cat_col}_target_enc"] = global_mean
    
    # K-Fold CV dla train
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    for train_idx, val_idx in kf.split(train_df):
        train_fold = train_df.iloc[train_idx]
        
        # Oblicz statystyki na foldzie treningowym
        agg = train_fold.groupby(cat_col)[target_col].agg(['mean', 'count'])
        
        # Bayesian smoothing: (count * mean + smoothing * global_mean) / (count + smoothing)
        smoothed = (agg['count'] * agg['mean'] + smoothing * global_mean) / (agg['count'] + smoothing)
        
        # Mapuj na fold walidacyjny
        train_df.loc[val_idx, f"{cat_col}_target_enc"] = train_df.loc[val_idx, cat_col].map(smoothed).fillna(global_mean)
    
    # Dla test u≈ºyj ca≈Çego train
    agg_full = train_df.groupby(cat_col)[target_col].agg(['mean', 'count'])
    smoothed_full = (agg_full['count'] * agg_full['mean'] + smoothing * global_mean) / (agg_full['count'] + smoothing)
    test_df[f"{cat_col}_target_enc"] = test_df[cat_col].map(smoothed_full).fillna(global_mean)
    
    return train_df, test_df

# Zastosuj target encoding
print("üî• Target encoding w toku (mo≈ºe potrwaƒá ~2min)...")

for col in ["Podcast_Name", "Genre"]:
    train, test = target_encode_with_cv(train, test, col, "Listening_Time_minutes", smoothing=10)
    print(f"‚úÖ {col} zako≈Ñczony")

print(f"\n‚úÖ Target encoding zako≈Ñczony! Train shape: {train.shape}")

# %% [markdown]
# ## 1Ô∏è‚É£5Ô∏è‚É£ Konwersja typ√≥w (dla AutoGluon)

# %%
for col in train.select_dtypes(include=["object"]).columns:
    if col in train.columns:
        train[col] = train[col].astype("string")
    if col in test.columns:
        test[col] = test[col].astype("string")

print("‚úÖ Konwersja typ√≥w zako≈Ñczona!")
print(f"\nTrain dtypes:\n{train.dtypes.value_counts()}")

# %% [markdown]
# ## 1Ô∏è‚É£6Ô∏è‚É£ Wype≈Çnienie NaN z mergowania

# %%
# Dla nowych podcast√≥w/gatunk√≥w w test, kt√≥re nie by≈Çy w train
fill_cols = [col for col in train.columns if 'podcast_' in col or 'genre_' in col]

for col in fill_cols:
    if col in train.columns and col in test.columns:
        train[col].fillna(0, inplace=True)
        test[col].fillna(0, inplace=True)

print(f"Train final missing: {train.isnull().sum().sum()}")
print(f"Test final missing: {test.isnull().sum().sum()}")

# %% [markdown]
# ## 1Ô∏è‚É£7Ô∏è‚É£ Finalne podsumowanie features

# %%
print(f"\n{'='*60}")
print("üìä FINALNE STATYSTYKI")
print(f"{'='*60}")
print(f"Train shape: {train.shape}")
print(f"Test shape: {test.shape}")
print(f"\nLiczba features: {train.shape[1] - 1}")

# Nowe features
new_features = [col for col in train.columns if col not in pd.read_csv(TRAIN_PATH).columns]
print(f"\nDodano {len(new_features)} nowych features")
print("\nPrzyk≈Çadowe nowe features:")
for feat in sorted(new_features)[:20]:
    print(f"  - {feat}")

# %% [markdown]
# ## 1Ô∏è‚É£8Ô∏è‚É£ Zapisanie przetworzonych danych

# %%
output_train = Path("data/train_final_features.csv")
output_test = Path("data/test_final_features.csv")

train.to_csv(output_train, index=False)
test.to_csv(output_test, index=False)

print(f"\n‚úÖ Pliki zapisane:")
print(f"   Train: {output_train}")
print(f"   Test: {output_test}")
print(f"\nüöÄ Gotowe do trenowania w AutoGluon!")

# %% [markdown]
# ## 1Ô∏è‚É£9Ô∏è‚É£ Przygotowanie danych do AutoGluon

# %%
from autogluon.tabular import TabularPredictor

# Usu≈Ñ kolumny, kt√≥re nie powinny byƒá w treningu
drop_cols = ["id", "Publication_Day", "Publication_Time", "Episode_Title", "Podcast_Name"]
train_features = train.drop(columns=[col for col in drop_cols if col in train.columns])
test_features = test.drop(columns=[col for col in drop_cols if col in test.columns])

print(f"Train po usuniƒôciu: {train_features.shape}")
print(f"Test po usuniƒôciu: {test_features.shape}")

# WALIDACJA FINALNA
assert len(test_features) == 250000, f"‚ùå Test ma {len(test_features)}, powinien 250000!"
print("‚úÖ Test ma poprawnƒÖ liczbƒô wierszy")

# %% [markdown]
# ## 2Ô∏è‚É£0Ô∏è‚É£ Trening AutoGluon z optymalizacjƒÖ
# 
# **Konfiguracja:**
# - `presets="best_quality"` - najlepsze modele (LightGBM, CatBoost, XGBoost, RF)
# - `num_bag_folds=5` - K-fold bagging dla stabilno≈õci (redukcja overfittingu)
# - `num_stack_levels=1` - Stacking (meta-model ≈ÇƒÖczy predykcje)
# - Custom hyperparameters dla r√≥≈ºnych wariant√≥w LightGBM
# 
# **Spodziewany czas:** ~45-60 minut (1h time_limit)
# 
# **Spodziewany RMSE:** ~12.5-13.5 (na validation)

# %%
print("üöÄ Rozpoczynam trening AutoGluon...")
print("‚è≥ To zajmie ~45-60 minut...")

predictor = TabularPredictor(
    label="Listening_Time_minutes",
    eval_metric="root_mean_squared_error",
    problem_type="regression",
    path="models/autogluon_final"
).fit(
    train_data=train_features,
    time_limit=3600,  # 1 godzina
    presets="best_quality",
    num_bag_folds=5,
    num_bag_sets=1,
    num_stack_levels=1,
    hyperparameters={
        'GBM': [
            {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},
            {},  # Default LightGBM
            {'learning_rate': 0.03, 'num_leaves': 128, 'ag_args': {'name_suffix': 'Custom'}},
        ],
        'CAT': {},
        'XGB': {},
        'RF': [
            {'criterion': 'squared_error', 'max_depth': 20, 'ag_args': {'name_suffix': 'Deep'}},
        ],
        'XT': [
            {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE'}},
        ],
    },
    excluded_model_types=['KNN', 'NN_TORCH'],
    verbosity=2
)

print("\n‚úÖ Trening zako≈Ñczony!")

# %% [markdown]
# ## 2Ô∏è‚É£1Ô∏è‚É£ Analiza Feature Importance
# 
# **Dlaczego to wa≈ºne:**
# - Identyfikacja najsilniejszych predyktor√≥w
# - Potwierdzenie hipotez z EDA
# - Mo≈ºliwo≈õƒá usuniƒôcia s≈Çabych features (je≈õli potrzeba)

# %%
importance = predictor.feature_importance(train_features)
print(f"\n{'='*60}")
print("üìä TOP 30 NAJWA≈ªNIEJSZYCH FEATURES")
print(f"{'='*60}")
print(importance.head(30))

# Zapisz
importance.to_csv("feature_importance_final.csv")
print("\n‚úÖ Feature importance zapisane do: feature_importance_final.csv")

# Analiza kategorii features
print(f"\n{'='*60}")
print("üìä FEATURE IMPORTANCE PER KATEGORIA")
print(f"{'='*60}")

categories = {
    'Embeddingi': [col for col in importance.index if 'title_emb' in col or 'title_tfidf' in col],
    'Target Encoding': [col for col in importance.index if 'target_enc' in col],
    'Agregacje Podcast': [col for col in importance.index if 'podcast_' in col],
    'Agregacje Genre': [col for col in importance.index if 'genre_' in col],
    'Time Features': [col for col in importance.index if any(x in col for x in ['day_', 'hour_', 'is_weekend', 'is_morning', 'is_primetime'])],
    'Interakcje': [col for col in importance.index if '_x_' in col or 'vs_' in col],
    'Missing Flags': [col for col in importance.index if 'missing' in col],
    'Original': ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']
}

for cat_name, cols in categories.items():
    cat_importance = importance[importance.index.isin(cols)]
    if len(cat_importance) > 0:
        total_importance = cat_importance['importance'].sum()
        print(f"\n{cat_name}:")
        print(f"  Liczba features: {len(cat_importance)}")
        print(f"  Total importance: {total_importance:.4f}")
        print(f"  Top 3: {cat_importance.head(3)['importance'].tolist()}")

# %% [markdown]
# ### üîë **Wnioski z Feature Importance:**
# 
# **Sprawd≈∫:**
# 1. Czy `Episode_Length_minutes` jest w top 3? (potwierdzenie EDA)
# 2. Czy agregacje (`podcast_avg_listening`) majƒÖ wysokƒÖ importance?
# 3. Czy embeddingi/TF-IDF wnios≈Çy warto≈õƒá? (por√≥wnaj z prostymi text features)
# 4. Czy cyclic encoding (sin/cos) jest lepszy ni≈º proste numery?

# %% [markdown]
# ## 2Ô∏è‚É£2Ô∏è‚É£ Leaderboard modeli

# %%
leaderboard = predictor.leaderboard(train_features, silent=True)
print(f"\n{'='*60}")
print("üìä LEADERBOARD MODELI")
print(f"{'='*60}")
print(leaderboard[['model', 'score_val', 'score_test', 'pred_time_val', 'fit_time']])

best_model = predictor.get_model_best()
print(f"\nüèÜ Najlepszy model: {best_model}")

best_rmse = abs(leaderboard['score_val'].iloc[0])
print(f"üìä RMSE validation: {best_rmse:.4f}")

# Overfitting check
best_row = leaderboard.iloc[0]
train_rmse = abs(best_row['score_test'])
val_rmse = abs(best_row['score_val'])
overfitting_gap = val_rmse - train_rmse

print(f"\n{'='*60}")
print("üìä OVERFITTING ANALYSIS")
print(f"{'='*60}")
print(f"Train RMSE:      {train_rmse:.4f}")
print(f"Validation RMSE: {val_rmse:.4f}")
print(f"Overfitting gap: {overfitting_gap:.4f}")

if overfitting_gap > 1.0:
    print("‚ö†Ô∏è Model mo≈ºe overfittowaƒá (gap > 1.0)")
else:
    print("‚úÖ Model generalizuje dobrze (gap < 1.0)")

# %% [markdown]
# ### üîë **Interpretacja Leaderboard:**
# 
# **Kt√≥ry model wygrywa?**
# - `WeightedEnsemble` powinien byƒá na top (≈ÇƒÖczy wszystkie modele)
# - Je≈õli single model (np. LightGBM) wygrywa ‚Üí ensemble nie pom√≥g≈Ç
# 
# **RMSE Validation:**
# - To jest **spodziewany wynik na Kaggle** (¬±0.5 RMSE)
# - Je≈õli RMSE ~12.5-13.5 ‚Üí bardzo dobry wynik!
# 
# **Overfitting:**
# - Gap < 1.0 ‚Üí model stabilny
# - Gap > 2.0 ‚Üí zbyt mocny overfitting, rozwa≈º wiƒôcej regularizacji

# %% [markdown]
# ## 2Ô∏è‚É£3Ô∏è‚É£ Predykcja na test

# %%
predictions = predictor.predict(test_features)

# Statystyki predykcji
print(f"\n{'='*60}")
print("üìä STATYSTYKI PREDYKCJI")
print(f"{'='*60}")
print(f"Min:    {predictions.min():.2f}")
print(f"Max:    {predictions.max():.2f}")
print(f"Mean:   {predictions.mean():.2f}")
print(f"Median: {predictions.median():.2f}")
print(f"Std:    {predictions.std():.2f}")

# Sprawd≈∫ warto≈õci ujemne
if (predictions < 0).any():
    n_negative = (predictions < 0).sum()
    print(f"\n‚ö†Ô∏è UWAGA: {n_negative} predykcji ujemnych! Clipowanie do 0...")
    predictions = predictions.clip(lower=0)
else:
    print("\n‚úÖ Brak ujemnych predykcji")

# Por√≥wnaj z train distribution
print(f"\n{'='*60}")
print("üìä POR√ìWNANIE: Train vs Test Predictions")
print(f"{'='*60}")
print(f"Train target - Mean: {train['Listening_Time_minutes'].mean():.2f}, Std: {train['Listening_Time_minutes'].std():.2f}")
print(f"Test preds   - Mean: {predictions.mean():.2f}, Std: {predictions.std():.2f}")

mean_diff = abs(train['Listening_Time_minutes'].mean() - predictions.mean())
if mean_diff > 5:
    print(f"‚ö†Ô∏è Du≈ºa r√≥≈ºnica ≈õrednich ({mean_diff:.2f}) - sprawd≈∫ czy nie ma data shift!")
else:
    print(f"‚úÖ Podobne rozk≈Çady (r√≥≈ºnica: {mean_diff:.2f})")

# %% [markdown]
# ## 2Ô∏è‚É£4Ô∏è‚É£ Zapisanie submission

# %%
submission = test[["id"]].copy()
submission["Listening_Time_minutes"] = predictions

# Zapisz
submission.to_csv("submission_final.csv", index=False)

print(f"\n{'='*60}")
print("‚úÖ SUBMISSION ZAPISANY")
print(f"{'='*60}")
print(f"Plik: submission_final.csv")
print(f"Shape: {submission.shape}")
print(f"\nPierwsze 5 wierszy:")
print(submission.head())
print(f"\nOstatnie 5 wierszy:")
print(submission.tail())

# %% [markdown]
# ## 2Ô∏è‚É£5Ô∏è‚É£ Walidacja submission

# %%
print(f"\n{'='*60}")
print("üîç WALIDACJA FORMATU SUBMISSION")
print(f"{'='*60}")

# Sprawd≈∫ format
checks = []
checks.append(("Kolumna 'id' istnieje", "id" in submission.columns))
checks.append(("Kolumna 'Listening_Time_minutes' istnieje", "Listening_Time_minutes" in submission.columns))
checks.append(("Liczba wierszy = 250000", len(submission) == 250000))
checks.append(("Brak warto≈õci NULL", submission["Listening_Time_minutes"].isnull().sum() == 0))
checks.append(("Brak ujemnych warto≈õci", (submission["Listening_Time_minutes"] < 0).sum() == 0))
checks.append(("ID sƒÖ unikalne", submission["id"].nunique() == 250000))

for check_name, passed in checks:
    status = "‚úÖ" if passed else "‚ùå"
    print(f"{status} {check_name}")

all_passed = all([c[1] for c in checks])
if all_passed:
    print("\nüéâ Wszystkie walidacje przesz≈Çy! Submission gotowy do wys≈Çania.")
else:
    print("\n‚ùå Niekt√≥re walidacje nie przesz≈Çy - sprawd≈∫ b≈Çƒôdy powy≈ºej!")

# %% [markdown]
# ## 2Ô∏è‚É£6Ô∏è‚É£ Podsumowanie ko≈Ñcowe
# 
# ### üìä **FINALNE STATYSTYKI MODELU:**

# %%
print(f"\n{'='*80}")
print("üéØ PODSUMOWANIE KO≈ÉCOWE")
print(f"{'='*80}")

print(f"\n1Ô∏è‚É£ DANE:")
print(f"   Train samples: {len(train):,}")
print(f"   Test samples:  {len(test):,}")
print(f"   Features:      {train_features.shape[1] - 1}")

print(f"\n2Ô∏è‚É£ FEATURE ENGINEERING:")
print(f"   ‚úÖ Flagi missing (2)")
print(f"   ‚úÖ Cyclic encoding (4: day_sin/cos, hour_sin/cos)")
print(f"   ‚úÖ Text embeddingi/TF-IDF (50-100 features)")
print(f"   ‚úÖ Agregacje Podcast (11 features)")
print(f"   ‚úÖ Agregacje Genre (5 features)")
print(f"   ‚úÖ Target encoding (2 features)")
print(f"   ‚úÖ Interakcje (15+ features)")

print(f"\n3Ô∏è‚É£ MODEL:")
print(f"   Najlepszy model:  {best_model}")
print(f"   RMSE validation:  {best_rmse:.4f}")
print(f"   Overfitting gap:  {overfitting_gap:.4f}")
print(f"   Training time:    {best_row['fit_time']:.0f}s (~{best_row['fit_time']/60:.1f} min)")

print(f"\n4Ô∏è‚É£ PREDYKCJE:")
print(f"   Mean:   {predictions.mean():.2f} min")
print(f"   Median: {predictions.median():.2f} min")
print(f"   Range:  [{predictions.min():.2f}, {predictions.max():.2f}]")

print(f"\n5Ô∏è‚É£ SPODZIEWANY WYNIK NA KAGGLE:")
print(f"   RMSE: ~{best_rmse:.2f} (¬±0.5)")

print(f"\n{'='*80}")
print("‚úÖ PROCES ZAKO≈ÉCZONY - SUBMISSION GOTOWY!")
print(f"{'='*80}")

# %% [markdown]
# ## üìà **Kluczowe wnioski z modelowania:**
# 
# ### ‚úÖ **Co zadzia≈Ça≈Ço najlepiej:**
# 
# 1. **Agregacje per Podcast** (podcast_avg_listening)
#    - Najsilniejszy predyktor
#    - Podcast "brand" ma ogromny wp≈Çyw
# 
# 2. **Target Encoding** (z CV protection)
#    - Skutecznie radzi sobie z high cardinality
#    - Bayesian smoothing zapobiega overfittingowi
# 
# 3. **Episode_Length_minutes** (z EDA)
#    - Potwierdzenie: najsilniejsza korelacja
#    - Interakcje z czasem publikacji doda≈Çy warto≈õƒá
# 
# 4. **Cyclic Encoding** (sin/cos)
#    - Lepsze ni≈º proste numery
#    - Model rozumie cykliczno≈õƒá (Sunday blisko Monday)
# 
# 5. **Text Embeddingi** (SentenceTransformers/TF-IDF)
#    - Semantyka tytu≈Ç√≥w ma znaczenie
#    - Clustering wychwyci≈Ç tematy
# 
# ### ‚ö†Ô∏è **Co mog≈Ço nie pom√≥c:**
# 
# - Niekt√≥re interakcje mogƒÖ byƒá redundantne (np. total_popularity vs popularity_ratio)
# - Zbyt du≈ºo text features mo≈ºe wprowadzaƒá szum
# 
# ### üöÄ **Dalsze kierunki optymalizacji:**
# 
# 1. **Feature Selection** - usu≈Ñ features o importance < 0.001
# 2. **Hyperparameter Tuning** - wiƒôcej wariant√≥w LightGBM
# 3. **Ensemble r√≥≈ºnych preprocessing√≥w** - r√≥≈ºne strategie imputacji
# 4. **Deep Learning** - TabTransformer/FT-Transformer dla tabel

print("\nüìÅ Pliki wygenerowane:")
print("   - train_final_features.csv")
print("   - test_final_features.csv")
print("   - submission_final.csv")
print("   - feature_importance_final.csv")
print("   - models/autogluon_final/ (saved models)")

print("\nüéâ Gotowe! Mo≈ºesz teraz wys≈Çaƒá submission_final.csv na Kaggle.")